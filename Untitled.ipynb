{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3de2584b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"PDB dataset loader.\"\"\"\n",
    "# import math\n",
    "# from typing import Optional\n",
    "\n",
    "# import torch\n",
    "# import torch.distributed as dist\n",
    "\n",
    "# import tree\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import pandas as pd\n",
    "# import logging\n",
    "# import random\n",
    "# import functools as fn\n",
    "\n",
    "# from torch.utils import data\n",
    "# from data import utils as du\n",
    "# from openfold.data import data_transforms\n",
    "# from openfold.np import residue_constants\n",
    "# from openfold.utils import rigid_utils\n",
    "\n",
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "# from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28b5ad6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openfold.np import residue_constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c54836d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "711cb211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import tree\n",
    "#clear memory better\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "import numpy as np\n",
    "import util.npose_util as nu\n",
    "import os\n",
    "import pathlib\n",
    "import dgl\n",
    "from dgl import backend as F\n",
    "import torch_geometric\n",
    "from torch.utils.data import random_split, DataLoader, Dataset\n",
    "from typing import Dict\n",
    "from torch import Tensor\n",
    "from dgl import DGLGraph\n",
    "from torch import nn\n",
    "# from chemical import cos_ideal_NCAC #from RoseTTAFold2\n",
    "from torch import einsum\n",
    "import time\n",
    "import random\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09df63cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import se3_diffuse.utils as du\n",
    "from data_rigid_diffuser.diffuser import FrameDiffNoise\n",
    "from gudiff_model import Data_Graph\n",
    "from gudiff_model.Data_Graph import build_npose_from_coords, dump_coord_pdb, define_graph_edges, make_pe_encoding\n",
    "from gudiff_model.Data_Graph import Helix4_Dataset, Make_KNN_MP_Graphs\n",
    "from se3_transformer.model.basis import get_basis, update_basis_with_fused\n",
    "from se3_transformer.model.transformer import Sequential, SE3Transformer\n",
    "from se3_transformer.model.transformer_topk import SE3Transformer_topK\n",
    "from se3_transformer.model.FAPE_Loss import FAPE_loss, Qs2Rs, normQ\n",
    "from se3_transformer.model.layers.attentiontopK import AttentionBlockSE3\n",
    "from se3_transformer.model.layers.linear import LinearSE3\n",
    "from se3_transformer.model.layers.convolution import ConvSE3, ConvSE3FuseLevel\n",
    "from se3_transformer.model.layers.norm import NormSE3\n",
    "from se3_transformer.model.layers.pooling import GPooling, Latent_Unpool, Unpool_Layer\n",
    "from se3_transformer.runtime.utils import str2bool, to_cuda\n",
    "from se3_transformer.model.fiber import Fiber\n",
    "from se3_transformer.model.transformer import get_populated_edge_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dffe08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#indices for, unsure if needed\n",
    "CA = Data_Graph.CA\n",
    "N = Data_Graph.N\n",
    "C = Data_Graph.C\n",
    "\n",
    "#find better way to incorporate coord_scale\n",
    "\n",
    "#needed\n",
    "N_CA_dist = (Data_Graph.N_CA_dist/10.).to('cuda')\n",
    "C_CA_dist = (Data_Graph.C_CA_dist/10.).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8ed61fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#basically a I just need data loader to grab coordinates from datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9da631a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########CURRENTLY REWRITING THIS######\n",
    "\n",
    "def torch_normalize(v, eps=1e-6):\n",
    "    \"\"\"Normalize vector in last axis\"\"\"\n",
    "    norm = torch.linalg.vector_norm(v, dim=len(v.shape)-1)+eps\n",
    "    return v / norm[...,None]\n",
    "def gudiff_parse_chain_feats(chain_feats, scale_factor=10., cast_type=torch.float32):\n",
    "    ca_idx = residue_constants.atom_order['CA']\n",
    "    n_idx = residue_constants.atom_order['N']\n",
    "    c_idx = residue_constants.atom_order['C']\n",
    "    chain_feats['bb_mask'] = chain_feats['atom_mask'][:, ca_idx]\n",
    "    \n",
    "    bb_pos = chain_feats['atom_positions'][:, ca_idx]/scale_factor #scale factor mod\n",
    "    bb_center = np.sum(bb_pos, axis=0) / (np.sum(chain_feats['bb_mask']) + 1e-5)\n",
    "    centered_pos = chain_feats['atom_positions'] - bb_center[None, None, :]\n",
    "    \n",
    "    \n",
    "    coordinates = centered_pos/scale_factor\n",
    "    #unsqueeze to stack together later\n",
    "    N_CA_vec = torch.tensor(coordinates[:,n_idx] - coordinates[:,ca_idx], dtype=cast_type)/scale_factor\n",
    "    C_CA_vec = torch.tensor(coordinates[:,c_idx] - coordinates[:,ca_idx], dtype=cast_type)/scale_factor\n",
    "        \n",
    "    N_CA_vec = torch_normalize(N_CA_vec)#.unsqueeze(2) #do the unsqueeze later\n",
    "    C_CA_vec = torch_normalize(C_CA_vec)#.unsqueeze(2)\n",
    "\n",
    "    scaled_pos = centered_pos / scale_factor\n",
    "    chain_feats['atom_positions'] = scaled_pos * chain_feats['atom_mask'][..., None]\n",
    "    \n",
    "    chain_feats['CA'] = torch.tensor(coordinates[:,ca_idx],dtype=cast_type)\n",
    "    chain_feats['N_CA_vec'] = N_CA_vec\n",
    "    chain_feats['C_CA_vec'] = C_CA_vec\n",
    "    return chain_feats\n",
    "\n",
    "\n",
    "\n",
    "class smallPDBDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            diffuser,\n",
    "            meta_data_path = '/mnt/h/datasets/p200/metadata.csv',\n",
    "            filter_dict=True,\n",
    "            maxlen=None,\n",
    "            is_training=True,\n",
    "            input_t=None\n",
    "        ):\n",
    "        #self._log = logging.getLogger(__name__)\n",
    "        self._is_training = is_training\n",
    "        self.meta_data_path = meta_data_path\n",
    "        self._init_metadata(filter_dict=filter_dict,maxlen=maxlen) #includes create split that saves self.csv\n",
    "        self._diffuser = diffuser\n",
    "        self.input_t = input_t\n",
    "        \n",
    "    @property\n",
    "    def is_training(self):\n",
    "        return self._is_training\n",
    "\n",
    "    @property\n",
    "    def diffuser(self):\n",
    "        return self._diffuser\n",
    "\n",
    "    @property\n",
    "    def data_conf(self):\n",
    "        return self._data_conf\n",
    "\n",
    "    def _init_metadata(self, filter_dict=True, maxlen=None):\n",
    "        \"\"\"Initialize metadata.\"\"\"\n",
    "        \n",
    "        #meta_data_path = '/mnt/h/datasets/p200/metadata.csv'\n",
    "        pdb_csv = pd.read_csv(self.meta_data_path)\n",
    "        \n",
    "        if filter_dict:\n",
    "            filter_conf = {'allowed_oligomer': ['monomeric'],\n",
    "                           'max_loop_percent': 0.75}\n",
    "            pdb_csv = pdb_csv[pdb_csv.oligomeric_detail.isin(filter_conf['allowed_oligomer'])]\n",
    "            pdb_csv = pdb_csv[pdb_csv.coil_percent < filter_conf['max_loop_percent']]\n",
    "            pdb_csv = pdb_csv.sort_values('modeled_seq_len', ascending=False)\n",
    "            \n",
    "        if maxlen is not None:\n",
    "            pdb_csv = pdb_csv[:maxlen]\n",
    "        #self._create_split(pdb_csv)\n",
    "        self.csv = pdb_csv\n",
    "    def _create_split(self, pdb_csv):\n",
    "        # Training or validation specific logic.\n",
    "        #if self.is_training:\n",
    "        self.csv = pdb_csv\n",
    "        #self._log.info(\n",
    "        #    f'Training: {len(self.csv)} examples')\n",
    "#         else:\n",
    "#             all_lengths = np.sort(pdb_csv.modeled_seq_len.unique())\n",
    "#             length_indices = (len(all_lengths) - 1) * np.linspace(\n",
    "#                 0.0, 1.0, self._data_conf.num_eval_lengths)\n",
    "#             length_indices = length_indices.astype(int)\n",
    "            \n",
    "#             if self._simple:\n",
    "#                 eval_lengths = np.array([65]).astype(int)\n",
    "#             else:\n",
    "#                 eval_lengths = all_lengths[length_indices]\n",
    "                \n",
    "#             eval_csv = pdb_csv[pdb_csv.modeled_seq_len.isin(eval_lengths)]\n",
    "#             # Fix a random seed to get the same split each time.\n",
    "#             eval_csv = eval_csv.groupby('modeled_seq_len').sample(\n",
    "#                 self._data_conf.samples_per_eval_length, replace=True, random_state=123)\n",
    "#             eval_csv = eval_csv.sort_values('modeled_seq_len', ascending=False)\n",
    "#             self.csv = eval_csv\n",
    "#             self._log.info(\n",
    "#                 f'Validation: {len(self.csv)} examples with lengths {eval_lengths}')\n",
    "    # cache make the same sample in same batch \n",
    "    #@fn.lru_cache(maxsize=100)\n",
    "    def _process_csv_row(self, processed_file_path, index=0):\n",
    "        \n",
    "        processed_feats = du.read_pkl(processed_file_path)\n",
    "        chain_feats = gudiff_parse_chain_feats(processed_feats,scale_factor=10.)\n",
    "        \n",
    "        # Only take modeled residues.\n",
    "        modeled_idx = processed_feats['modeled_idx']\n",
    "        min_idx = np.min(modeled_idx)\n",
    "        max_idx = np.max(modeled_idx)\n",
    "        del processed_feats['modeled_idx']\n",
    "        processed_feats = tree.map_structure(\n",
    "            lambda x: x[min_idx:(max_idx+1)], processed_feats)\n",
    "        chain_feats = tree.map_structure(\n",
    "            lambda x: x[min_idx:(max_idx+1)], chain_feats)\n",
    "        \n",
    "\n",
    "        # Run through OpenFold data transforms.\n",
    "        # Re-number residue indices for each chain such that it starts from 1.\n",
    "        # Randomize chain indices.\n",
    "        chain_idx = processed_feats[\"chain_index\"]\n",
    "        res_idx = processed_feats['residue_index']\n",
    "        new_res_idx = np.zeros_like(res_idx)\n",
    "        new_chain_idx = np.zeros_like(res_idx)\n",
    "        all_chain_idx = np.unique(chain_idx).tolist()\n",
    "        shuffled_chain_idx = np.array(\n",
    "            random.sample(all_chain_idx, len(all_chain_idx))) - np.min(all_chain_idx) + 1\n",
    "        for i,chain_id in enumerate(all_chain_idx):\n",
    "            chain_mask = (chain_idx == chain_id).astype(int)\n",
    "            chain_min_idx = np.min(res_idx + (1 - chain_mask) * 1e3).astype(int)\n",
    "            new_res_idx = new_res_idx + (res_idx - chain_min_idx + 1) * chain_mask\n",
    "\n",
    "            # Shuffle chain_index\n",
    "            replacement_chain_id = shuffled_chain_idx[i]\n",
    "            new_chain_idx = new_chain_idx + replacement_chain_id * chain_mask\n",
    "\n",
    "        # To speed up processing, only take necessary features\n",
    "        final_feats = {\n",
    "            'chain_idx': new_chain_idx,\n",
    "            'residue_index': processed_feats['residue_index'],\n",
    "            'res_mask': processed_feats['bb_mask'],\n",
    "            'CA':   chain_feats['CA'],\n",
    "            'N_CA': chain_feats['N_CA_vec'], #when unsqueeze? later maybe take time to change this behavior\n",
    "            'C_CA': chain_feats['C_CA_vec'],\n",
    "            'file_path_index' : index\n",
    "        }\n",
    "        return final_feats\n",
    "    \n",
    "    def get_specific_t(self, idx, t):\n",
    "        example_idx = idx\n",
    "        csv_row = self.csv.iloc[example_idx]\n",
    "        if 'pdb_name' in csv_row:\n",
    "            pdb_name = csv_row['pdb_name']\n",
    "        elif 'chain_name' in csv_row:\n",
    "            pdb_name = csv_row['chain_name']\n",
    "        else:\n",
    "            raise ValueError('Need chain identifier.')\n",
    "            \n",
    "        processed_file_path = csv_row['processed_path']\n",
    "        chain_feats = self._process_csv_row(processed_file_path,index=idx)\n",
    "\n",
    "        bb_noised =  self._diffuser.forward(chain_feats, t=t)\n",
    "        chain_feats.update(bb_noised)\n",
    "\n",
    "        # Convert all features to tensors.\n",
    "        final_feats = tree.map_structure(\n",
    "                    lambda x: x if torch.is_tensor(x) else torch.tensor(x), chain_feats)\n",
    "        \n",
    "        return final_feats\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        example_idx = idx\n",
    "        csv_row = self.csv.iloc[example_idx]\n",
    "        if 'pdb_name' in csv_row:\n",
    "            pdb_name = csv_row['pdb_name']\n",
    "        elif 'chain_name' in csv_row:\n",
    "            pdb_name = csv_row['chain_name']\n",
    "        else:\n",
    "            raise ValueError('Need chain identifier.')\n",
    "        processed_file_path = csv_row['processed_path']\n",
    "        chain_feats = self._process_csv_row(processed_file_path, index=idx)\n",
    "\n",
    "        # Use a fixed seed for evaluation.\n",
    "#         if self.is_training:\n",
    "        rng = np.random.default_rng(None)\n",
    "#         else:\n",
    "#             rng = np.random.default_rng(idx)\n",
    "\n",
    "        # Sample t and diffuse.\n",
    "#         if self.is_training:\n",
    "        if self.input_t is None:\n",
    "            t = rng.uniform(1e-3, 1.0)\n",
    "        else:\n",
    "            t = self.input_t\n",
    "        bb_noised =  self._diffuser.forward(chain_feats, t=t)\n",
    "#         else:\n",
    "#             t = 1.0\n",
    "#             diff_feats_t = self.diffuser.sample_ref(\n",
    "#                 n_samples=gt_bb_rigid.shape[0],\n",
    "#                 impute=gt_bb_rigid,\n",
    "#                 diffuse_mask=None,\n",
    "#                 as_tensor_7=True,\n",
    "#             )\n",
    "        chain_feats.update(bb_noised)\n",
    "\n",
    "        # Convert all features to tensors.\n",
    "        final_feats = tree.map_structure(\n",
    "            lambda x: x if torch.is_tensor(x) else torch.tensor(x), chain_feats)\n",
    "        #final_feats = du.pad_feats(final_feats, csv_row['modeled_seq_len'])\n",
    "        #if self.is_training:\n",
    "#         else:\n",
    "#             return final_feats, pdb_name\n",
    "        \n",
    "        return final_feats\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "387ae865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils import data\n",
    "\n",
    "class TrainSampler(torch.utils.data.Sampler):\n",
    "\n",
    "    def __init__(self, batch_size, dataset,\n",
    "                 sample_mode='length_batch'):\n",
    "        \n",
    "        self.dataset = dataset\n",
    "        self._data_csv = dataset.csv\n",
    "        self._dataset_indices = list(range(len(self._data_csv)))\n",
    "        self._data_csv['index'] = self._dataset_indices\n",
    "        self._batch_size = batch_size\n",
    "        self.epoch = 0\n",
    "        self._sample_mode = sample_mode\n",
    "        self.sampler_len = len(self._dataset_indices) * self._batch_size\n",
    "        self.min_t = 1e-3\n",
    "        #self._log = logging.getLogger(__name__)\n",
    "        #self._data_conf = data_conf\n",
    "        #self._dataset = dataset\n",
    "        #self._data_csv = self._dataset.csv\n",
    "    def __iter__(self):\n",
    "        if self._sample_mode == 'length_batch':\n",
    "            # Each batch contains multiple proteins of the same length.\n",
    "            sampled_order = self._data_csv.groupby('modeled_seq_len').sample(\n",
    "                self._batch_size, replace=True, random_state=self.epoch) #one batch per length\n",
    "            return iter(sampled_order['index'].tolist())\n",
    "        elif self._sample_mode == 'single_length':\n",
    "            rand_index = self._data_csv['index'].to_numpy()\n",
    "            np.random.shuffle(rand_index)\n",
    "            num_batches = int(rand_index.shape[0]/self._batch_size)\n",
    "            rand_index = rand_index[:(num_batches*self._batch_size)] #drop last batch\n",
    "            return iter(rand_index)\n",
    "        else:\n",
    "            raise ValueError(f'Invalid sample mode: {self._sample_mode}')\n",
    "    \n",
    "#     def getbb(self, idx):\n",
    "#         csv_row = self._data_csv.iloc[idx]\n",
    "#         processed_file_path = csv_row['processed_path']\n",
    "#         chain_feats = self.dataset._process_csv_row(processed_file_path)\n",
    "#         return chain_feats  \n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.csv)\n",
    "\n",
    "    def set_epoch(self, epoch):\n",
    "        self.epoch = epoch\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.sampler_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ccb8820",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gudiff_model.Graph_UNet import GraphUNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "72647f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_step(batch_feats, noised_dict, graph_maker, graph_unet, train=True):\n",
    "    L = batch_feats['CA'].shape[1]\n",
    "    B = batch_feats['CA'].shape[0]\n",
    "    CA_t  = batch_feats['CA']\n",
    "    NC_t = CA_t +  batch_feats['N_CA']\n",
    "    CC_t = CA_t +  batch_feats['C_CA']\n",
    "    true =  torch.cat((NC_t,CA_t,CC_t),dim=2).reshape(B,L,3,3)\n",
    "\n",
    "    CA_n  = batch_feats['CA_noised'].reshape(B, L, 3)#.to('cuda')\n",
    "    NC_n = CA_n + batch_feats['N_CA_noised'].reshape(B, L, 3)#.to('cuda')\n",
    "    CC_n = CA_n + batch_feats['C_CA_noised'].reshape(B, L, 3)#.to('cuda')\n",
    "    noise_xyz =  torch.cat((NC_n,CA_n,CC_n),dim=2).reshape(B,L,3,3)\n",
    "    \n",
    "    x = gm.prep_for_network(noised_dict)\n",
    "    out = gu(x, batch_feats['t'])\n",
    "    CA_p = out['1'][:,0,:].reshape(B, L, 3) + CA_n #translation of Calpha\n",
    "    Qs = out['1'][:,1,:] # rotation of frame\n",
    "    Qs = Qs.unsqueeze(1).repeat((1,2,1))\n",
    "    Qs = torch.cat((torch.ones((B*L,2,1),device=Qs.device),Qs),dim=-1).reshape(B,L,2,4)\n",
    "    Qs = normQ(Qs)\n",
    "    Rs = Qs2Rs(Qs)\n",
    "    N_C_to_Rot = torch.cat((noised_dict['N_CA'].reshape(B, L, 3),\n",
    "                            noised_dict['C_CA'].reshape(B, L, 3)),dim=2).reshape(B,L,2,1,3)\n",
    "    \n",
    "    rot_vecs = einsum('bnkij,bnkhj->bnki',Rs, N_C_to_Rot)\n",
    "    NC_p = CA_p + rot_vecs[:,:,0,:]*N_CA_dist #remove bc who cares? me maybe\n",
    "    CC_p = CA_p + rot_vecs[:,:,1,:]*C_CA_dist #remove maybe\n",
    "\n",
    "    pred = torch.cat((NC_p,CA_p,CC_p),dim=2).reshape(B,L,3,3)\n",
    "\n",
    "    tloss, loss = FAPE_loss(pred.unsqueeze(0), true, batch_feats['score_scale'])\n",
    "    \n",
    "    return pred, tloss\n",
    "\n",
    "def get_noise_pred_true(batch_feats, noised_dict, graph_maker, graph_unet, scale=10):\n",
    "    \n",
    "    L = batch_feats['CA'].shape[1]\n",
    "    B = batch_feats['CA'].shape[0]\n",
    "    CA_t  = batch_feats['CA']\n",
    "    NC_t = CA_t +  batch_feats['N_CA']*N_CA_dist\n",
    "    CC_t = CA_t +  batch_feats['C_CA']*C_CA_dist\n",
    "    true =  torch.cat((NC_t,CA_t,CC_t),dim=2).reshape(B,L,3,3)\n",
    "\n",
    "    CA_n  = batch_feats['CA_noised'].reshape(B, L, 3)#.to('cuda')\n",
    "    NC_n = CA_n + batch_feats['N_CA_noised'].reshape(B, L, 3)*N_CA_dist\n",
    "    CC_n = CA_n + batch_feats['C_CA_noised'].reshape(B, L, 3)*C_CA_dist\n",
    "    noise_xyz =  torch.cat((NC_n,CA_n,CC_n),dim=2).reshape(B,L,3,3)\n",
    "    \n",
    "    x = graph_maker.prep_for_network(noised_dict)\n",
    "    out = graph_unet(x, batch_feats['t'])\n",
    "    CA_p = out['1'][:,0,:].reshape(B, L, 3) + CA_n #translation of Calpha\n",
    "    Qs = out['1'][:,1,:] # rotation of frame\n",
    "    Qs = Qs.unsqueeze(1).repeat((1,2,1))\n",
    "    Qs = torch.cat((torch.ones((B*L,2,1),device=Qs.device),Qs),dim=-1).reshape(B,L,2,4)\n",
    "    Qs = normQ(Qs)\n",
    "    Rs = Qs2Rs(Qs)\n",
    "    N_C_to_Rot = torch.cat((noised_dict['N_CA'].reshape(B, L, 3),\n",
    "                            noised_dict['C_CA'].reshape(B, L, 3)),dim=2).reshape(B,L,2,1,3)\n",
    "    \n",
    "    rot_vecs = einsum('bnkij,bnkhj->bnki',Rs, N_C_to_Rot)\n",
    "    NC_p = CA_p + rot_vecs[:,:,0,:]*N_CA_dist #remove bc who cares? me maybe\n",
    "    CC_p = CA_p + rot_vecs[:,:,1,:]*C_CA_dist #remove maybe\n",
    "\n",
    "    pred = torch.cat((NC_p,CA_p,CC_p),dim=2).reshape(B,L,3,3)\n",
    "    \n",
    "    return true.to('cpu').numpy()*scale, noise_xyz.to('cpu').numpy()*scale, pred.detach().to('cpu').numpy()*scale\n",
    "\n",
    "def generate_tbatch(pdbdataset, index_in, input_t):\n",
    "    batch_list = []\n",
    "    for i,t in enumerate(input_t):\n",
    "        batch_list.append(pdbdataset.get_specific_t(index_in[i], input_t[i]))\n",
    "\n",
    "    batch_feats = {}\n",
    "    for k in batch_list[0].keys():\n",
    "        batch_feats[k] = torch.stack([batch_list[i][k] for i in range(len(batch_list))])\n",
    "    return batch_feats\n",
    "\n",
    "\n",
    "def dump_tnp(true, noise, pred, t_val, e=0, numOut=1,outdir='output/'):\n",
    "    \n",
    "    if numOut>true.shape[0]:\n",
    "        numOut = true.shape[0]\n",
    "    \n",
    "    for x in range(numOut):\n",
    "        dump_coord_pdb(true[x], fileOut=f'{outdir}/true_{t_val[x]*100:.0f}_e{e}_{x}.pdb')\n",
    "        dump_coord_pdb(noise[x], fileOut=f'{outdir}/noise_{t_val[x]*100:.0f}_e{e}_{x}.pdb')\n",
    "        dump_coord_pdb(pred[x], fileOut=f'{outdir}/pred_{t_val[x]*100:.0f}_e{e}_{x}.pdb')\n",
    "        \n",
    "def visualize_model(pdbdataset, view_t, epoch=1, numOut=1, outdir='output/'):\n",
    "    device='cuda'\n",
    "    index_in = np.random.choice(np.arange(len(pdbdataset)), size=len(view_t))\n",
    "    batch_feats = generate_tbatch(pdbdataset , index_in, view_t)\n",
    "    \n",
    "    batch_feats= tree.map_structure(\n",
    "                    lambda x: x.to(device), batch_feats)\n",
    "    noised_dict =   {'CA': batch_feats['CA_noised'] ,\n",
    "                     'N_CA': batch_feats['N_CA_noised'].unsqueeze(-2) ,\n",
    "                     'C_CA': batch_feats['C_CA_noised'].unsqueeze(-2)  }\n",
    "    \n",
    "    true, noise, pred = get_noise_pred_true(batch_feats, noised_dict, gm, gu, scale=10)\n",
    "    dump_tnp(true,noise,pred, view_t, e=epoch, numOut=numOut, outdir=f'{outdir}/models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0452154b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_save_folder(name=''):\n",
    "    base_folder = time.strftime(f'log/%y%b%d_%I%M%p_{name}/', time.localtime())\n",
    "    if not os.path.exists(base_folder):\n",
    "        os.makedirs(base_folder)\n",
    "    subfolders = ['models']\n",
    "    for subfolder in subfolders:\n",
    "        if not os.path.exists(base_folder + subfolder):\n",
    "            os.makedirs(base_folder + subfolder)\n",
    "            \n",
    "    return base_folder\n",
    "        \n",
    "def save_chkpt(model_path, model, optimizer, epoch, batch, val_losses, train_losses):\n",
    "    \"\"\"Save a training checkpoint\n",
    "    Args:\n",
    "        model_path (str): the path to save the model to\n",
    "        model (nn.Module): the model to save\n",
    "        optimizer (torch.optim.Optimizer): the optimizer to save\n",
    "        epoch (int): the current epoch\n",
    "        batch (int): the current batch in the epoch\n",
    "        loss_domain (list of int): a list of the shared domain for val and training \n",
    "            losses\n",
    "        val_losses (list of float): a list containing the validation losses\n",
    "        train_losses (list of float): a list containing the training losses\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "    state_dict = dict()\n",
    "    state_dict.update({'model':model.state_dict(),\n",
    "                       'optimizer':optimizer.state_dict(),\n",
    "                       'epoch':epoch,\n",
    "                       'batch':batch,\n",
    "                       'train_losses':train_losses,\n",
    "                       'val_losses':val_losses\n",
    "                       })\n",
    "    torch.save(state_dict, f'{model_path}model_e{epoch}')\n",
    "    \n",
    "def load_model(model_path, model_class):\n",
    "    \"\"\"Load a saved model\"\"\"\n",
    "    \n",
    "    device = 'cuda:0'\n",
    "    model = model_class()\n",
    "    model.load_state_dict(torch.load(model_path)['model'])\n",
    "    model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e0174936",
   "metadata": {},
   "outputs": [],
   "source": [
    "#since length list is all the same is that the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bba3980d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdn = FrameDiffNoise()\n",
    "B = 16\n",
    "sd = smallPDBDataset(fdn , meta_data_path = '/mnt/h/datasets/bCov_4H/metadata.csv', \n",
    "                     filter_dict=False, maxlen=500, input_t=0.05)\n",
    "ts = TrainSampler(B,sd, sample_mode='single_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "770d5f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ts = TrainSampler(B,sd, sample_mode='length_batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "91d5f736",
   "metadata": {},
   "outputs": [],
   "source": [
    "gu = GraphUNet(batch_size = B, num_layers_ca = 2).to('cuda')\n",
    "opti = torch.optim.Adam(gu.parameters(), lr=0.0005, weight_decay=5e-6) #prev lr=0.0005\n",
    "gm = Make_KNN_MP_Graphs() #consider precalculating graphs for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "920c0ed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3f404ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dL = torch.utils.data.DataLoader(sd,\n",
    "        sampler=ts,\n",
    "        batch_size=B,\n",
    "        shuffle=False,\n",
    "        collate_fn=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "18cb85fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch_feats in dL:\n",
    "#     device='cuda'\n",
    "#     batch_feats= tree.map_structure(\n",
    "#                     lambda x: x.to(device), batch_feats)\n",
    "#     noised_dict =   {'CA': batch_feats['CA_noised'] ,\n",
    "#                      'N_CA': batch_feats['N_CA_noised'].unsqueeze(-2) ,\n",
    "#                      'C_CA': batch_feats['C_CA_noised'].unsqueeze(-2)  }\n",
    "#     break\n",
    "# t,n,p = get_noise_pred_true(batch_feats, noised_dict, gm, gu, scale=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da479c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d44fd192",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_model(sd, view_t, epoch=1, numOut=1, outdir='output/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1c421b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss Epoch 0: 0.31683850288391113;   Epoch time: 38\n",
      "Average Train Loss Epoch 1: 0.2965218126773834;   Epoch time: 38\n",
      "Average Train Loss Epoch 2: 0.28856685757637024;   Epoch time: 37\n",
      "Average Train Loss Epoch 3: 0.28646761178970337;   Epoch time: 38\n",
      "Average Train Loss Epoch 4: 0.28713902831077576;   Epoch time: 37\n",
      "Average Train Loss Epoch 5: 0.2861272692680359;   Epoch time: 38\n",
      "Average Train Loss Epoch 6: 0.28468209505081177;   Epoch time: 38\n",
      "Average Train Loss Epoch 7: 0.2848116159439087;   Epoch time: 38\n",
      "Average Train Loss Epoch 8: 0.2857855260372162;   Epoch time: 38\n",
      "Average Train Loss Epoch 9: 0.28512683510780334;   Epoch time: 39\n",
      "Average Train Loss Epoch 10: 0.29245567321777344;   Epoch time: 38\n",
      "Average Train Loss Epoch 11: 0.28564608097076416;   Epoch time: 37\n",
      "Average Train Loss Epoch 12: 0.28413382172584534;   Epoch time: 38\n",
      "Average Train Loss Epoch 13: 0.28399738669395447;   Epoch time: 38\n",
      "Average Train Loss Epoch 14: 0.2834002673625946;   Epoch time: 38\n",
      "Average Train Loss Epoch 15: 0.2813481092453003;   Epoch time: 38\n",
      "Average Train Loss Epoch 16: 0.2833770215511322;   Epoch time: 38\n",
      "Average Train Loss Epoch 17: 0.28153225779533386;   Epoch time: 38\n",
      "Average Train Loss Epoch 18: 0.2807997465133667;   Epoch time: 38\n",
      "Average Train Loss Epoch 19: 0.2812636196613312;   Epoch time: 38\n",
      "Average Train Loss Epoch 20: 0.281074196100235;   Epoch time: 37\n",
      "Average Train Loss Epoch 21: 0.28039616346359253;   Epoch time: 37\n",
      "Average Train Loss Epoch 22: 0.278935968875885;   Epoch time: 38\n",
      "Average Train Loss Epoch 23: 0.2804422676563263;   Epoch time: 38\n",
      "Average Train Loss Epoch 24: 0.27972543239593506;   Epoch time: 38\n",
      "Average Train Loss Epoch 25: 0.2801951467990875;   Epoch time: 38\n",
      "Average Train Loss Epoch 26: 0.27912619709968567;   Epoch time: 38\n",
      "Average Train Loss Epoch 27: 0.27819761633872986;   Epoch time: 38\n",
      "Average Train Loss Epoch 28: 0.2761348783969879;   Epoch time: 38\n",
      "Average Train Loss Epoch 29: 0.2766774594783783;   Epoch time: 38\n",
      "Average Train Loss Epoch 30: 0.274081826210022;   Epoch time: 37\n",
      "Average Train Loss Epoch 31: 0.2722274661064148;   Epoch time: 38\n",
      "Average Train Loss Epoch 32: 0.2719591557979584;   Epoch time: 37\n",
      "Average Train Loss Epoch 33: 0.2704996168613434;   Epoch time: 37\n",
      "Average Train Loss Epoch 34: 0.27019309997558594;   Epoch time: 38\n",
      "Average Train Loss Epoch 35: 0.26833376288414;   Epoch time: 37\n",
      "Average Train Loss Epoch 36: 0.26688864827156067;   Epoch time: 37\n",
      "Average Train Loss Epoch 37: 0.26565471291542053;   Epoch time: 39\n",
      "Average Train Loss Epoch 38: 0.2620498836040497;   Epoch time: 38\n",
      "Average Train Loss Epoch 39: 0.2606988847255707;   Epoch time: 38\n",
      "Average Train Loss Epoch 40: 0.2574586570262909;   Epoch time: 38\n",
      "Average Train Loss Epoch 41: 0.25527676939964294;   Epoch time: 38\n",
      "Average Train Loss Epoch 42: 0.2542985677719116;   Epoch time: 38\n",
      "Average Train Loss Epoch 43: 0.25311630964279175;   Epoch time: 37\n",
      "Average Train Loss Epoch 44: 0.25026533007621765;   Epoch time: 38\n",
      "Average Train Loss Epoch 45: 0.2505935728549957;   Epoch time: 37\n",
      "Average Train Loss Epoch 46: 0.24814929068088531;   Epoch time: 38\n",
      "Average Train Loss Epoch 47: 0.24653685092926025;   Epoch time: 36\n",
      "Average Train Loss Epoch 48: 0.24479514360427856;   Epoch time: 38\n",
      "Average Train Loss Epoch 49: 0.24382412433624268;   Epoch time: 37\n",
      "Average Train Loss Epoch 50: 0.24186424911022186;   Epoch time: 38\n",
      "Average Train Loss Epoch 51: 0.23946548998355865;   Epoch time: 38\n",
      "Average Train Loss Epoch 52: 0.23688527941703796;   Epoch time: 37\n",
      "Average Train Loss Epoch 53: 0.236202672123909;   Epoch time: 38\n",
      "Average Train Loss Epoch 54: 0.24744673073291779;   Epoch time: 39\n",
      "Average Train Loss Epoch 55: 0.23857104778289795;   Epoch time: 38\n",
      "Average Train Loss Epoch 56: 0.23474468290805817;   Epoch time: 38\n",
      "Average Train Loss Epoch 57: 0.23297147452831268;   Epoch time: 38\n",
      "Average Train Loss Epoch 58: 0.2316802591085434;   Epoch time: 39\n",
      "Average Train Loss Epoch 59: 0.2285669445991516;   Epoch time: 39\n",
      "Average Train Loss Epoch 60: 0.227919802069664;   Epoch time: 39\n",
      "Average Train Loss Epoch 61: 0.22672201693058014;   Epoch time: 38\n",
      "Average Train Loss Epoch 62: 0.22764959931373596;   Epoch time: 38\n",
      "Average Train Loss Epoch 63: 0.22611196339130402;   Epoch time: 39\n",
      "Average Train Loss Epoch 64: 0.22524212300777435;   Epoch time: 39\n",
      "Average Train Loss Epoch 65: 0.22297225892543793;   Epoch time: 39\n",
      "Average Train Loss Epoch 66: 0.22256873548030853;   Epoch time: 39\n",
      "Average Train Loss Epoch 67: 0.22292940318584442;   Epoch time: 39\n",
      "Average Train Loss Epoch 68: 0.22156170010566711;   Epoch time: 38\n",
      "Average Train Loss Epoch 69: 0.22103169560432434;   Epoch time: 38\n",
      "Average Train Loss Epoch 70: 0.22070558369159698;   Epoch time: 39\n",
      "Average Train Loss Epoch 71: 0.21998748183250427;   Epoch time: 39\n",
      "Average Train Loss Epoch 72: 0.2192872315645218;   Epoch time: 39\n",
      "Average Train Loss Epoch 73: 0.21893970668315887;   Epoch time: 39\n",
      "Average Train Loss Epoch 74: 0.21855372190475464;   Epoch time: 38\n",
      "Average Train Loss Epoch 75: 0.21765001118183136;   Epoch time: 39\n",
      "Average Train Loss Epoch 76: 0.21736137568950653;   Epoch time: 38\n",
      "Average Train Loss Epoch 77: 0.21574777364730835;   Epoch time: 38\n",
      "Average Train Loss Epoch 78: 0.215082049369812;   Epoch time: 39\n",
      "Average Train Loss Epoch 79: 0.21472813189029694;   Epoch time: 39\n",
      "Average Train Loss Epoch 80: 0.2128925323486328;   Epoch time: 38\n",
      "Average Train Loss Epoch 81: 0.21194732189178467;   Epoch time: 39\n",
      "Average Train Loss Epoch 82: 0.210040882229805;   Epoch time: 38\n",
      "Average Train Loss Epoch 83: 0.2067079395055771;   Epoch time: 39\n",
      "Average Train Loss Epoch 84: 0.20163998007774353;   Epoch time: 39\n",
      "Average Train Loss Epoch 85: 0.19988329708576202;   Epoch time: 38\n",
      "Average Train Loss Epoch 86: 0.19471968710422516;   Epoch time: 38\n",
      "Average Train Loss Epoch 87: 0.19276857376098633;   Epoch time: 39\n",
      "Average Train Loss Epoch 88: 0.19050684571266174;   Epoch time: 38\n",
      "Average Train Loss Epoch 89: 0.18727070093154907;   Epoch time: 38\n",
      "Average Train Loss Epoch 90: 0.18656021356582642;   Epoch time: 39\n",
      "Average Train Loss Epoch 91: 0.18511515855789185;   Epoch time: 38\n",
      "Average Train Loss Epoch 92: 0.181287482380867;   Epoch time: 38\n",
      "Average Train Loss Epoch 93: 0.1808461844921112;   Epoch time: 39\n",
      "Average Train Loss Epoch 94: 0.17968273162841797;   Epoch time: 39\n",
      "Average Train Loss Epoch 95: 0.17904876172542572;   Epoch time: 38\n",
      "Average Train Loss Epoch 96: 0.17730577290058136;   Epoch time: 39\n",
      "Average Train Loss Epoch 97: 0.1767520308494568;   Epoch time: 39\n",
      "Average Train Loss Epoch 98: 0.17520517110824585;   Epoch time: 39\n",
      "Average Train Loss Epoch 99: 0.17599211633205414;   Epoch time: 39\n"
     ]
    }
   ],
   "source": [
    "model_path = make_save_folder(name=f'4h_load_check')\n",
    "num_epochs = 100\n",
    "e_start= 0\n",
    "save_per=10\n",
    "avg_vloss=0\n",
    "\n",
    "#visualize_T\n",
    "#view_t = np.array([0.05]*8)\n",
    "\n",
    "view_t = np.array([0.01,0.05,0.1,0.2,0.3,0.5,0.8,1.0])\n",
    "view_t = view_t[None,...].repeat(int(np.ceil(B/len(view_t))),axis=0).flatten()[:B]\n",
    "\n",
    "\n",
    "for e in range(e_start, e_start+num_epochs):\n",
    "    \n",
    "    running_tloss = 0 \n",
    "    start = time.time()\n",
    "    for i,batch_feats in enumerate(dL):\n",
    "\n",
    "        device='cuda'\n",
    "        batch_feats= tree.map_structure(\n",
    "                        lambda x: x.to(device), batch_feats)\n",
    "        noised_dict =   {'CA': batch_feats['CA_noised'] ,\n",
    "                         'N_CA': batch_feats['N_CA_noised'].unsqueeze(-2) ,\n",
    "                         'C_CA': batch_feats['C_CA_noised'].unsqueeze(-2)  }\n",
    "\n",
    "        pred, train_loss = model_step(batch_feats, noised_dict, gm, gu, train=True)\n",
    "        opti.zero_grad()\n",
    "        train_loss.backward()\n",
    "        opti.step()\n",
    "\n",
    "        running_tloss += train_loss.detach().cpu()\n",
    "        \n",
    "    \n",
    "\n",
    "    #n,p,t = get_noise_pred_true(batch_feats, noised_dict, gm, gu, scale=10)\n",
    "    end = time.time()\n",
    "    avg_tloss = running_tloss/(i+1)\n",
    "    print(f'Average Train Loss Epoch {e}: {avg_tloss};   Epoch time: {end-start:.0f}')\n",
    "    if e %save_per==save_per-1:\n",
    "        visualize_model(sd, view_t, epoch=e, numOut=8, outdir=f'{model_path}')\n",
    "        save_chkpt(f'{model_path}', gu, opti, e, B, avg_vloss, avg_tloss)\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "27010f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch_feats in dL:\n",
    "#     device='cuda'\n",
    "#     batch_feats= tree.map_structure(\n",
    "#                     lambda x: x.to(device), batch_feats)\n",
    "#     noised_dict =   {'CA': batch_feats['CA_noised'] ,\n",
    "#                      'N_CA': batch_feats['N_CA_noised'].unsqueeze(-2) ,\n",
    "#                      'C_CA': batch_feats['C_CA_noised'].unsqueeze(-2)  }\n",
    "#     break\n",
    "# t,n,p = get_noise_pred_true(batch_feats, noised_dict, gm, gu, scale=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9cec9c7a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dump_tnp(t,\u001b[43mn\u001b[49m,p,batch_feats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m'\u001b[39m], e\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, numOut\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,outdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'n' is not defined"
     ]
    }
   ],
   "source": [
    "dump_tnp(t,n,p,batch_feats['t'], e=0, numOut=1,outdir='output/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02152d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the conversions don't seem correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d800e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_model(bb_dict, noised_bb, batched_t, epoch, numOut=1, outdir='output/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb8b9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "t=0.05\n",
    "t_vec = np.ones((B,))*t\n",
    "print(t_vec)\n",
    "model_path = make_save_folder(name=f'fdiff_modelStep_doublecheck')\n",
    "num_epochs = 50\n",
    "e_start= 0\n",
    "save_per=10\n",
    "avg_vloss=0\n",
    "\n",
    "for e in range(e_start, e_start+num_epochs):\n",
    "    \n",
    "    running_tloss = 0 \n",
    "    start = time.time()\n",
    "    for i, bb_dict in enumerate(train_dL):\n",
    "        noised_bb, tv, ss = fdn.forward_fixed_nodes(bb_dict,t_vec=t_vec, useR3=useR3)\n",
    "        tv = tv.to('cuda')\n",
    "        ss = ss.to('cuda')\n",
    "        train_loss = model_step(bb_dict, noised_bb, tv, ss, gm, gu)\n",
    "        opti.zero_grad()\n",
    "        train_loss.backward()\n",
    "        opti.step()\n",
    "\n",
    "        running_tloss += train_loss.detach().cpu()\n",
    "    \n",
    "    end = time.time()\n",
    "    avg_tloss = running_tloss/(i+1)\n",
    "    print(f'Average Train Loss Epoch {e}: {avg_tloss};   Epoch time: {end-start:.0f}')\n",
    "\n",
    "    if e %save_per==save_per-1:\n",
    "        with torch.no_grad():\n",
    "            running_vloss = 0\n",
    "            for i, bb_dictv in enumerate(val_dL):\n",
    "                noised_bb, tv, ss = fdn.forward_fixed_nodes(bb_dictv,t_vec=None, useR3=useR3) #this does the opposite of traditional, upweighting lower\n",
    "                tv = tv.to('cuda')\n",
    "                ss = ss.to('cuda')\n",
    "                valid_loss = model_step(bb_dictv, noised_bb, tv, ss, gm, gu)\n",
    "                running_vloss += valid_loss\n",
    "                \n",
    "        avg_vloss = running_vloss/(i+1)\n",
    "        print(f'Average Valid Loss Epoch {e}: {avg_vloss}')\n",
    "                \n",
    "        noised_bb, tv, ss = fdn.forward_fixed_nodes(bb_dict, t_vec=vis_t)\n",
    "        tv = tv.to('cuda')\n",
    "        visualize_model(bb_dict, noised_bb, tv, e, numOut=8,outdir=f'{model_path}')\n",
    "        save_chkpt(f'{model_path}', gu, opti, e, B, avg_vloss, avg_tloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d886a502",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c9db18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "se33",
   "language": "python",
   "name": "se33"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
