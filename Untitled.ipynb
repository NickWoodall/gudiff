{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3de2584b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"PDB dataset loader.\"\"\"\n",
    "# import math\n",
    "# from typing import Optional\n",
    "\n",
    "# import torch\n",
    "# import torch.distributed as dist\n",
    "\n",
    "# import tree\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import pandas as pd\n",
    "# import logging\n",
    "# import random\n",
    "# import functools as fn\n",
    "\n",
    "# from torch.utils import data\n",
    "# from data import utils as du\n",
    "# from openfold.data import data_transforms\n",
    "# from openfold.np import residue_constants\n",
    "# from openfold.utils import rigid_utils\n",
    "\n",
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "# from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28b5ad6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openfold.np import residue_constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c54836d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "711cb211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import tree\n",
    "#clear memory better\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "import numpy as np\n",
    "import util.npose_util as nu\n",
    "import os\n",
    "import pathlib\n",
    "import dgl\n",
    "from dgl import backend as F\n",
    "import torch_geometric\n",
    "from torch.utils.data import random_split, DataLoader, Dataset\n",
    "from typing import Dict\n",
    "from torch import Tensor\n",
    "from dgl import DGLGraph\n",
    "from torch import nn\n",
    "# from chemical import cos_ideal_NCAC #from RoseTTAFold2\n",
    "from torch import einsum\n",
    "import time\n",
    "import random\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09df63cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import se3_diffuse.utils as du\n",
    "from data_rigid_diffuser.diffuser import FrameDiffNoise\n",
    "from gudiff_model import Data_Graph\n",
    "from gudiff_model.Data_Graph import build_npose_from_coords, dump_coord_pdb, define_graph_edges, make_pe_encoding\n",
    "from gudiff_model.Data_Graph import Helix4_Dataset, Make_KNN_MP_Graphs\n",
    "from se3_transformer.model.basis import get_basis, update_basis_with_fused\n",
    "from se3_transformer.model.transformer import Sequential, SE3Transformer\n",
    "from se3_transformer.model.transformer_topk import SE3Transformer_topK\n",
    "from se3_transformer.model.FAPE_Loss import FAPE_loss, Qs2Rs, normQ\n",
    "from se3_transformer.model.layers.attentiontopK import AttentionBlockSE3\n",
    "from se3_transformer.model.layers.linear import LinearSE3\n",
    "from se3_transformer.model.layers.convolution import ConvSE3, ConvSE3FuseLevel\n",
    "from se3_transformer.model.layers.norm import NormSE3\n",
    "from se3_transformer.model.layers.pooling import GPooling, Latent_Unpool, Unpool_Layer\n",
    "from se3_transformer.runtime.utils import str2bool, to_cuda\n",
    "from se3_transformer.model.fiber import Fiber\n",
    "from se3_transformer.model.transformer import get_populated_edge_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dffe08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#indices for, unsure if needed\n",
    "CA = Data_Graph.CA\n",
    "N = Data_Graph.N\n",
    "C = Data_Graph.C\n",
    "\n",
    "#find better way to incorporate coord_scale\n",
    "\n",
    "#needed\n",
    "N_CA_dist = (Data_Graph.N_CA_dist/10.).to('cuda')\n",
    "C_CA_dist = (Data_Graph.C_CA_dist/10.).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8ed61fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#basically a I just need data loader to grab coordinates from datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9da631a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########CURRENTLY REWRITING THIS######\n",
    "\n",
    "def torch_normalize(v, eps=1e-6):\n",
    "    \"\"\"Normalize vector in last axis\"\"\"\n",
    "    norm = torch.linalg.vector_norm(v, dim=len(v.shape)-1)+eps\n",
    "    return v / norm[...,None]\n",
    "def gudiff_parse_chain_feats(chain_feats, scale_factor=10., cast_type=torch.float32):\n",
    "    ca_idx = residue_constants.atom_order['CA']\n",
    "    n_idx = residue_constants.atom_order['N']\n",
    "    c_idx = residue_constants.atom_order['C']\n",
    "    chain_feats['bb_mask'] = chain_feats['atom_mask'][:, ca_idx]\n",
    "    \n",
    "    bb_pos = chain_feats['atom_positions'][:, ca_idx]/scale_factor #scale factor mod\n",
    "    bb_center = np.sum(bb_pos, axis=0) / (np.sum(chain_feats['bb_mask']) + 1e-5)\n",
    "    centered_pos = chain_feats['atom_positions'] - bb_center[None, None, :]\n",
    "    \n",
    "    \n",
    "    coordinates = centered_pos/scale_factor\n",
    "    #unsqueeze to stack together later\n",
    "    N_CA_vec = torch.tensor(coordinates[:,N] - coordinates[:,ca_idx], dtype=cast_type)\n",
    "    C_CA_vec = torch.tensor(coordinates[:,C] - coordinates[:,ca_idx], dtype=cast_type)\n",
    "        \n",
    "    N_CA_vec = torch_normalize(N_CA_vec)#.unsqueeze(2) #do the unsqueeze later\n",
    "    C_CA_vec = torch_normalize(C_CA_vec)#.unsqueeze(2)\n",
    "\n",
    "    scaled_pos = centered_pos / scale_factor\n",
    "    chain_feats['atom_positions'] = scaled_pos * chain_feats['atom_mask'][..., None]\n",
    "    \n",
    "    chain_feats['CA'] = torch.tensor(coordinates[:,ca_idx],dtype=cast_type)\n",
    "    chain_feats['N_CA_vec'] = N_CA_vec\n",
    "    chain_feats['C_CA_vec'] = C_CA_vec\n",
    "    return chain_feats\n",
    "\n",
    "\n",
    "\n",
    "class smallPDBDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            diffuser,\n",
    "            meta_data_path = '/mnt/h/datasets/p200/metadata.csv',\n",
    "            is_training=True\n",
    "        ):\n",
    "        #self._log = logging.getLogger(__name__)\n",
    "        self._is_training = is_training\n",
    "        self.meta_data_path = meta_data_path\n",
    "        self._init_metadata() #includes create split that saves self.csv\n",
    "        self._diffuser = diffuser\n",
    "        \n",
    "    @property\n",
    "    def is_training(self):\n",
    "        return self._is_training\n",
    "\n",
    "    @property\n",
    "    def diffuser(self):\n",
    "        return self._diffuser\n",
    "\n",
    "    @property\n",
    "    def data_conf(self):\n",
    "        return self._data_conf\n",
    "\n",
    "    def _init_metadata(self):\n",
    "        \"\"\"Initialize metadata.\"\"\"\n",
    "        \n",
    "        meta_data_path = '/mnt/h/datasets/p200/metadata.csv'\n",
    "        pdb_csv = pd.read_csv(meta_data_path)\n",
    "\n",
    "        filter_conf = {'allowed_oligomer': ['monomeric'],\n",
    "                       'max_loop_percent': 0.75}\n",
    "        pdb_csv = pdb_csv[pdb_csv.oligomeric_detail.isin(filter_conf['allowed_oligomer'])]\n",
    "        pdb_csv = pdb_csv[pdb_csv.coil_percent < filter_conf['max_loop_percent']]\n",
    "        pdb_csv = pdb_csv.sort_values('modeled_seq_len', ascending=False)\n",
    "        #self._create_split(pdb_csv)\n",
    "        self.csv = pdb_csv\n",
    "    def _create_split(self, pdb_csv):\n",
    "        # Training or validation specific logic.\n",
    "        #if self.is_training:\n",
    "        self.csv = pdb_csv\n",
    "        #self._log.info(\n",
    "        #    f'Training: {len(self.csv)} examples')\n",
    "#         else:\n",
    "#             all_lengths = np.sort(pdb_csv.modeled_seq_len.unique())\n",
    "#             length_indices = (len(all_lengths) - 1) * np.linspace(\n",
    "#                 0.0, 1.0, self._data_conf.num_eval_lengths)\n",
    "#             length_indices = length_indices.astype(int)\n",
    "            \n",
    "#             if self._simple:\n",
    "#                 eval_lengths = np.array([65]).astype(int)\n",
    "#             else:\n",
    "#                 eval_lengths = all_lengths[length_indices]\n",
    "                \n",
    "#             eval_csv = pdb_csv[pdb_csv.modeled_seq_len.isin(eval_lengths)]\n",
    "#             # Fix a random seed to get the same split each time.\n",
    "#             eval_csv = eval_csv.groupby('modeled_seq_len').sample(\n",
    "#                 self._data_conf.samples_per_eval_length, replace=True, random_state=123)\n",
    "#             eval_csv = eval_csv.sort_values('modeled_seq_len', ascending=False)\n",
    "#             self.csv = eval_csv\n",
    "#             self._log.info(\n",
    "#                 f'Validation: {len(self.csv)} examples with lengths {eval_lengths}')\n",
    "    # cache make the same sample in same batch \n",
    "    #@fn.lru_cache(maxsize=100)\n",
    "    def _process_csv_row(self, processed_file_path):\n",
    "        \n",
    "        processed_feats = du.read_pkl(processed_file_path)\n",
    "        chain_feats = gudiff_parse_chain_feats(processed_feats,scale_factor=10.)\n",
    "        \n",
    "        # Only take modeled residues.\n",
    "        modeled_idx = processed_feats['modeled_idx']\n",
    "        min_idx = np.min(modeled_idx)\n",
    "        max_idx = np.max(modeled_idx)\n",
    "        del processed_feats['modeled_idx']\n",
    "        processed_feats = tree.map_structure(\n",
    "            lambda x: x[min_idx:(max_idx+1)], processed_feats)\n",
    "        chain_feats = tree.map_structure(\n",
    "            lambda x: x[min_idx:(max_idx+1)], chain_feats)\n",
    "        \n",
    "\n",
    "        # Run through OpenFold data transforms.\n",
    "        # Re-number residue indices for each chain such that it starts from 1.\n",
    "        # Randomize chain indices.\n",
    "        chain_idx = processed_feats[\"chain_index\"]\n",
    "        res_idx = processed_feats['residue_index']\n",
    "        new_res_idx = np.zeros_like(res_idx)\n",
    "        new_chain_idx = np.zeros_like(res_idx)\n",
    "        all_chain_idx = np.unique(chain_idx).tolist()\n",
    "        shuffled_chain_idx = np.array(\n",
    "            random.sample(all_chain_idx, len(all_chain_idx))) - np.min(all_chain_idx) + 1\n",
    "        for i,chain_id in enumerate(all_chain_idx):\n",
    "            chain_mask = (chain_idx == chain_id).astype(int)\n",
    "            chain_min_idx = np.min(res_idx + (1 - chain_mask) * 1e3).astype(int)\n",
    "            new_res_idx = new_res_idx + (res_idx - chain_min_idx + 1) * chain_mask\n",
    "\n",
    "            # Shuffle chain_index\n",
    "            replacement_chain_id = shuffled_chain_idx[i]\n",
    "            new_chain_idx = new_chain_idx + replacement_chain_id * chain_mask\n",
    "\n",
    "        # To speed up processing, only take necessary features\n",
    "        final_feats = {\n",
    "            'chain_idx': new_chain_idx,\n",
    "            'residue_index': processed_feats['residue_index'],\n",
    "            'res_mask': processed_feats['bb_mask'],\n",
    "            'CA':   chain_feats['CA'],\n",
    "            'N_CA': chain_feats['N_CA_vec'], #when unsqueeze? later maybe take time to change this behavior\n",
    "            'C_CA': chain_feats['C_CA_vec']\n",
    "        }\n",
    "        return final_feats\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        #######workign on this\n",
    "        # Sample data example.\n",
    "        example_idx = idx\n",
    "        csv_row = self.csv.iloc[example_idx]\n",
    "        if 'pdb_name' in csv_row:\n",
    "            pdb_name = csv_row['pdb_name']\n",
    "        elif 'chain_name' in csv_row:\n",
    "            pdb_name = csv_row['chain_name']\n",
    "        else:\n",
    "            raise ValueError('Need chain identifier.')\n",
    "        processed_file_path = csv_row['processed_path']\n",
    "        chain_feats = self._process_csv_row(processed_file_path)\n",
    "\n",
    "        # Use a fixed seed for evaluation.\n",
    "#         if self.is_training:\n",
    "        rng = np.random.default_rng(None)\n",
    "#         else:\n",
    "#             rng = np.random.default_rng(idx)\n",
    "\n",
    "        # Sample t and diffuse.\n",
    "#         if self.is_training:\n",
    "        t = rng.uniform(1e-3, 1.0)\n",
    "        bb_noised =  self._diffuser.forward(chain_feats, t=t)\n",
    "#         else:\n",
    "#             t = 1.0\n",
    "#             diff_feats_t = self.diffuser.sample_ref(\n",
    "#                 n_samples=gt_bb_rigid.shape[0],\n",
    "#                 impute=gt_bb_rigid,\n",
    "#                 diffuse_mask=None,\n",
    "#                 as_tensor_7=True,\n",
    "#             )\n",
    "        chain_feats.update(bb_noised)\n",
    "\n",
    "        # Convert all features to tensors.\n",
    "        final_feats = tree.map_structure(\n",
    "            lambda x: x if torch.is_tensor(x) else torch.tensor(x), chain_feats)\n",
    "        #final_feats = du.pad_feats(final_feats, csv_row['modeled_seq_len'])\n",
    "        #if self.is_training:\n",
    "#         else:\n",
    "#             return final_feats, pdb_name\n",
    "        \n",
    "        return final_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "387ae865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils import data\n",
    "\n",
    "class TrainSampler(torch.utils.data.Sampler):\n",
    "\n",
    "    def __init__(self, batch_size, dataset,\n",
    "                 sample_mode='length_batch'):\n",
    "        \n",
    "        self.dataset = dataset\n",
    "        self._data_csv = dataset.csv\n",
    "        self._dataset_indices = list(range(len(self._data_csv)))\n",
    "        self._data_csv['index'] = self._dataset_indices\n",
    "        self._batch_size = batch_size\n",
    "        self.epoch = 0\n",
    "        self._sample_mode = sample_mode\n",
    "        self.sampler_len = len(self._dataset_indices) * self._batch_size\n",
    "        self.min_t = 1e-3\n",
    "        #self._log = logging.getLogger(__name__)\n",
    "        #self._data_conf = data_conf\n",
    "        #self._dataset = dataset\n",
    "        #self._data_csv = self._dataset.csv\n",
    "    def __iter__(self):\n",
    "        if self._sample_mode == 'length_batch':\n",
    "            # Each batch contains multiple proteins of the same length.\n",
    "            sampled_order = self._data_csv.groupby('modeled_seq_len').sample(\n",
    "                self._batch_size, replace=True, random_state=self.epoch)\n",
    "            return iter(sampled_order['index'].tolist())\n",
    "        else:\n",
    "            raise ValueError(f'Invalid sample mode: {self._sample_mode}')\n",
    "    \n",
    "#     def getbb(self, idx):\n",
    "#         csv_row = self._data_csv.iloc[idx]\n",
    "#         processed_file_path = csv_row['processed_path']\n",
    "#         chain_feats = self.dataset._process_csv_row(processed_file_path)\n",
    "#         return chain_feats  \n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.csv)\n",
    "\n",
    "    def set_epoch(self, epoch):\n",
    "        self.epoch = epoch\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.sampler_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ccb8820",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gudiff_model.Graph_UNet import GraphUNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0174936",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bba3980d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdn = FrameDiffNoise()\n",
    "B = 2\n",
    "sd = smallPDBDataset(fdn)\n",
    "ts = TrainSampler(B,sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91d5f736",
   "metadata": {},
   "outputs": [],
   "source": [
    "gu = GraphUNet(batch_size = B, num_layers_ca = 2).to('cuda')\n",
    "opti = torch.optim.Adam(gu.parameters(), lr=0.0005, weight_decay=5e-6) #prev lr=0.0005\n",
    "gm = Make_KNN_MP_Graphs() #consider precalculating graphs for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cee0a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e5a35be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3074\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sd.csv)-1,0,-1):\n",
    "    row = sd.csv.iloc[i]\n",
    "    processed_feats = du.read_pkl(row['processed_path'])\n",
    "    chain_feats = gudiff_parse_chain_feats(processed_feats,scale_factor=10.)\n",
    "    if chain_feats['bb_mask'].shape[0] != chain_feats['bb_mask'].sum():\n",
    "        print(i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f404ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dL = torch.utils.data.DataLoader(sd,\n",
    "        sampler=ts,\n",
    "        batch_size=B,\n",
    "        shuffle=False,\n",
    "        collate_fn=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c421b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_feats in dL:\n",
    "    \n",
    "    #train_loss = model_step(bb_dict, noised_bb, tv, ss, gm, gu)\n",
    "    pass\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d01f4db0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['chain_idx', 'residue_index', 'res_mask', 'CA', 'N_CA', 'C_CA', 'CA_noised', 'N_CA_noised', 'C_CA_noised', 't', 'score_scale'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_feats.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "567bc93f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_feats['chain_idx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a32fea27",
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda'\n",
    "batch_feats= tree.map_structure(\n",
    "                lambda x: x.to(device), batch_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3436aec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 3])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_feats['CA'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6ab04e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d46c428",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = batch_feats['CA'].shape[1]\n",
    "CA_t  = batch_feats['CA']#.reshape(B, L, 3)\n",
    "NC_t = CA_t +  batch_feats['N_CA']#.reshape(B, L, 3)#not mult earlier #th\n",
    "CC_t = CA_t +  batch_feats['C_CA']#.reshape(B, L, 3)#not mult earlier\n",
    "true =  torch.cat((NC_t,CA_t,CC_t),dim=2).reshape(B,L,3,3)\n",
    "\n",
    "CA_n  = batch_feats['CA_noised'].reshape(B, L, 3)#.to('cuda')\n",
    "NC_n = CA_n + batch_feats['N_CA_noised'].reshape(B, L, 3)#.to('cuda')\n",
    "CC_n = CA_n + batch_feats['C_CA_noised'].reshape(B, L, 3)#.to('cuda')\n",
    "noise_xyz =  torch.cat((NC_n,CA_n,CC_n),dim=2).reshape(B,L,3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34a6350",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_step(batch_feats, train=True):\n",
    "    CA_t  = backbone_dict['CA'].reshape(B, L, 3).to('cuda')\n",
    "    NC_t = CA_t + backbone_dict['N_CA'].reshape(B, L, 3).to('cuda')#not mult earlier #th\n",
    "    CC_t = CA_t + backbone_dict['C_CA'].reshape(B, L, 3).to('cuda')#not mult earlier\n",
    "    true =  torch.cat((NC_t,CA_t,CC_t),dim=2).reshape(B,L,3,3)\n",
    "    \n",
    "    CA_n  = noised_dict['CA'].reshape(B, L, 3).to('cuda')\n",
    "    NC_n = CA_n + noised_dict['N_CA'].reshape(B, L, 3).to('cuda')\n",
    "    CC_n = CA_n + noised_dict['C_CA'].reshape(B, L, 3).to('cuda')\n",
    "    noise_xyz =  torch.cat((NC_n,CA_n,CC_n),dim=2).reshape(B,L,3,3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c08f2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_step(backbone_dict, noised_dict, batched_t, scores_scales, graph_maker, graph_unet, train=True):\n",
    "    \n",
    "    \n",
    "    CA_t  = backbone_dict['CA'].reshape(B, L, 3).to('cuda')\n",
    "    NC_t = CA_t + backbone_dict['N_CA'].reshape(B, L, 3).to('cuda')#not mult earlier #th\n",
    "    CC_t = CA_t + backbone_dict['C_CA'].reshape(B, L, 3).to('cuda')#not mult earlier\n",
    "    true =  torch.cat((NC_t,CA_t,CC_t),dim=2).reshape(B,L,3,3)\n",
    "    \n",
    "    CA_n  = noised_dict['CA'].reshape(B, L, 3).to('cuda')\n",
    "    NC_n = CA_n + noised_dict['N_CA'].reshape(B, L, 3).to('cuda')\n",
    "    CC_n = CA_n + noised_dict['C_CA'].reshape(B, L, 3).to('cuda')\n",
    "    noise_xyz =  torch.cat((NC_n,CA_n,CC_n),dim=2).reshape(B,L,3,3)\n",
    "    \n",
    "#     #rot mask for high T\n",
    "#     rot_t_max = 0.2\n",
    "#     t_mask = (batched_t>rot_t_max).to('cpu')\n",
    "#     noised_dict['N_CA'][t_mask] = backbone_dict['N_CA'].reshape(B, L,1, 3)[t_mask]\n",
    "#     noised_dict['C_CA'][t_mask] = backbone_dict['C_CA'].reshape(B, L,1, 3)[t_mask]\n",
    "    \n",
    "    x = graph_maker.prep_for_network(noised_dict)\n",
    "    out = graph_unet(x, batched_t)\n",
    "    CA_p = out['1'][:,0,:].reshape(B, L, 3)+CA_n #translation of Calpha\n",
    "    Qs = out['1'][:,1,:] # rotation\n",
    "    Qs = Qs.unsqueeze(1).repeat((1,2,1))\n",
    "    Qs = torch.cat((torch.ones((B*L,2,1),device=Qs.device),Qs),dim=-1).reshape(B,L,2,4)\n",
    "    Qs = normQ(Qs)\n",
    "    Rs = Qs2Rs(Qs)\n",
    "    N_C_to_Rot = torch.cat((noised_dict['N_CA'].reshape(B, L, 3).to('cuda'),\n",
    "                            noised_dict['C_CA'].reshape(B, L, 3).to('cuda')),dim=2).reshape(B,L,2,1,3)\n",
    "\n",
    "    \n",
    "    \n",
    "    rot_vecs = einsum('bnkij,bnkhj->bnki',Rs, N_C_to_Rot)\n",
    "    \n",
    "\n",
    "    \n",
    "    NC_p = CA_p + rot_vecs[:,:,0,:]*N_CA_dist #remove bc who cares? me maybe\n",
    "    CC_p = CA_p + rot_vecs[:,:,1,:]*C_CA_dist #remove maybe\n",
    "\n",
    "    pred = torch.cat((NC_p,CA_p,CC_p),dim=2).reshape(B,L,3,3)\n",
    "    \n",
    "    tloss, loss = FAPE_loss(pred.unsqueeze(0), true, scores_scales)\n",
    "    #tloss, loss = FAPE_loss(pred.unsqueeze(0), true, scores_scales)\n",
    "    \n",
    "    return tloss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "se33",
   "language": "python",
   "name": "se33"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
