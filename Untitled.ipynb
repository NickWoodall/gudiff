{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f096334",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (774967704.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    python -m se3_transformer.runtime.training \\\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "TASK=homo\n",
    "python -m se3_transformer.runtime.training \\\n",
    "  --amp \"$AMP\" \\\n",
    "  --batch_size \"$BATCH_SIZE\" \\\n",
    "  --epochs \"$NUM_EPOCHS\" \\\n",
    "  --lr \"$LEARNING_RATE\" \\\n",
    "  --weight_decay \"$WEIGHT_DECAY\" \\\n",
    "  --use_layer_norm \\\n",
    "  --norm \\\n",
    "  --save_ckpt_path model_qm9.pth \\\n",
    "  --precompute_bases \\\n",
    "  --seed 42 \\\n",
    "  --task \"$TASK\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d150e37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93ffb278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch_geometric.nn.pool.asap.ASAPooling"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_geometric.nn.pool.ASAPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47f1fe02",
   "metadata": {},
   "outputs": [],
   "source": [
    "?torch_geometric.nn.pool.ASAPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c6f7971",
   "metadata": {},
   "outputs": [],
   "source": [
    "?torch_geometric.nn.pool.topk_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff35a8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pathlib\n",
    "from typing import List\n",
    "import dgl\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import collections.abc as container_abcs\n",
    "#from apex.optimizers import FusedAdam, FusedLAMB\n",
    "from torch.nn.modules.loss import _Loss\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.optim import Optimizer\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "from se3_transformer.data_loading import QM9DataModule\n",
    "from se3_transformer.model import SE3TransformerPooled\n",
    "from se3_transformer.model.fiber import Fiber\n",
    "from se3_transformer.runtime import gpu_affinity\n",
    "from se3_transformer.runtime.arguments import PARSER\n",
    "from se3_transformer.runtime.callbacks import QM9MetricCallback, QM9LRSchedulerCallback, BaseCallback, \\\n",
    "    PerformanceCallback\n",
    "from se3_transformer.runtime.inference import evaluate\n",
    "from se3_transformer.runtime.loggers import LoggerCollection, DLLogger, WandbLogger, Logger\n",
    "from se3_transformer.runtime.utils import to_cuda, get_local_rank, init_distributed, seed_everything, \\\n",
    "    using_tensor_cores, increase_l2_fetch_granularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0a0213b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split, DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dd8dadb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading data from cached files.\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "p = pathlib.Path('.data/')\n",
    "datamodule = QM9DataModule(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c133bced",
   "metadata": {},
   "outputs": [],
   "source": [
    "td = datamodule.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b23f510",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,c in enumerate(td):\n",
    "    gr, nf, ef, a= c\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b75b5e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.6686)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(gr.ndata['pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31fe73d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_relative_pos(graph_in: dgl.DGLGraph) -> torch.Tensor:\n",
    "    x = graph_in.ndata['pos']\n",
    "    src, dst = graph_in.edges()\n",
    "    rel_pos = x[dst] - x[src]\n",
    "    return rel_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76789469",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_pos = _get_relative_pos(gr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4655ab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8936, 3])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ea1ea26a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0019, -1.4342, -0.0966],\n",
       "        [ 1.0159,  0.3693,  0.1580],\n",
       "        [-0.4029,  0.4443, -0.9131],\n",
       "        ...,\n",
       "        [-0.7653,  0.4699, -0.6284],\n",
       "        [ 0.2170,  0.8194, -0.6725],\n",
       "        [ 0.0027,  0.8138,  0.7131]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b5fa834a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.8049)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(rel_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d548c315",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = rel_pos.norm(dim=-1,keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c9f3891e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8936, 1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7cefa084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.8065)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3e5d08b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.4375],\n",
       "        [1.0924],\n",
       "        [1.0925],\n",
       "        ...,\n",
       "        [1.0961],\n",
       "        [1.0820],\n",
       "        [1.0820]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6761a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make midpoint zero\n",
    "def get_midpoint(ep_in):\n",
    "    \"\"\"Get midpoints of input endpoints, 2 points per helix\"\"\"\n",
    "    #calculate midpoint\n",
    "    midpoint = ep_in.sum(axis=1)/np.repeat(ep_in.shape[1], ep_in.shape[2])\n",
    "    \n",
    "    return midpoint\n",
    "\n",
    "rr = np.load('data/ep_for_gmp.npz')\n",
    "ep = [rr[f] for f in rr.files][0]\n",
    "mp = get_midpoint(ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27336df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphDataset(Dataset):\n",
    "    def __init__(self, ep_file : pathlib.Path):\n",
    "        self.data_path = ep_file\n",
    "        rr = np.load(data_path)\n",
    "        ep = [rr[f] for f in rr.files][0]\n",
    "        \n",
    "        v1 = np.arange(ep.shape[1]-1) #vertex 1 of edges in chronological order\n",
    "        v2 = np.arange(1,ep.shape[1]) #vertex 2 of edges in chronological order\n",
    "\n",
    "        ss = np.zeros(len(v1))\n",
    "        ss[np.arange(ss.shape[0])%2==0]=1  #alternate 0,1 for helix, loop, helix, etc\n",
    "        ss = ss[:,None] #unsqueeze\n",
    "\n",
    "        #positional encoding of node\n",
    "        embed_dim = 8\n",
    "        scale = 10\n",
    "        i_array = np.arange(1,(embed_dim/2)+1)\n",
    "        wk = (1/(scale**(i_array*2/embed_dim)))\n",
    "        t_array = np.arange(ep.shape[1])\n",
    "        si = torch.tensor(np.sin(wk*t_array.reshape((-1,1))))\n",
    "        ci = torch.tensor(np.cos(wk*t_array.reshape((-1,1))))\n",
    "        pe = torch.stack((si,ci),axis=2).reshape(ep.shape[1],embed_dim)\n",
    "\n",
    "        graphList = []\n",
    "\n",
    "        for i,c in enumerate(ep):\n",
    "\n",
    "            g = dgl.graph((v1,v2))\n",
    "            g.ndata['pos'] = torch.tensor(c)\n",
    "            g.edata['ss'] = torch.tensor(ss)\n",
    "            g.ndata['pe'] = pe\n",
    "\n",
    "            graphList.append(g)\n",
    "        \n",
    "        self.graphList = graphList\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graphList)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return graphList[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a8793bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dgl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_relative_pos\u001b[39m(graph_in: \u001b[43mdgl\u001b[49m\u001b[38;5;241m.\u001b[39mDGLGraph) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m      2\u001b[0m     x \u001b[38;5;241m=\u001b[39m graph_in\u001b[38;5;241m.\u001b[39mndata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpos\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      3\u001b[0m     src, dst \u001b[38;5;241m=\u001b[39m graph_in\u001b[38;5;241m.\u001b[39medges()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dgl' is not defined"
     ]
    }
   ],
   "source": [
    "def _get_relative_pos(graph_in: dgl.DGLGraph) -> torch.Tensor:\n",
    "    x = graph_in.ndata['pos']\n",
    "    src, dst = graph_in.edges()\n",
    "    rel_pos = x[dst] - x[src]\n",
    "    return rel_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4eae4fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pathlib' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mHGenDataModule\u001b[39;00m():\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    Datamodule wrapping hGen data set. 8 Helical endpoints defining a four helix protein.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m#8 long positional encoding\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 10\u001b[0m, in \u001b[0;36mHGenDataModule\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m NODE_FEATURE_DIM \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[1;32m      7\u001b[0m EDGE_FEATURE_DIM \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;66;03m# 0 or 1 helix or loop\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m---> 10\u001b[0m              data_dir: \u001b[43mpathlib\u001b[49m\u001b[38;5;241m.\u001b[39mPath, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_dir \u001b[38;5;241m=\u001b[39m data_dir \n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgds \u001b[38;5;241m=\u001b[39m DataLoader(GraphDataset(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_dir),batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collate)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pathlib' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "class HGenDataModule():\n",
    "    \"\"\"\n",
    "    Datamodule wrapping hGen data set. 8 Helical endpoints defining a four helix protein.\n",
    "    \"\"\"\n",
    "    #8 long positional encoding\n",
    "    NODE_FEATURE_DIM = 8\n",
    "    EDGE_FEATURE_DIM = 1 # 0 or 1 helix or loop\n",
    "\n",
    "    def __init__(self,\n",
    "                 data_dir: pathlib.Path, batch_size=32):\n",
    "        \n",
    "        self.data_dir = data_dir \n",
    "        self.gds = DataLoader(GraphDataset(self.data_dir),batch_size=32, shuffle=True, collate_fn=self._collate)\n",
    "\n",
    "        \n",
    "        \n",
    "    def _collate(self, graphs):\n",
    "        batched_graph = dgl.batch(graphs)\n",
    "        #reshape that batched graph to redivide into the individual graphs\n",
    "        edge_feats = {'0': batched_graph.edata['ss'][:, :self.EDGE_FEATURE_DIM, None]}\n",
    "        batched_graph.edata['rel_pos'] = _get_relative_pos(batched_graph)\n",
    "        # get node features\n",
    "        node_feats = {'0': batched_graph.ndata['pe'][:, :self.NODE_FEATURE_DIM, None]}\n",
    "        \n",
    "        return (batched_graph, node_feats, edge_feats, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "fa3e1ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = pathlib.Path('data/ep_for_gmp.npz')\n",
    "dm =  HGenDataModule(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "3c72894b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Graph(num_nodes=256, num_edges=224,\n",
      "      ndata_schemes={'pos': Scheme(shape=(3,), dtype=torch.float64), 'pe': Scheme(shape=(8,), dtype=torch.float64)}\n",
      "      edata_schemes={'ss': Scheme(shape=(1,), dtype=torch.float64), 'rel_pos': Scheme(shape=(3,), dtype=torch.float64)}), {'0': tensor([[[ 0.0000],\n",
      "         [ 1.0000],\n",
      "         [ 0.0000],\n",
      "         ...,\n",
      "         [ 1.0000],\n",
      "         [ 0.0000],\n",
      "         [ 1.0000]],\n",
      "\n",
      "        [[ 0.5332],\n",
      "         [ 0.8460],\n",
      "         [ 0.3110],\n",
      "         ...,\n",
      "         [ 0.9842],\n",
      "         [ 0.0998],\n",
      "         [ 0.9950]],\n",
      "\n",
      "        [[ 0.9021],\n",
      "         [ 0.4315],\n",
      "         [ 0.5911],\n",
      "         ...,\n",
      "         [ 0.9374],\n",
      "         [ 0.1987],\n",
      "         [ 0.9801]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.3239],\n",
      "         [-0.9461],\n",
      "         [ 0.9999],\n",
      "         ...,\n",
      "         [ 0.6301],\n",
      "         [ 0.4794],\n",
      "         [ 0.8776]],\n",
      "\n",
      "        [[-0.2304],\n",
      "         [-0.9731],\n",
      "         [ 0.9471],\n",
      "         ...,\n",
      "         [ 0.4828],\n",
      "         [ 0.5646],\n",
      "         [ 0.8253]],\n",
      "\n",
      "        [[-0.7137],\n",
      "         [-0.7004],\n",
      "         [ 0.8004],\n",
      "         ...,\n",
      "         [ 0.3203],\n",
      "         [ 0.6442],\n",
      "         [ 0.7648]]], dtype=torch.float64)}, {'0': tensor([[[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]]], dtype=torch.float64)}, None)\n"
     ]
    }
   ],
   "source": [
    "for i,c in enumerate(dm.gds):\n",
    "    print(c)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gudiff",
   "language": "python",
   "name": "gudiff"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
