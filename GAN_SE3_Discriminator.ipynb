{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c740047",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import logging\n",
    "import pathlib\n",
    "from typing import List\n",
    "import dgl\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import collections.abc as container_abcs\n",
    "#from apex.optimizers import FusedAdam, FusedLAMB\n",
    "from torch.nn.modules.loss import _Loss\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.optim import Optimizer\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "from se3_transformer.data_loading import QM9DataModule\n",
    "from se3_transformer.model import SE3TransformerPooled\n",
    "from se3_transformer.model.fiber import Fiber\n",
    "#from se3_transformer.runtime import gpu_affinity\n",
    "#from se3_transformer.runtime.arguments import PARSER\n",
    "#from se3_transformer.runtime.callbacks import QM9MetricCallback, QM9LRSchedulerCallback, BaseCallback, \\\n",
    "    #PerformanceCallback\n",
    "#from se3_transformer.runtime.inference import evaluate\n",
    "#from se3_transformer.runtime.loggers import LoggerCollection, DLLogger, WandbLogger, Logger\n",
    "from se3_transformer.runtime.utils import to_cuda, get_local_rank, init_distributed, seed_everything, \\\n",
    "    using_tensor_cores, increase_l2_fetch_granularity\n",
    "#import helix.helix_bb as hh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc7e5ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split, DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57dba110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_midpoint(ep_in):\n",
    "    \"\"\"Get midpoints of input endpoints, 2 points per helix\"\"\"\n",
    "    \n",
    "    #calculate midpoint\n",
    "    midpoint = ep_in.sum(axis=1)/np.repeat(ind_ep.shape[1], ind_ep.shape[2])\n",
    "    \n",
    "    return midpoint\n",
    "\n",
    "\n",
    "\n",
    "def normalize_pc(points):\n",
    "    \"\"\"Center at Zero Divide furtherst points\"\"\"\n",
    "    centroid = np.mean(points, axis=0)\n",
    "    points -= centroid\n",
    "    furthest_distance = np.max(np.sqrt(np.sum(abs(points)**2,axis=-1)))\n",
    "    points /= furthest_distance\n",
    "\n",
    "    return points, furthest_distance\n",
    "    \n",
    "def make_pe_encoding(i_pos=8, embed_dim = 8, scale = 10, cast_type=torch.float32):\n",
    "    #positional encoding of node\n",
    "    i_array = np.arange(1,(embed_dim/2)+1)\n",
    "    wk = (1/(scale**(i_array*2/embed_dim)))\n",
    "    t_array = np.arange(i_pos)\n",
    "    si = torch.tensor(np.sin(wk*t_array.reshape((-1,1))))\n",
    "    ci = torch.tensor(np.cos(wk*t_array.reshape((-1,1))))\n",
    "    pe = torch.stack((si,ci),axis=2).reshape(t_array.shape[0],embed_dim).type(cast_type)\n",
    "    return pe\n",
    "\n",
    "\n",
    "def make_graph_struct(batch_size=32, n_nodes = 8):\n",
    "    # make a fake graph to be filled with generator outputs\n",
    "    \n",
    "    v1 = np.arange(n_nodes-1) #vertex 1 of edges in chronological order\n",
    "    v2 = np.arange(1,n_nodes) #vertex 2 of edges in chronological order\n",
    "\n",
    "    ss = np.zeros(len(v1),dtype=np.int32)\n",
    "    ss[np.arange(ss.shape[0])%2==0]=1  #alternate 0,1 for helix, loop, helix, etc\n",
    "    ss = ss[:,None] #unsqueeze\n",
    "    \n",
    "    pe = make_pe_encoding(i_pos=8, embed_dim = 8, scale = 10, cast_type=torch.float32)\n",
    "\n",
    "    graphList = []\n",
    "    for i in range(batch_size):\n",
    "        g = dgl.graph((v1,v2))\n",
    "        g.edata['ss'] = torch.tensor(ss,dtype=torch.float32)\n",
    "        g.ndata['pe'] = pe\n",
    "\n",
    "        graphList.append(g)\n",
    "\n",
    "    batched_graph = dgl.batch(graphList)\n",
    "\n",
    "    return batched_graph\n",
    "\n",
    "\n",
    "class GraphDataset(Dataset):\n",
    "    def __init__(self, ep_file : pathlib.Path, limit=1000):\n",
    "        self.data_path = ep_file\n",
    "        rr = np.load(self.data_path)\n",
    "        ep = [rr[f] for f in rr.files][0][:1000]\n",
    "        \n",
    "        #need to save furthest distance to regen later\n",
    "        #maybe consider small change for next steps\n",
    "        ep, self.furthest_distance = normalize_pc(ep.reshape((-1,3)))\n",
    "        self.ep = ep.reshape((-1,8,3))\n",
    "        \n",
    "        \n",
    "        v1 = np.arange(self.ep.shape[1]-1) #vertex 1 of edges in chronological order\n",
    "        v2 = np.arange(1,self.ep.shape[1]) #vertex 2 of edges in chronological order\n",
    "\n",
    "        ss = np.zeros(len(v1))\n",
    "        ss[np.arange(ss.shape[0])%2==0]=1  #alternate 0,1 for helix, loop, helix, etc\n",
    "        ss = ss[:,None] #unsqueeze\n",
    "\n",
    "        #positional encoding of node\n",
    "        pe = make_pe_encoding(i_pos=8, embed_dim = 8, scale = 10, cast_type=torch.float32)\n",
    "\n",
    "        graphList = []\n",
    "\n",
    "        for i,c in enumerate(self.ep):\n",
    "\n",
    "            g = dgl.graph((v1,v2))\n",
    "            g.ndata['pos'] = torch.tensor(c,dtype=torch.float32)\n",
    "            g.edata['ss'] = torch.tensor(ss,dtype=torch.float32)\n",
    "            g.ndata['pe'] = pe\n",
    "\n",
    "            graphList.append(g)\n",
    "        \n",
    "        self.graphList = graphList\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graphList)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.graphList[idx]\n",
    "\n",
    "    \n",
    "class HGenDataModule():\n",
    "    \"\"\"\n",
    "    Datamodule wrapping hGen data set. 8 Helical endpoints defining a four helix protein.\n",
    "    \"\"\"\n",
    "    #8 long positional encoding\n",
    "    NODE_FEATURE_DIM = 8\n",
    "    EDGE_FEATURE_DIM = 1 # 0 or 1 helix or loop\n",
    "\n",
    "    def __init__(self,\n",
    "                 data_dir: pathlib.Path, batch_size=32):\n",
    "        \n",
    "        self.data_dir = data_dir \n",
    "        self.GraphDatasetObj = GraphDataset(self.data_dir)\n",
    "        self.gds = DataLoader(self.GraphDatasetObj,batch_size=batch_size, shuffle=True, drop_last=True,\n",
    "                              collate_fn=self._collate)\n",
    "        \n",
    "    \n",
    "        \n",
    "    def _collate(self, graphs):\n",
    "        batched_graph = dgl.batch(graphs)\n",
    "        #reshape that batched graph to redivide into the individual graphs\n",
    "        edge_feats = {'0': batched_graph.edata['ss'][:, :self.EDGE_FEATURE_DIM, None]}\n",
    "        batched_graph.edata['rel_pos'] = _get_relative_pos(batched_graph)\n",
    "        # get node features\n",
    "        node_feats = {'0': batched_graph.ndata['pe'][:, :self.NODE_FEATURE_DIM, None]}\n",
    "        \n",
    "        return (batched_graph, node_feats, edge_feats)\n",
    "    \n",
    "def eval_gen(batch_size=8,z=12):\n",
    "    \n",
    "    in_z = torch.randn((batch_size,z), device='cuda',dtype = torch.float32)\n",
    "    out = hg(in_z)*31\n",
    "    out = out.reshape((-1,8,3)).detach().cpu().numpy()\n",
    "    \n",
    "    return eval_endpoints(out)\n",
    "    \n",
    "    \n",
    "\n",
    "def eval_endpoints(ep_in): \n",
    "    \n",
    "    ep = ep_in.reshape((-1,8,3))\n",
    "\n",
    "    v1 = np.arange(ep.shape[1]-1) #vertex 1 of edges in chronological order\n",
    "    v2 = np.arange(1,ep.shape[1]) #vertex 2 of edges in chronological order\n",
    "\n",
    "    hLL = np.linalg.norm(ep[:,v1]-ep[:,v2],axis=2)\n",
    "\n",
    "    hLoc = np.array([0,2,4,6])\n",
    "    lLoc = np.array([1,3,5])\n",
    "\n",
    "    return np.mean(hLL[:,hLoc]), np.mean(hLL[:,lLoc])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed98db5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28455a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#num channels relates to self interaction of features of the same degree on the same node (1x1 convolution)\n",
    "#learnable skip connections, since nodes don't attend to themselves nedded\n",
    "\n",
    "\n",
    "#--num_degrees: Number of degrees to use. Hidden features will have types [0, ..., num_degrees - 1] (default: 4)\n",
    "#so num degrees is 3?\n",
    "# Fiber\n",
    "\n",
    "# A fiber can be viewed as a representation of a set of features of different types or degrees (positive integers), where each feature type transforms according to its rule.\n",
    "# In this repository, a fiber can be seen as a dictionary with degrees as keys and numbers of channels as values.\n",
    "\n",
    "#Edge feature dimension does not include rel_pos which is concatenated per forward pass\n",
    "\n",
    "#I believe channels needs to equal degrees times heads to match expansion by heads\n",
    "#with the self atention by channels \n",
    "def to_detach(x):\n",
    "    \"\"\" Try to convert a Tensor, a collection of Tensors or a DGLGraph to CUDA \"\"\"\n",
    "    if isinstance(x, Tensor):\n",
    "        return x.detach()\n",
    "    elif isinstance(x, tuple):\n",
    "        return (to_detach(v) for v in x)\n",
    "    elif isinstance(x, list):\n",
    "        return [to_detach(v) for v in x]\n",
    "    elif isinstance(x, dict):\n",
    "        return {k: to_detach(v) for k, v in x.items()}\n",
    "    else:\n",
    "        # DGLGraph or other objects\n",
    "        return x\n",
    "def _get_relative_pos(graph_in: dgl.DGLGraph) -> torch.Tensor:\n",
    "    x = graph_in.ndata['pos']\n",
    "    src, dst = graph_in.edges()\n",
    "    rel_pos = x[dst] - x[src]\n",
    "    return rel_pos\n",
    "\n",
    "class helixGen(nn.Module):\n",
    "    def __init__(self, input_z=12, hidden=64, output=24):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_z, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, output),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_xyz = self.linear_relu_stack(x)\n",
    "        return out_xyz.reshape((-1,3))\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "num_channels = 16\n",
    "num_degrees = 4\n",
    "\n",
    "kwargs = dict()\n",
    "kwargs['pooling'] = 'max'\n",
    "kwargs['num_layers'] = 4\n",
    "kwargs['num_heads'] = 8\n",
    "kwargs['channels_div'] =torch.tensor(2,dtype=torch.int32)\n",
    "\n",
    "#channels dive = channels/num_heads\n",
    "\n",
    "\n",
    "dm = HGenDataModule(pathlib.Path('data/ep_for_gmp.npz'),batch_size=batch_size)\n",
    "\n",
    "\n",
    "model = SE3TransformerPooled(\n",
    "        fiber_in=Fiber({0: dm.NODE_FEATURE_DIM}),\n",
    "        fiber_out=Fiber({0: num_degrees * num_channels}),\n",
    "        fiber_edge=Fiber({0: dm.EDGE_FEATURE_DIM}),\n",
    "        output_dim=1,\n",
    "        tensor_cores=False,\n",
    "        num_degrees=num_degrees,\n",
    "        num_channels=num_channels,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "device=torch.cuda.current_device()\n",
    "\n",
    "hg = helixGen()\n",
    "hg.to(torch.float32)\n",
    "hg.to(device)\n",
    "\n",
    "model.to(device)\n",
    "model.to(torch.float32)\n",
    "loss_fn = nn.BCEWithLogitsLoss().to(device)\n",
    "#loss_fn = nn.MSELoss()\n",
    "d_opt = torch.optim.SGD(model.parameters(), lr=0.001, weight_decay=0.001)\n",
    "g_opt = torch.optim.SGD(hg.parameters(), lr=0.001, weight_decay=0.001)\n",
    "batched_graph = make_graph_struct(batch_size=batch_size)\n",
    "batched_graph = to_cuda(batched_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db5068a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(input_z, input_real):\n",
    "    \n",
    "    batched_graph_real, node_feats_real, edge_feats_real = input_real\n",
    "    batched_graph_real = to_cuda(batched_graph_real)\n",
    "    node_feats_real = to_cuda(node_feats_real)\n",
    "    edge_feats_real = to_cuda(edge_feats_real)\n",
    "    \n",
    "    model.zero_grad()\n",
    "    hg.zero_grad()\n",
    "    #----------------compute the generators loss---------------------------\n",
    "    outpos = hg(input_z)\n",
    "\n",
    "    batched_graph.ndata['pos'] = outpos\n",
    "    batched_graph.edata['rel_pos'] = _get_relative_pos(batched_graph)\n",
    "    #these node and edges feats are static, the s03 transformer is order independent\n",
    "    #right now they just denote position\n",
    "    g_fake_out = model(batched_graph, node_feats_real, edge_feats_real, compute_gradients=True) \n",
    "    real_fake_targets = torch.ones(g_fake_out.shape[0],dtype=torch.float32, device='cuda')\n",
    "    g_loss = loss_fn(g_fake_out, real_fake_targets)\n",
    "    \n",
    "    #retain_graph=True\n",
    "    g_loss.backward()\n",
    "    g_opt.step()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #----------compute the discrimanators loss\n",
    "    model.zero_grad()\n",
    "    d_real_out = model(batched_graph_real, node_feats_real, edge_feats_real)\n",
    "    real_targets = torch.ones(d_real_out.shape[0],dtype=torch.float32, device='cuda')\n",
    "    d_loss_real = loss_fn(d_real_out, real_targets)\n",
    "    \n",
    "    batched_graph.edata['rel_pos'] = batched_graph.edata['rel_pos'].detach()\n",
    "    \n",
    "    d_fake_out = model(batched_graph, node_feats_real, edge_feats_real)\n",
    "    # I believe we can detach the input batch_graph position here, and not use retain_graph equals true\n",
    "    #also fix repeated used of node_feats_real, edge_feats_real\n",
    "    fake_targets = torch.zeros(d_fake_out.shape[0],dtype=torch.float32, device='cuda')\n",
    "    d_loss_fake = loss_fn(d_fake_out, fake_targets)\n",
    "    \n",
    "    d_loss = d_loss_real + d_loss_fake\n",
    "    \n",
    "    d_loss.backward()\n",
    "    d_opt.step()\n",
    "    \n",
    "    \n",
    "    #save probs here\n",
    "    #d_loss.detach(), g_loss.detach(),\n",
    "    \n",
    "    return  g_loss.detach(), d_loss.detach(), g_fake_out.detach(), d_fake_out.detach(), d_real_out.detach(), \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c834bbfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.20556601787868"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.GraphDatasetObj.furthest_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ecb584b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.GraphDatasetObj.ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c6ba7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eval_endpoints(dm.GraphDatasetObj.ep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea291cf2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nwoodall/miniconda3/envs/se33/lib/python3.9/site-packages/dgl/backend/pytorch/tensor.py:449: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 |  ET 0.28 min AvgLosses >> G/D 1.027/1.024 D Real :0.506 D Fake :0.507 Length EvaL (8.068716, 10.25965)\n",
      "Epoch 001 |  ET 0.31 min AvgLosses >> G/D 1.021/1.018 D Real :0.506 D Fake :0.505 Length EvaL (8.63751, 10.347848)\n",
      "Epoch 002 |  ET 0.34 min AvgLosses >> G/D 1.012/1.010 D Real :0.507 D Fake :0.510 Length EvaL (7.9941826, 10.26891)\n",
      "Epoch 003 |  ET 0.36 min AvgLosses >> G/D 1.003/1.005 D Real :0.511 D Fake :0.509 Length EvaL (8.932562, 11.068898)\n",
      "Epoch 004 |  ET 0.39 min AvgLosses >> G/D 1.000/0.997 D Real :0.509 D Fake :0.511 Length EvaL (8.0107355, 10.384983)\n",
      "Epoch 005 |  ET 0.41 min AvgLosses >> G/D 0.994/0.995 D Real :0.511 D Fake :0.508 Length EvaL (8.7419195, 10.533969)\n",
      "Epoch 006 |  ET 0.44 min AvgLosses >> G/D 0.991/0.989 D Real :0.510 D Fake :0.511 Length EvaL (8.523566, 10.109623)\n",
      "Epoch 007 |  ET 0.46 min AvgLosses >> G/D 0.988/0.985 D Real :0.508 D Fake :0.510 Length EvaL (9.120443, 10.897237)\n",
      "Epoch 008 |  ET 0.49 min AvgLosses >> G/D 0.986/0.986 D Real :0.510 D Fake :0.509 Length EvaL (8.558287, 10.562505)\n",
      "Epoch 009 |  ET 0.51 min AvgLosses >> G/D 0.981/0.984 D Real :0.510 D Fake :0.508 Length EvaL (8.72756, 10.6617775)\n",
      "Epoch 010 |  ET 0.54 min AvgLosses >> G/D 0.981/0.977 D Real :0.508 D Fake :0.511 Length EvaL (8.50391, 9.9673195)\n",
      "Epoch 011 |  ET 0.57 min AvgLosses >> G/D 0.983/0.977 D Real :0.504 D Fake :0.508 Length EvaL (8.901067, 10.460109)\n",
      "Epoch 012 |  ET 0.59 min AvgLosses >> G/D 0.979/0.979 D Real :0.506 D Fake :0.505 Length EvaL (8.786852, 10.67395)\n",
      "Epoch 013 |  ET 0.62 min AvgLosses >> G/D 0.978/0.975 D Real :0.506 D Fake :0.507 Length EvaL (8.884214, 10.739803)\n",
      "Epoch 014 |  ET 0.64 min AvgLosses >> G/D 0.977/0.975 D Real :0.504 D Fake :0.505 Length EvaL (9.260862, 10.560022)\n",
      "Epoch 015 |  ET 0.67 min AvgLosses >> G/D 0.975/0.970 D Real :0.504 D Fake :0.506 Length EvaL (9.474781, 10.829037)\n",
      "Epoch 016 |  ET 0.69 min AvgLosses >> G/D 0.979/0.974 D Real :0.501 D Fake :0.504 Length EvaL (9.357432, 10.804576)\n",
      "Epoch 017 |  ET 0.72 min AvgLosses >> G/D 0.979/0.972 D Real :0.499 D Fake :0.504 Length EvaL (9.5202265, 11.15475)\n",
      "Epoch 018 |  ET 0.75 min AvgLosses >> G/D 0.979/0.976 D Real :0.498 D Fake :0.500 Length EvaL (9.478054, 10.682632)\n",
      "Epoch 019 |  ET 0.77 min AvgLosses >> G/D 0.973/0.970 D Real :0.502 D Fake :0.503 Length EvaL (9.436256, 10.624261)\n",
      "Epoch 020 |  ET 0.80 min AvgLosses >> G/D 0.967/0.964 D Real :0.506 D Fake :0.511 Length EvaL (9.9649315, 10.777862)\n",
      "Epoch 021 |  ET 0.82 min AvgLosses >> G/D 0.971/0.966 D Real :0.504 D Fake :0.505 Length EvaL (10.040313, 11.129083)\n",
      "Epoch 022 |  ET 0.85 min AvgLosses >> G/D 0.969/0.968 D Real :0.503 D Fake :0.503 Length EvaL (10.169096, 11.092174)\n",
      "Epoch 023 |  ET 0.87 min AvgLosses >> G/D 0.964/0.964 D Real :0.507 D Fake :0.508 Length EvaL (10.631583, 11.044841)\n",
      "Epoch 024 |  ET 0.90 min AvgLosses >> G/D 0.964/0.965 D Real :0.506 D Fake :0.509 Length EvaL (10.812679, 11.315808)\n",
      "Epoch 025 |  ET 0.93 min AvgLosses >> G/D 0.956/0.957 D Real :0.513 D Fake :0.514 Length EvaL (11.324539, 11.858661)\n",
      "Epoch 026 |  ET 0.96 min AvgLosses >> G/D 0.954/0.954 D Real :0.518 D Fake :0.517 Length EvaL (11.392193, 11.460704)\n",
      "Epoch 027 |  ET 0.98 min AvgLosses >> G/D 0.948/0.955 D Real :0.524 D Fake :0.515 Length EvaL (11.589833, 11.307782)\n",
      "Epoch 028 |  ET 1.01 min AvgLosses >> G/D 0.949/0.948 D Real :0.519 D Fake :0.522 Length EvaL (12.002235, 12.160278)\n",
      "Epoch 029 |  ET 1.03 min AvgLosses >> G/D 0.941/0.942 D Real :0.526 D Fake :0.525 Length EvaL (12.024544, 11.538834)\n",
      "Epoch 030 |  ET 1.06 min AvgLosses >> G/D 0.946/0.941 D Real :0.522 D Fake :0.527 Length EvaL (12.329988, 11.602538)\n",
      "Epoch 031 |  ET 1.09 min AvgLosses >> G/D 0.942/0.938 D Real :0.524 D Fake :0.527 Length EvaL (12.678423, 12.37056)\n",
      "Epoch 032 |  ET 1.11 min AvgLosses >> G/D 0.949/0.939 D Real :0.517 D Fake :0.524 Length EvaL (12.850592, 11.8157625)\n",
      "Epoch 033 |  ET 1.14 min AvgLosses >> G/D 0.952/0.947 D Real :0.510 D Fake :0.513 Length EvaL (12.798458, 11.937206)\n",
      "Epoch 034 |  ET 1.16 min AvgLosses >> G/D 0.950/0.951 D Real :0.509 D Fake :0.508 Length EvaL (13.234437, 11.844082)\n",
      "Epoch 035 |  ET 1.19 min AvgLosses >> G/D 0.952/0.948 D Real :0.507 D Fake :0.509 Length EvaL (13.286511, 11.828067)\n",
      "Epoch 036 |  ET 1.22 min AvgLosses >> G/D 0.944/0.943 D Real :0.512 D Fake :0.512 Length EvaL (13.391651, 11.476481)\n",
      "Epoch 037 |  ET 1.24 min AvgLosses >> G/D 0.943/0.942 D Real :0.512 D Fake :0.513 Length EvaL (13.620066, 12.091957)\n",
      "Epoch 038 |  ET 1.27 min AvgLosses >> G/D 0.939/0.942 D Real :0.514 D Fake :0.512 Length EvaL (13.973131, 11.708493)\n",
      "Epoch 039 |  ET 1.30 min AvgLosses >> G/D 0.942/0.939 D Real :0.512 D Fake :0.514 Length EvaL (13.880165, 11.605945)\n",
      "Epoch 040 |  ET 1.32 min AvgLosses >> G/D 0.943/0.955 D Real :0.512 D Fake :0.504 Length EvaL (14.675141, 11.896945)\n",
      "Epoch 041 |  ET 1.35 min AvgLosses >> G/D 0.944/0.951 D Real :0.511 D Fake :0.507 Length EvaL (14.7987175, 11.721576)\n",
      "Epoch 042 |  ET 1.38 min AvgLosses >> G/D 0.945/0.947 D Real :0.510 D Fake :0.510 Length EvaL (14.942357, 11.231141)\n",
      "Epoch 043 |  ET 1.40 min AvgLosses >> G/D 0.955/0.936 D Real :0.507 D Fake :0.519 Length EvaL (15.353103, 11.447563)\n",
      "Epoch 044 |  ET 1.43 min AvgLosses >> G/D 0.941/0.952 D Real :0.517 D Fake :0.511 Length EvaL (15.327355, 11.1202345)\n",
      "Epoch 045 |  ET 1.46 min AvgLosses >> G/D 0.956/0.954 D Real :0.509 D Fake :0.512 Length EvaL (15.554808, 11.075864)\n",
      "Epoch 046 |  ET 1.48 min AvgLosses >> G/D 0.951/0.956 D Real :0.515 D Fake :0.510 Length EvaL (15.407423, 10.240268)\n",
      "Epoch 047 |  ET 1.51 min AvgLosses >> G/D 0.943/0.956 D Real :0.520 D Fake :0.513 Length EvaL (16.268116, 10.89518)\n",
      "Epoch 048 |  ET 1.53 min AvgLosses >> G/D 0.951/0.949 D Real :0.518 D Fake :0.518 Length EvaL (16.297802, 10.331544)\n",
      "Epoch 049 |  ET 1.56 min AvgLosses >> G/D 0.958/0.947 D Real :0.512 D Fake :0.520 Length EvaL (17.17419, 10.352038)\n",
      "Epoch 050 |  ET 1.59 min AvgLosses >> G/D 0.949/0.968 D Real :0.519 D Fake :0.508 Length EvaL (16.906967, 10.2773485)\n",
      "Epoch 051 |  ET 1.61 min AvgLosses >> G/D 0.941/0.949 D Real :0.524 D Fake :0.520 Length EvaL (17.29459, 10.12551)\n",
      "Epoch 052 |  ET 1.64 min AvgLosses >> G/D 0.928/0.935 D Real :0.533 D Fake :0.529 Length EvaL (18.426323, 10.780441)\n",
      "Epoch 053 |  ET 1.66 min AvgLosses >> G/D 0.937/0.947 D Real :0.528 D Fake :0.520 Length EvaL (18.889124, 10.327137)\n",
      "Epoch 054 |  ET 1.69 min AvgLosses >> G/D 0.930/0.952 D Real :0.532 D Fake :0.521 Length EvaL (18.817354, 9.70058)\n",
      "Epoch 055 |  ET 1.72 min AvgLosses >> G/D 0.948/0.929 D Real :0.522 D Fake :0.534 Length EvaL (20.050938, 10.858335)\n",
      "Epoch 056 |  ET 1.74 min AvgLosses >> G/D 0.931/0.942 D Real :0.530 D Fake :0.522 Length EvaL (19.91167, 10.295256)\n",
      "Epoch 057 |  ET 1.77 min AvgLosses >> G/D 0.947/0.928 D Real :0.522 D Fake :0.537 Length EvaL (20.722527, 10.639494)\n",
      "Epoch 058 |  ET 1.80 min AvgLosses >> G/D 0.939/0.946 D Real :0.528 D Fake :0.521 Length EvaL (21.244934, 10.46355)\n",
      "Epoch 059 |  ET 1.82 min AvgLosses >> G/D 0.928/0.978 D Real :0.537 D Fake :0.506 Length EvaL (22.3042, 10.641357)\n",
      "Epoch 060 |  ET 1.85 min AvgLosses >> G/D 0.961/0.926 D Real :0.514 D Fake :0.543 Length EvaL (23.280308, 10.668647)\n",
      "Epoch 061 |  ET 1.88 min AvgLosses >> G/D 0.905/1.022 D Real :0.565 D Fake :0.484 Length EvaL (23.552633, 10.908381)\n",
      "Epoch 062 |  ET 1.91 min AvgLosses >> G/D 1.043/0.886 D Real :0.469 D Fake :0.591 Length EvaL (24.067865, 10.662009)\n",
      "Epoch 063 |  ET 1.93 min AvgLosses >> G/D 0.889/1.071 D Real :0.602 D Fake :0.454 Length EvaL (24.548649, 11.200771)\n",
      "Epoch 064 |  ET 1.96 min AvgLosses >> G/D 0.992/0.954 D Real :0.508 D Fake :0.531 Length EvaL (24.594004, 11.396491)\n",
      "Epoch 065 |  ET 1.99 min AvgLosses >> G/D 0.966/1.000 D Real :0.522 D Fake :0.507 Length EvaL (25.037949, 11.452553)\n",
      "Epoch 066 |  ET 2.02 min AvgLosses >> G/D 1.019/0.940 D Real :0.478 D Fake :0.553 Length EvaL (25.434002, 11.343572)\n",
      "Epoch 067 |  ET 2.04 min AvgLosses >> G/D 0.917/1.042 D Real :0.576 D Fake :0.464 Length EvaL (26.315887, 11.9991045)\n",
      "Epoch 068 |  ET 2.07 min AvgLosses >> G/D 0.947/1.051 D Real :0.539 D Fake :0.456 Length EvaL (25.41594, 11.981069)\n",
      "Epoch 069 |  ET 2.10 min AvgLosses >> G/D 1.046/0.953 D Real :0.469 D Fake :0.543 Length EvaL (25.362429, 11.927386)\n",
      "Epoch 070 |  ET 2.12 min AvgLosses >> G/D 0.972/1.063 D Real :0.514 D Fake :0.452 Length EvaL (26.579172, 12.206234)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 071 |  ET 2.15 min AvgLosses >> G/D 1.038/0.975 D Real :0.470 D Fake :0.523 Length EvaL (25.90562, 11.868195)\n",
      "Epoch 072 |  ET 2.17 min AvgLosses >> G/D 0.965/1.036 D Real :0.525 D Fake :0.468 Length EvaL (25.903322, 12.134791)\n",
      "Epoch 073 |  ET 2.20 min AvgLosses >> G/D 1.124/0.907 D Real :0.417 D Fake :0.580 Length EvaL (25.9601, 12.28291)\n",
      "Epoch 074 |  ET 2.23 min AvgLosses >> G/D 0.914/1.100 D Real :0.574 D Fake :0.429 Length EvaL (25.829723, 12.139056)\n",
      "Epoch 075 |  ET 2.25 min AvgLosses >> G/D 1.067/0.928 D Real :0.450 D Fake :0.559 Length EvaL (25.704674, 11.964114)\n",
      "Epoch 076 |  ET 2.28 min AvgLosses >> G/D 0.995/1.011 D Real :0.503 D Fake :0.489 Length EvaL (24.880882, 11.397389)\n",
      "Epoch 077 |  ET 2.30 min AvgLosses >> G/D 1.081/0.958 D Real :0.442 D Fake :0.529 Length EvaL (25.462776, 11.527448)\n",
      "Epoch 078 |  ET 2.33 min AvgLosses >> G/D 0.922/1.067 D Real :0.557 D Fake :0.451 Length EvaL (25.595926, 11.170032)\n",
      "Epoch 079 |  ET 2.36 min AvgLosses >> G/D 1.017/0.984 D Real :0.486 D Fake :0.505 Length EvaL (25.34115, 10.334653)\n",
      "Epoch 080 |  ET 2.38 min AvgLosses >> G/D 0.926/1.087 D Real :0.562 D Fake :0.444 Length EvaL (25.766989, 10.798537)\n",
      "Epoch 081 |  ET 2.41 min AvgLosses >> G/D 0.967/1.037 D Real :0.518 D Fake :0.470 Length EvaL (26.238224, 10.676133)\n",
      "Epoch 082 |  ET 2.44 min AvgLosses >> G/D 1.150/0.928 D Real :0.412 D Fake :0.561 Length EvaL (25.386717, 10.628344)\n",
      "Epoch 083 |  ET 2.46 min AvgLosses >> G/D 0.918/1.162 D Real :0.579 D Fake :0.402 Length EvaL (25.559427, 10.739545)\n",
      "Epoch 084 |  ET 2.49 min AvgLosses >> G/D 1.074/0.959 D Real :0.451 D Fake :0.529 Length EvaL (25.363522, 10.343453)\n",
      "Epoch 085 |  ET 2.52 min AvgLosses >> G/D 0.929/1.069 D Real :0.554 D Fake :0.450 Length EvaL (25.63646, 10.660012)\n",
      "Epoch 086 |  ET 2.55 min AvgLosses >> G/D 0.997/1.018 D Real :0.496 D Fake :0.488 Length EvaL (25.57563, 10.650863)\n",
      "Epoch 087 |  ET 2.57 min AvgLosses >> G/D 1.132/0.921 D Real :0.415 D Fake :0.570 Length EvaL (25.105099, 10.373875)\n",
      "Epoch 088 |  ET 2.60 min AvgLosses >> G/D 0.904/1.128 D Real :0.573 D Fake :0.416 Length EvaL (25.619122, 10.332157)\n",
      "Epoch 089 |  ET 2.63 min AvgLosses >> G/D 1.056/0.974 D Real :0.464 D Fake :0.508 Length EvaL (25.174667, 10.021431)\n",
      "Epoch 090 |  ET 2.65 min AvgLosses >> G/D 1.016/1.022 D Real :0.487 D Fake :0.482 Length EvaL (25.933907, 10.470677)\n",
      "Epoch 091 |  ET 2.68 min AvgLosses >> G/D 1.116/0.932 D Real :0.427 D Fake :0.546 Length EvaL (24.982586, 10.368685)\n",
      "Epoch 092 |  ET 2.71 min AvgLosses >> G/D 0.936/1.082 D Real :0.542 D Fake :0.450 Length EvaL (24.757196, 9.892825)\n",
      "Epoch 093 |  ET 2.74 min AvgLosses >> G/D 1.125/0.925 D Real :0.429 D Fake :0.552 Length EvaL (24.60199, 9.659858)\n",
      "Epoch 094 |  ET 2.76 min AvgLosses >> G/D 0.958/1.054 D Real :0.527 D Fake :0.467 Length EvaL (24.614601, 9.669372)\n",
      "Epoch 095 |  ET 2.79 min AvgLosses >> G/D 1.010/0.983 D Real :0.492 D Fake :0.512 Length EvaL (24.932615, 9.548864)\n",
      "Epoch 096 |  ET 2.82 min AvgLosses >> G/D 0.963/1.062 D Real :0.525 D Fake :0.468 Length EvaL (24.662266, 8.845738)\n",
      "Epoch 097 |  ET 2.84 min AvgLosses >> G/D 1.077/0.943 D Real :0.459 D Fake :0.539 Length EvaL (24.581308, 9.12007)\n",
      "Epoch 098 |  ET 2.87 min AvgLosses >> G/D 0.993/1.039 D Real :0.508 D Fake :0.479 Length EvaL (24.28986, 8.999366)\n",
      "Epoch 099 |  ET 2.89 min AvgLosses >> G/D 1.025/0.977 D Real :0.490 D Fake :0.518 Length EvaL (24.765392, 9.006052)\n",
      "Epoch 100 |  ET 2.92 min AvgLosses >> G/D 0.976/1.006 D Real :0.518 D Fake :0.503 Length EvaL (24.221931, 9.370095)\n",
      "Epoch 101 |  ET 2.95 min AvgLosses >> G/D 1.036/0.975 D Real :0.484 D Fake :0.522 Length EvaL (24.03702, 9.183806)\n",
      "Epoch 102 |  ET 2.97 min AvgLosses >> G/D 0.990/1.043 D Real :0.511 D Fake :0.484 Length EvaL (24.234892, 9.199074)\n",
      "Epoch 103 |  ET 3.00 min AvgLosses >> G/D 1.008/1.020 D Real :0.501 D Fake :0.496 Length EvaL (23.978085, 9.240245)\n",
      "Epoch 104 |  ET 3.03 min AvgLosses >> G/D 0.958/1.095 D Real :0.533 D Fake :0.460 Length EvaL (23.747337, 9.365985)\n",
      "Epoch 105 |  ET 3.05 min AvgLosses >> G/D 1.147/0.968 D Real :0.434 D Fake :0.531 Length EvaL (23.64226, 9.6357355)\n",
      "Epoch 106 |  ET 3.08 min AvgLosses >> G/D 0.983/1.089 D Real :0.519 D Fake :0.467 Length EvaL (23.15171, 9.492198)\n",
      "Epoch 107 |  ET 3.11 min AvgLosses >> G/D 1.064/0.957 D Real :0.480 D Fake :0.537 Length EvaL (23.942015, 10.257512)\n",
      "Epoch 108 |  ET 3.14 min AvgLosses >> G/D 0.953/1.103 D Real :0.549 D Fake :0.454 Length EvaL (23.568718, 9.811767)\n",
      "Epoch 109 |  ET 3.17 min AvgLosses >> G/D 1.091/1.019 D Real :0.467 D Fake :0.504 Length EvaL (23.68801, 9.963532)\n",
      "Epoch 110 |  ET 3.20 min AvgLosses >> G/D 0.968/1.141 D Real :0.541 D Fake :0.448 Length EvaL (23.33274, 9.850711)\n",
      "Epoch 111 |  ET 3.23 min AvgLosses >> G/D 1.093/1.025 D Real :0.471 D Fake :0.504 Length EvaL (23.3948, 9.609778)\n",
      "Epoch 112 |  ET 3.25 min AvgLosses >> G/D 0.975/1.125 D Real :0.531 D Fake :0.455 Length EvaL (24.136395, 9.664284)\n",
      "Epoch 113 |  ET 3.28 min AvgLosses >> G/D 1.110/0.975 D Real :0.461 D Fake :0.532 Length EvaL (23.491821, 9.316014)\n",
      "Epoch 114 |  ET 3.31 min AvgLosses >> G/D 0.957/1.130 D Real :0.549 D Fake :0.454 Length EvaL (23.579699, 9.238942)\n",
      "Epoch 115 |  ET 3.33 min AvgLosses >> G/D 1.065/1.000 D Real :0.486 D Fake :0.517 Length EvaL (23.433395, 8.852501)\n",
      "Epoch 116 |  ET 3.36 min AvgLosses >> G/D 0.909/1.180 D Real :0.571 D Fake :0.431 Length EvaL (23.349304, 9.106219)\n",
      "Epoch 117 |  ET 3.39 min AvgLosses >> G/D 1.101/0.964 D Real :0.466 D Fake :0.549 Length EvaL (23.069733, 9.045273)\n",
      "Epoch 118 |  ET 3.41 min AvgLosses >> G/D 1.018/1.024 D Real :0.513 D Fake :0.510 Length EvaL (23.840767, 9.154969)\n",
      "Epoch 119 |  ET 3.44 min AvgLosses >> G/D 1.257/0.913 D Real :0.406 D Fake :0.584 Length EvaL (23.391933, 9.045139)\n",
      "Epoch 120 |  ET 3.46 min AvgLosses >> G/D 0.928/1.235 D Real :0.564 D Fake :0.410 Length EvaL (23.178474, 9.089971)\n",
      "Epoch 121 |  ET 3.49 min AvgLosses >> G/D 1.011/1.049 D Real :0.514 D Fake :0.494 Length EvaL (23.130114, 8.883835)\n",
      "Epoch 122 |  ET 3.52 min AvgLosses >> G/D 1.191/0.898 D Real :0.428 D Fake :0.587 Length EvaL (23.201908, 9.226977)\n",
      "Epoch 123 |  ET 3.54 min AvgLosses >> G/D 0.942/1.147 D Real :0.580 D Fake :0.445 Length EvaL (23.207354, 9.426425)\n",
      "Epoch 124 |  ET 3.57 min AvgLosses >> G/D 1.171/0.917 D Real :0.441 D Fake :0.596 Length EvaL (23.07201, 9.855269)\n",
      "Epoch 125 |  ET 3.60 min AvgLosses >> G/D 0.906/1.163 D Real :0.607 D Fake :0.442 Length EvaL (23.288544, 10.401398)\n",
      "Epoch 126 |  ET 3.63 min AvgLosses >> G/D 1.356/0.913 D Real :0.353 D Fake :0.605 Length EvaL (22.9035, 10.306018)\n",
      "Epoch 127 |  ET 3.66 min AvgLosses >> G/D 0.934/1.281 D Real :0.577 D Fake :0.381 Length EvaL (22.612556, 10.004347)\n",
      "Epoch 128 |  ET 3.68 min AvgLosses >> G/D 1.406/0.971 D Real :0.341 D Fake :0.590 Length EvaL (22.634926, 10.4339)\n",
      "Epoch 129 |  ET 3.71 min AvgLosses >> G/D 0.920/1.372 D Real :0.597 D Fake :0.351 Length EvaL (22.333471, 10.327581)\n",
      "Epoch 130 |  ET 3.74 min AvgLosses >> G/D 1.218/0.903 D Real :0.407 D Fake :0.607 Length EvaL (22.758099, 10.502884)\n",
      "Epoch 131 |  ET 3.77 min AvgLosses >> G/D 0.948/1.288 D Real :0.566 D Fake :0.392 Length EvaL (22.733921, 10.4150305)\n",
      "Epoch 132 |  ET 3.80 min AvgLosses >> G/D 1.228/0.985 D Real :0.410 D Fake :0.533 Length EvaL (22.887287, 10.578084)\n",
      "Epoch 133 |  ET 3.82 min AvgLosses >> G/D 0.980/1.120 D Real :0.549 D Fake :0.456 Length EvaL (22.576939, 10.49573)\n",
      "Epoch 134 |  ET 3.85 min AvgLosses >> G/D 1.353/0.947 D Real :0.364 D Fake :0.561 Length EvaL (22.098793, 10.364072)\n",
      "Epoch 135 |  ET 3.88 min AvgLosses >> G/D 0.946/1.210 D Real :0.579 D Fake :0.410 Length EvaL (22.795418, 10.74574)\n",
      "Epoch 136 |  ET 3.91 min AvgLosses >> G/D 1.463/0.898 D Real :0.321 D Fake :0.598 Length EvaL (22.654303, 10.236989)\n",
      "Epoch 137 |  ET 3.94 min AvgLosses >> G/D 0.979/1.068 D Real :0.534 D Fake :0.468 Length EvaL (22.98018, 10.667117)\n",
      "Epoch 138 |  ET 3.97 min AvgLosses >> G/D 1.362/0.920 D Real :0.353 D Fake :0.610 Length EvaL (22.658361, 10.54879)\n",
      "Epoch 139 |  ET 3.99 min AvgLosses >> G/D 0.933/1.356 D Real :0.583 D Fake :0.357 Length EvaL (22.728186, 10.519615)\n",
      "Epoch 140 |  ET 4.02 min AvgLosses >> G/D 1.110/0.993 D Real :0.472 D Fake :0.525 Length EvaL (22.679539, 10.400328)\n",
      "Epoch 141 |  ET 4.05 min AvgLosses >> G/D 0.940/1.186 D Real :0.565 D Fake :0.434 Length EvaL (22.637142, 10.499465)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142 |  ET 4.08 min AvgLosses >> G/D 1.282/0.930 D Real :0.395 D Fake :0.579 Length EvaL (22.762112, 10.680062)\n",
      "Epoch 143 |  ET 4.10 min AvgLosses >> G/D 0.951/1.264 D Real :0.556 D Fake :0.402 Length EvaL (22.958635, 10.38737)\n",
      "Epoch 144 |  ET 4.13 min AvgLosses >> G/D 1.282/0.856 D Real :0.386 D Fake :0.634 Length EvaL (22.765224, 10.519515)\n",
      "Epoch 145 |  ET 4.16 min AvgLosses >> G/D 0.978/1.071 D Real :0.536 D Fake :0.480 Length EvaL (22.357494, 10.123593)\n",
      "Epoch 146 |  ET 4.19 min AvgLosses >> G/D 1.259/0.920 D Real :0.402 D Fake :0.580 Length EvaL (22.402143, 9.833534)\n",
      "Epoch 147 |  ET 4.22 min AvgLosses >> G/D 0.920/1.175 D Real :0.579 D Fake :0.437 Length EvaL (22.537233, 9.919255)\n",
      "Epoch 148 |  ET 4.24 min AvgLosses >> G/D 1.139/0.944 D Real :0.444 D Fake :0.556 Length EvaL (22.396545, 9.899922)\n",
      "Epoch 149 |  ET 4.27 min AvgLosses >> G/D 0.913/1.207 D Real :0.577 D Fake :0.420 Length EvaL (22.27145, 9.894733)\n",
      "Epoch 150 |  ET 4.30 min AvgLosses >> G/D 0.940/1.192 D Real :0.566 D Fake :0.426 Length EvaL (22.680075, 10.100379)\n",
      "Epoch 151 |  ET 4.33 min AvgLosses >> G/D 1.304/1.015 D Real :0.396 D Fake :0.526 Length EvaL (22.827183, 10.437402)\n",
      "Epoch 152 |  ET 4.36 min AvgLosses >> G/D 1.078/0.996 D Real :0.478 D Fake :0.542 Length EvaL (22.263947, 10.036141)\n",
      "Epoch 153 |  ET 4.38 min AvgLosses >> G/D 0.853/1.475 D Real :0.651 D Fake :0.312 Length EvaL (22.523615, 9.94635)\n",
      "Epoch 154 |  ET 4.41 min AvgLosses >> G/D 1.317/0.932 D Real :0.385 D Fake :0.598 Length EvaL (22.276604, 9.8181925)\n",
      "Epoch 155 |  ET 4.44 min AvgLosses >> G/D 0.943/1.288 D Real :0.581 D Fake :0.398 Length EvaL (22.535648, 9.716746)\n",
      "Epoch 156 |  ET 4.47 min AvgLosses >> G/D 0.967/1.098 D Real :0.555 D Fake :0.481 Length EvaL (22.860886, 9.776539)\n",
      "Epoch 157 |  ET 4.50 min AvgLosses >> G/D 0.916/1.427 D Real :0.615 D Fake :0.337 Length EvaL (22.556404, 9.487271)\n",
      "Epoch 158 |  ET 4.52 min AvgLosses >> G/D 1.219/0.926 D Real :0.406 D Fake :0.589 Length EvaL (22.497726, 9.484939)\n",
      "Epoch 159 |  ET 4.55 min AvgLosses >> G/D 1.017/1.068 D Real :0.522 D Fake :0.489 Length EvaL (22.47868, 9.5266485)\n",
      "Epoch 160 |  ET 4.58 min AvgLosses >> G/D 1.251/0.919 D Real :0.406 D Fake :0.590 Length EvaL (22.65612, 9.655797)\n",
      "Epoch 161 |  ET 4.60 min AvgLosses >> G/D 1.201/0.966 D Real :0.430 D Fake :0.566 Length EvaL (22.577732, 9.738332)\n",
      "Epoch 162 |  ET 4.63 min AvgLosses >> G/D 0.908/1.366 D Real :0.624 D Fake :0.359 Length EvaL (22.804695, 9.818399)\n",
      "Epoch 163 |  ET 4.66 min AvgLosses >> G/D 1.253/0.900 D Real :0.402 D Fake :0.622 Length EvaL (22.300062, 9.96573)\n",
      "Epoch 164 |  ET 4.69 min AvgLosses >> G/D 1.029/1.032 D Real :0.517 D Fake :0.497 Length EvaL (22.431416, 9.747689)\n",
      "Epoch 165 |  ET 4.71 min AvgLosses >> G/D 0.905/1.237 D Real :0.614 D Fake :0.407 Length EvaL (22.354012, 9.974395)\n",
      "Epoch 166 |  ET 4.74 min AvgLosses >> G/D 1.189/0.904 D Real :0.434 D Fake :0.609 Length EvaL (22.272545, 10.121999)\n",
      "Epoch 167 |  ET 4.77 min AvgLosses >> G/D 0.905/1.246 D Real :0.600 D Fake :0.406 Length EvaL (22.591507, 10.351092)\n",
      "Epoch 168 |  ET 4.80 min AvgLosses >> G/D 1.297/0.931 D Real :0.384 D Fake :0.615 Length EvaL (22.603941, 9.76709)\n",
      "Epoch 169 |  ET 4.82 min AvgLosses >> G/D 0.924/1.170 D Real :0.580 D Fake :0.443 Length EvaL (22.619547, 9.797004)\n",
      "Epoch 170 |  ET 4.85 min AvgLosses >> G/D 1.092/0.944 D Real :0.467 D Fake :0.572 Length EvaL (22.577116, 10.282482)\n",
      "Epoch 171 |  ET 4.88 min AvgLosses >> G/D 0.868/1.463 D Real :0.660 D Fake :0.325 Length EvaL (22.413893, 10.047074)\n",
      "Epoch 172 |  ET 4.90 min AvgLosses >> G/D 1.214/0.957 D Real :0.418 D Fake :0.554 Length EvaL (22.497852, 10.068417)\n",
      "Epoch 173 |  ET 4.93 min AvgLosses >> G/D 1.214/0.918 D Real :0.414 D Fake :0.587 Length EvaL (22.616177, 9.816022)\n",
      "Epoch 174 |  ET 4.96 min AvgLosses >> G/D 0.915/1.233 D Real :0.597 D Fake :0.410 Length EvaL (22.63435, 10.0332985)\n",
      "Epoch 175 |  ET 4.99 min AvgLosses >> G/D 1.475/0.921 D Real :0.327 D Fake :0.593 Length EvaL (22.397087, 10.053486)\n",
      "Epoch 176 |  ET 5.01 min AvgLosses >> G/D 0.910/1.210 D Real :0.593 D Fake :0.418 Length EvaL (22.546822, 10.085676)\n",
      "Epoch 177 |  ET 5.04 min AvgLosses >> G/D 1.208/0.985 D Real :0.422 D Fake :0.548 Length EvaL (22.735683, 10.293098)\n",
      "Epoch 178 |  ET 5.07 min AvgLosses >> G/D 0.884/1.488 D Real :0.643 D Fake :0.316 Length EvaL (22.673878, 10.443715)\n",
      "Epoch 179 |  ET 5.10 min AvgLosses >> G/D 1.124/0.915 D Real :0.460 D Fake :0.591 Length EvaL (22.581217, 10.17373)\n",
      "Epoch 180 |  ET 5.12 min AvgLosses >> G/D 0.989/1.082 D Real :0.536 D Fake :0.474 Length EvaL (22.56718, 10.104832)\n",
      "Epoch 181 |  ET 5.15 min AvgLosses >> G/D 1.281/0.934 D Real :0.402 D Fake :0.598 Length EvaL (22.426516, 10.016002)\n",
      "Epoch 182 |  ET 5.18 min AvgLosses >> G/D 0.981/1.046 D Real :0.539 D Fake :0.494 Length EvaL (22.644249, 9.95106)\n",
      "Epoch 183 |  ET 5.21 min AvgLosses >> G/D 1.575/0.908 D Real :0.283 D Fake :0.609 Length EvaL (22.151875, 9.544409)\n",
      "Epoch 184 |  ET 5.23 min AvgLosses >> G/D 0.894/1.291 D Real :0.617 D Fake :0.395 Length EvaL (22.365267, 9.893926)\n",
      "Epoch 185 |  ET 5.26 min AvgLosses >> G/D 1.199/0.955 D Real :0.433 D Fake :0.563 Length EvaL (22.286644, 9.70246)\n",
      "Epoch 186 |  ET 5.29 min AvgLosses >> G/D 0.938/1.120 D Real :0.573 D Fake :0.456 Length EvaL (22.543438, 9.955874)\n",
      "Epoch 187 |  ET 5.31 min AvgLosses >> G/D 1.226/0.916 D Real :0.412 D Fake :0.574 Length EvaL (22.683556, 9.921078)\n",
      "Epoch 188 |  ET 5.34 min AvgLosses >> G/D 0.965/1.076 D Real :0.551 D Fake :0.490 Length EvaL (22.572365, 9.943675)\n",
      "Epoch 189 |  ET 5.37 min AvgLosses >> G/D 1.088/0.968 D Real :0.476 D Fake :0.554 Length EvaL (22.66372, 9.763272)\n",
      "Epoch 190 |  ET 5.40 min AvgLosses >> G/D 0.901/1.263 D Real :0.606 D Fake :0.407 Length EvaL (22.454098, 10.053001)\n",
      "Epoch 191 |  ET 5.42 min AvgLosses >> G/D 1.045/1.024 D Real :0.508 D Fake :0.512 Length EvaL (22.866352, 10.185782)\n",
      "Epoch 192 |  ET 5.45 min AvgLosses >> G/D 0.995/1.137 D Real :0.549 D Fake :0.461 Length EvaL (22.687748, 9.823931)\n",
      "Epoch 193 |  ET 5.48 min AvgLosses >> G/D 1.687/1.031 D Real :0.260 D Fake :0.524 Length EvaL (22.925293, 10.213874)\n",
      "Epoch 194 |  ET 5.51 min AvgLosses >> G/D 1.018/1.153 D Real :0.530 D Fake :0.446 Length EvaL (22.874283, 9.895874)\n",
      "Epoch 195 |  ET 5.54 min AvgLosses >> G/D 1.510/0.906 D Real :0.308 D Fake :0.628 Length EvaL (22.839111, 9.84892)\n",
      "Epoch 196 |  ET 5.56 min AvgLosses >> G/D 1.053/0.974 D Real :0.508 D Fake :0.550 Length EvaL (23.242302, 10.638886)\n",
      "Epoch 197 |  ET 5.59 min AvgLosses >> G/D 1.171/0.938 D Real :0.450 D Fake :0.584 Length EvaL (22.571018, 10.000131)\n",
      "Epoch 198 |  ET 5.62 min AvgLosses >> G/D 0.936/1.203 D Real :0.585 D Fake :0.430 Length EvaL (22.487997, 9.869399)\n",
      "Epoch 199 |  ET 5.65 min AvgLosses >> G/D 1.173/0.993 D Real :0.442 D Fake :0.539 Length EvaL (22.680216, 10.090493)\n"
     ]
    }
   ],
   "source": [
    "all_losses = []\n",
    "all_d_vals = []\n",
    "epochs = 200\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    epoch_d_vals, epoch_losses = [],[]\n",
    "\n",
    "    for i, input_real in enumerate(dm.gds):\n",
    "        in_z = torch.randn((batch_size,12), device='cuda',dtype = torch.float32)\n",
    "\n",
    "        gloss, dloss, gfo, dfo, dfr = train_step(in_z, input_real)\n",
    "        d_probs_real = torch.mean(torch.sigmoid(dfr))\n",
    "        d_probs_fake = torch.mean(torch.sigmoid(dfo))\n",
    "        g_probs = torch.mean(torch.sigmoid(gfo))\n",
    "\n",
    "        epoch_losses.append((gloss.cpu().numpy(), dloss.cpu().numpy()))\n",
    "        epoch_d_vals.append((d_probs_real.cpu().numpy(), d_probs_fake.cpu().numpy()))\n",
    "        \n",
    "    all_losses.append(epoch_losses)\n",
    "    all_d_vals.append(epoch_d_vals)\n",
    "\n",
    "    track = f'Epoch {epoch:03d} |  ET {(time.time()-start_time)/60:.2f} min AvgLosses >> G/D '\n",
    "    track = f'{track}{(np.mean(all_losses[-1][0],axis=0)):.3f}/{(np.mean(all_losses[-1][1],axis=0)):.3f}'\n",
    "    track = f'{track} D Real :{(np.mean(all_d_vals[-1][0],axis=0)):.3f}'\n",
    "    track = f'{track} D Fake :{(np.mean(all_d_vals[-1][1],axis=0)):.3f}'\n",
    "    track = f'{track} Length EvaL {eval_gen(batch_size=batch_size)}'\n",
    "\n",
    "    print(track)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5701de9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22.54369, 9.892837)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_gen(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a755323d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/ep_for_gmp.npz'\n",
    "rr = np.load(data_path)\n",
    "ep = [rr[f] for f in rr.files][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afd7986a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21.155681206729565, 9.422310760234602)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_endpoints(ep)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "se33",
   "language": "python",
   "name": "se33"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
