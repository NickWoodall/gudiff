{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a02a1c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "#clear memory better\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "import numpy as np\n",
    "import util.npose_util as nu\n",
    "import util.framediff_utils as du\n",
    "import util.pdb_writer \n",
    "import os\n",
    "import pathlib\n",
    "import dgl\n",
    "import copy\n",
    "from dgl import backend as F\n",
    "import torch_geometric\n",
    "from torch.utils.data import random_split, DataLoader, Dataset\n",
    "from typing import Dict\n",
    "from torch import Tensor\n",
    "from dgl import DGLGraph\n",
    "from torch import nn\n",
    "from chemical import cos_ideal_NCAC #from RoseTTAFold2\n",
    "from torch import einsum\n",
    "import time\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "import tree\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "936c747c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gudiff_model.Graph_UNet_Null import  GraphUNet_Null\n",
    "from gudiff_model import Data_Graph\n",
    "from gudiff_model.Data_Graph_Null import build_npose_from_coords, dump_coord_pdb, define_graph_edges, make_pe_encoding\n",
    "from gudiff_model.Data_Graph_Null import  Make_nullKNN_MP_Graphs\n",
    "from data_rigid_diffuser.diffuser import FrameDiffNoise\n",
    "from se3_transformer.model.FAPE_Loss import FAPE_loss, Qs2Rs, normQ\n",
    "from se3_transformer.model.FAPE_Loss import FAPE_loss_null, FAPE_loss_real\n",
    "from se3_transformer.model.fiber import Fiber\n",
    "from se3_transformer.runtime.utils import str2bool, to_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3062c7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.transform import Rotation\n",
    "from se3_diffuse import rigid_utils as ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d298f50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#indices for, unsure if needed\n",
    "CA = Data_Graph.CA\n",
    "N = Data_Graph.N\n",
    "C = Data_Graph.C\n",
    "\n",
    "# #find better way to incorporate coord_scale\n",
    "\n",
    "#needed\n",
    "N_CA_dist = (Data_Graph.N_CA_dist/10.).to('cuda')\n",
    "C_CA_dist = (Data_Graph.C_CA_dist/10.).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27011d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3b7b32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "705e5873",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer_to(optim, device):\n",
    "    for param in optim.state.values():\n",
    "        # Not sure there are any global tensors in the state dict\n",
    "        if isinstance(param, torch.Tensor):\n",
    "            param.data = param.data.to(device)\n",
    "            if param._grad is not None:\n",
    "                param._grad.data = param._grad.data.to(device)\n",
    "        elif isinstance(param, dict):\n",
    "            for subparam in param.values():\n",
    "                if isinstance(subparam, torch.Tensor):\n",
    "                    subparam.data = subparam.data.to(device)\n",
    "                    if subparam._grad is not None:\n",
    "                        subparam._grad.data = subparam._grad.data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "72ce141d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "\n",
    "    def __init__(self,\n",
    "                 config_path= 'test.config',\n",
    "                 config_path_diffuser = 'data_rigid_diffuser/base.yaml',\n",
    "                 limit=5028,\n",
    "                 conf=None,\n",
    "                 ckpt_model=None,\n",
    "                 cur_step=None,\n",
    "                 cur_epoch=None,\n",
    "                name='gu_null',\n",
    "                ckpt_opt=None):\n",
    "        \"\"\"Initialize experiment.\n",
    "        Args:\n",
    "            exp_cfg: Experiment configuration.\n",
    "        \"\"\"\n",
    "#         with open(config_path, 'r') as file:\n",
    "#             config = yaml.safe_load(file)\n",
    "#         conf = Struct(config)\n",
    "        \n",
    "        logging.basicConfig(filename='test.log', level=logging.INFO)\n",
    "        self._log = logging.getLogger(__name__)\n",
    "        \n",
    "        \n",
    "        #indices for, unsure if needed\n",
    "        self.CA = Data_Graph.CA\n",
    "        self.N = Data_Graph.N\n",
    "        self.C = Data_Graph.C\n",
    "\n",
    "        self.name=name\n",
    "        \n",
    "        config_path='data_rigid_diffuser/base.yaml'\n",
    "        \n",
    "        if conf is None:\n",
    "            #currently faking valid loader in create_dataset\n",
    "            conf = {'batch_size'  : 4,\n",
    "                          'topk'  : 4,\n",
    "                        'stride'  : 8,\n",
    "                            'KNN' : 30,\n",
    "                      'num_heads' : 16,\n",
    "                       'channels' : 64,\n",
    "                   'channels_div' : 8,\n",
    "                     'num_layers' : 1,\n",
    "                 'num_layers_ca'  : 2,\n",
    "               'edge_feature_dim' : 1,\n",
    "              'latent_pool_type'  : 'avg',\n",
    "                        't_size'  : 12,\n",
    "                         'max_t'  : 0.2,\n",
    "                           'mult' : 2,\n",
    "                       'zero_lin' : True,\n",
    "                      'use_tdeg1' : False,\n",
    "                       'roll': False,\n",
    "                       'circ_pe':False,\n",
    "                            'cuda': True,\n",
    "                  'learning rate' : 0.0001,\n",
    "                   'weight_decay' : 0.000001,\n",
    "                    'sc_nf_real'  : 0.05 ,\n",
    "                    'sc_3D_real'  : 1.0 ,\n",
    "                    'sc_3D_null'  : 0.05 ,\n",
    "                    'device'      : 'cuda',\n",
    "                    'num_epoch'   : 100,\n",
    "                    'log_freq'    : 1000,\n",
    "                    'ckpt_freq'   : 10000,\n",
    "                    'early_chkpt' : 2,\n",
    "                    'coord_scale' : 10.0,\n",
    "                    'nf_threshold_real': 1.99,\n",
    "                    'nf_dim': 5}\n",
    "                \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        conf_check = {'ckpt_dir' : 'GUN_checkpoints/',\n",
    "                      'eval_dir' : 'Eval_Direc/',\n",
    "                      'apa_path' : 'data_npose/h4_apa_coords.npz',\n",
    "                      'tog_path' : 'data_npose/h4_tog_coords.npz',\n",
    "                     }\n",
    "        self._conf = conf\n",
    "        \n",
    "\n",
    "        \n",
    "        self.device = conf['device']\n",
    "        self.paths_valid = conf_check['apa_path']\n",
    "        self.paths_train = conf_check['tog_path']\n",
    "        self.score_weights = {}\n",
    "        self.score_weights['nf_real'] = torch.tensor(float(conf['sc_nf_real']),device=self.device)\n",
    "        self.score_weights['3D_real'] = torch.tensor(float(conf['sc_3D_real']),device=self.device)\n",
    "        self.score_weights['3D_null'] = torch.tensor(float(conf['sc_3D_null']),device=self.device)\n",
    "\n",
    "        #needed\n",
    "        self.coord_scale = conf['coord_scale']\n",
    "        self.N_CA_dist = (Data_Graph.N_CA_dist/self.coord_scale).to('cuda')\n",
    "        self.C_CA_dist = (Data_Graph.C_CA_dist/self.coord_scale).to('cuda')\n",
    "        \n",
    "        self.num_epoch = conf['num_epoch']\n",
    "        self.log_freq = conf['log_freq']\n",
    "        self.ckpt_freq = conf['ckpt_freq']\n",
    "        self.early_ckpt = conf['early_chkpt']\n",
    "        \n",
    "        self.B = conf['batch_size']\n",
    "        self.L = 128\n",
    "        self.limit = limit\n",
    "        self.KNN = conf['KNN']\n",
    "        self.stride = conf['stride']\n",
    "        self.real_threshold = conf['nf_threshold_real']\n",
    "        self.nf_dim = conf['nf_dim'] #need to connect via creation hardcoded just here now for loss\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.fnd = FrameDiffNoise(config_path_diffuser, roll = conf['roll'], max_t = conf['max_t'])\n",
    "        self.mkg = Make_nullKNN_MP_Graphs(KNN = self.KNN, mp_stride = self.stride, n_nodes = self.L, \n",
    "                                           real_threshold = self.real_threshold,  pe_circ_encode=conf['circ_pe'])\n",
    "        \n",
    "        #set nf_dim on fiber out\n",
    "        self._model = GraphUNet_Null(fiber_start = Fiber({0:17, 1:2}),\n",
    "                                     fiber_out = Fiber({0:5,1:2}),\n",
    "                                          k = conf['topk'],\n",
    "                                batch_size  = self.B,\n",
    "                                     stride = conf['stride'],\n",
    "                                 max_degree = 3,\n",
    "                                   channels = conf['channels'],\n",
    "                                  num_heads = conf['num_heads'],\n",
    "                               channels_div = conf['channels_div'],\n",
    "                                 num_layers = conf['num_layers'],\n",
    "                              num_layers_ca = conf['num_layers_ca'],\n",
    "                           edge_feature_dim = conf['edge_feature_dim'],\n",
    "                           latent_pool_type = conf['latent_pool_type'],\n",
    "                                     t_size = conf['t_size'],\n",
    "                                       mult = conf['mult'],\n",
    "                                   zero_lin = conf['zero_lin'],\n",
    "                                  use_tdeg1 = conf['use_tdeg1'],\n",
    "                                        cuda= conf['cuda'])\n",
    "        \n",
    "        \n",
    "        num_parameters = sum(p.numel() for p in self._model.parameters())\n",
    "        self.num_parameters = num_parameters\n",
    "        self._log.info(f'Number of model parameters {num_parameters}')\n",
    "#         self._optimizer = EMA(0.980)\n",
    "#         for name, param in self._model.named_parameters():\n",
    "#             if param.requires_grad:\n",
    "#                 self._optimizer.register(name, param.data)\n",
    "\n",
    "        if ckpt_model is not None:\n",
    "            ckpt_model = {k.replace('module.', ''):v for k,v in ckpt_model.items()}\n",
    "            self._model.load_state_dict(ckpt_model, strict=True)\n",
    "        \n",
    "        \n",
    "        self._optimizer = torch.optim.Adam( self._model.parameters(),\n",
    "                                                       lr=conf['learning rate'],\n",
    "                                                       weight_decay=conf['weight_decay'])\n",
    "        if ckpt_opt is not None:\n",
    "            self._optimizer.load_state_dict(ckpt_opt)\n",
    "            optimizer_to(self._optimizer, self.device)\n",
    "        \n",
    "        \n",
    "        dt_string = datetime.now().strftime(\"%dD_%mM_%YY_%Hh_%Mm_%Ss\")\n",
    "        dt_string_short = datetime.now().strftime(\"%dD_%mM_%YY\")\n",
    "        self.ckpt_dir =  conf_check['ckpt_dir']\n",
    "        self.eval_dir = conf_check['eval_dir']\n",
    "        eval_name = f'{self.name}_{dt_string_short}'\n",
    "        if self.ckpt_dir is not None:\n",
    "            # Set-up checkpoint location\n",
    "            ckpt_dir = os.path.join(\n",
    "                 self.ckpt_dir,\n",
    "                 self.name,\n",
    "                 dt_string)\n",
    "            if not os.path.exists(ckpt_dir):\n",
    "                os.makedirs(ckpt_dir, exist_ok=True)\n",
    "            self.ckpt_dir = ckpt_dir\n",
    "            self._log.info(f'Checkpoints saved to: {ckpt_dir}')\n",
    "        else:  \n",
    "            self._log.info('Checkpoint not being saved.')\n",
    "            \n",
    "        if self.eval_dir is not None :\n",
    "            self.eval_dir = os.path.join(\n",
    "                self.eval_dir,\n",
    "                eval_name,\n",
    "                dt_string)\n",
    "            self.eval_dir = self.eval_dir\n",
    "            self._log.info(f'Evaluation saved to: {self.eval_dir}')\n",
    "        else:\n",
    "            self.eval_dir = os.devnull\n",
    "            self._log.info(f'Evaluation will not be saved.')\n",
    "    #         self._aux_data_history = deque(maxlen=100)\n",
    "    \n",
    "        if cur_epoch is None:\n",
    "            self.trained_epochs = 0\n",
    "        else:\n",
    "            self.trained_epochs = cur_epoch\n",
    "            \n",
    "        if cur_step is None:\n",
    "            self.trained_steps = 0\n",
    "        else:\n",
    "            self.trained_steps = cur_step\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def diffuser(self):\n",
    "        return self._diffuser\n",
    "\n",
    "    @property\n",
    "    def model(self):\n",
    "        return self._model\n",
    "\n",
    "    @property\n",
    "    def conf(self):\n",
    "        return self._conf\n",
    "    \n",
    "    def create_dataset(self, fake_valid=True):\n",
    "        \n",
    "        \n",
    "        \n",
    "        #grab the first 3 atoms which are N,CA,C\n",
    "        rr = np.load(self.paths_train)\n",
    "        coords_train = [rr[f] for f in rr.files][0][:self.limit,:]\n",
    "        \n",
    "        rr = np.load(self.paths_valid)\n",
    "        coords_valid = [rr[f] for f in rr.files][0][:self.limit,:]\n",
    "        \n",
    "        self.B\n",
    "        \n",
    "        device='cuda'\n",
    "        \n",
    "        prot_trainData = Data_Graph.ProteinBB_Dataset(coords_train[:self.limit], n_nodes=self.L,\n",
    "                      n_atoms=5, coord_div=10, cast_type=torch.float32)\n",
    "        train_dL = DataLoader(prot_trainData, batch_size=self.B, shuffle=True, drop_last=True)\n",
    "        \n",
    "        prot_validData = Data_Graph.ProteinBB_Dataset(coords_valid[:self.limit], n_nodes=self.L,\n",
    "                      n_atoms=5, coord_div=10, cast_type=torch.float32)\n",
    "        if fake_valid:\n",
    "            valid_dL = train_dL\n",
    "        else:\n",
    "            valid_dL = DataLoader(prot_trainData, batch_size=self.B, shuffle=True, drop_last=True)\n",
    "        \n",
    "        return train_dL, valid_dL\n",
    "    \n",
    "    def model_to_device(self):\n",
    "        self._model = self._model.to(self.device)\n",
    "        print(f\"Using device: {self.device}\")\n",
    "    \n",
    "    def start_training(self, return_logs=False):\n",
    "\n",
    "\n",
    "        self._model = self._model.to(self.device)\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "        self._model.train()\n",
    "        (train_loader, valid_loader) = self.create_dataset()\n",
    "\n",
    "        logs = []\n",
    "        print('number of epochs', self.num_epoch)\n",
    "        for epoch in range(self.trained_epochs, self.num_epoch+self.trained_epochs):\n",
    "            print('epoch', epoch)\n",
    "            print('mem_used',torch.cuda.memory_allocated('cuda:0'))\n",
    "            epoch_log = self.train_epoch(\n",
    "                train_loader,\n",
    "                valid_loader,\n",
    "                epoch=epoch,\n",
    "                return_logs=return_logs\n",
    "            )\n",
    "            if return_logs:\n",
    "                logs.append(epoch_log)\n",
    "\n",
    "        self._log.info('Done')\n",
    "        return logs\n",
    "    \n",
    "    \n",
    "    \n",
    "    def train_epoch(self, train_loader, valid_loader,epoch=0, return_logs=False):\n",
    "        \n",
    "        log_lossses = defaultdict(list)\n",
    "        \n",
    "        global_logs = []\n",
    "        \n",
    "        log_time = time.time()\n",
    "        step_time = time.time()\n",
    "        losskeeper = []\n",
    "        for train_feats in train_loader:\n",
    "            \n",
    "            #train_feats = tree.map_structure(lambda x: x.to(device), train_feats)\n",
    "            loss, aux_data = self.update_fn(train_feats)\n",
    "            \n",
    "            losskeeper.append(float(loss))\n",
    "            \n",
    "            \n",
    "            for k,v in aux_data.items():\n",
    "                log_lossses[k].append(np.array(v))\n",
    "            \n",
    "            self.trained_steps += 1\n",
    "\n",
    "            \n",
    "            \n",
    "            # Logging to terminal\n",
    "            if self.trained_steps == 1 or self.trained_steps % self.log_freq == 0:\n",
    "                elapsed_time = time.time() - log_time\n",
    "                log_time = time.time()\n",
    "                step_per_sec = self.log_freq / elapsed_time\n",
    "                rolling_losses = tree.map_structure(np.mean, log_lossses)\n",
    "                loss_log = ' '.join([\n",
    "                    f'{k}={v[0]:.4f}'\n",
    "                    for k,v in rolling_losses.items() if 'batch' not in k\n",
    "                ])\n",
    "                \n",
    "                self._log.info(\n",
    "                    f'[{self.trained_steps}]: {loss_log}, steps/sec={step_per_sec:.5f}')\n",
    "                log_lossses = defaultdict(list)\n",
    "                \n",
    "                print(f'[{self.trained_steps}]: {loss_log}, steps/sec={step_per_sec:.5f}')\n",
    "                print(np.mean(losskeeper[-1000:]))\n",
    "\n",
    "            # Take checkpoint\n",
    "            \n",
    "            if self.ckpt_dir is not None and (\n",
    "                    (self.trained_steps % self.ckpt_freq) == 0\n",
    "                    or (self.early_ckpt and self.trained_steps == 2)\n",
    "                ):\n",
    "                ckpt_path = os.path.join(\n",
    "                    self.ckpt_dir, f'step_{self.trained_steps}.pth')\n",
    "                du.write_checkpoint(\n",
    "                    ckpt_path,\n",
    "                    copy.deepcopy(self.model.state_dict()),\n",
    "                    self._conf,\n",
    "                    copy.deepcopy(self._optimizer.state_dict()),\n",
    "                    self.trained_epochs,\n",
    "                    self.trained_steps,\n",
    "                    logger=self._log,\n",
    "                    use_torch=True\n",
    "                )\n",
    "                \n",
    "\n",
    "                # Run evaluation\n",
    "                self._log.info(f'Running evaluation of {ckpt_path}')\n",
    "                start_time = time.time()\n",
    "                eval_dir = os.path.join(self.eval_dir, f'step_{self.trained_steps}')\n",
    "                print('eval',eval_dir)\n",
    "                os.makedirs(eval_dir, exist_ok=True)\n",
    "                ckpt_metrics = self.eval_fn(valid_loader,eval_dir,epoch=epoch)\n",
    "                eval_time = time.time() - start_time\n",
    "                self._log.info(f'Finished evaluation in {eval_time:.2f}s')\n",
    "            else:\n",
    "                ckpt_metrics = None\n",
    "                eval_time = None\n",
    "\n",
    "\n",
    "            if torch.isnan(loss):                \n",
    "                raise Exception(f'NaN encountered')\n",
    "                \n",
    "            del loss\n",
    "            for k,v in aux_data.items():\n",
    "                del v\n",
    "\n",
    "\n",
    "\n",
    "    def update_fn(self, data):\n",
    "        \"\"\"Updates the state using some data and returns metrics.\"\"\"\n",
    "        self._optimizer.zero_grad()\n",
    "        loss, aux_data = self.loss_fn(data)\n",
    "        loss.backward()\n",
    "        self._optimizer.step()\n",
    "        loss_out = loss.detach().cpu()\n",
    "        #del loss\n",
    "        return loss_out , aux_data\n",
    "    \n",
    "    def eval_model(self, noised_dict, batched_t):\n",
    "        \n",
    "        def convert_pV_to_points(dict_in, L, key_in='bb_firstp', return_indiv=False):\n",
    "            \"\"\"Concatenates to xyz from Calpha+atom vectors\"\"\"\n",
    "            CA_fp  = dict_in[key_in]['CA'].reshape(self.B, L, 3).to(self.device)\n",
    "            NC_fp = CA_fp + dict_in[key_in]['N_CA'].reshape(self.B, L, 3).to(self.device)\n",
    "            CC_fp = CA_fp + dict_in[key_in]['C_CA'].reshape(self.B, L, 3).to(self.device)\n",
    "            fp =  torch.cat((NC_fp,CA_fp,CC_fp),dim=2).reshape(self.B,L,3,3)\n",
    "            if return_indiv:\n",
    "                return fp, CA_fp, NC_fp, CC_fp\n",
    "            return fp\n",
    "    \n",
    "        CA_t  = noised_dict['bb_shifted']['CA'].reshape(self.B, self.L, 3).to(self.device)\n",
    "        NC_t = CA_t + noised_dict['bb_shifted']['N_CA'].reshape(self.B, self.L, 3).to(self.device)*self.N_CA_dist\n",
    "        CC_t = CA_t + noised_dict['bb_shifted']['C_CA'].reshape(self.B, self.L, 3).to(self.device)*self.C_CA_dist\n",
    "        true =  torch.cat((NC_t,CA_t,CC_t),dim=2).reshape(self.B,self.L,3,3)\n",
    "\n",
    "        CA_n  = noised_dict['bb_noised']['CA'].reshape(self.B, self.L, 3).to(self.device)\n",
    "        NC_n = CA_n + noised_dict['bb_noised']['N_CA'].reshape(self.B, self.L, 3).to(self.device)*self.N_CA_dist\n",
    "        CC_n = CA_n + noised_dict['bb_noised']['C_CA'].reshape(self.B, self.L, 3).to(self.device)*self.C_CA_dist\n",
    "        noise_xyz =  torch.cat((NC_n,CA_n,CC_n),dim=2).reshape(self.B, self.L,3,3)\n",
    "        \n",
    "        feat_dict = self.mkg.prep_for_network(noised_dict) #prepares graphs\n",
    "        with torch.no_grad():\n",
    "            out = self._model(feat_dict, batched_t)\n",
    "            CA_p = out['1'][:,0,:].reshape(self.B, self.L, 3) + CA_n #translation of Calpha\n",
    "            Qs = out['1'][:,1,:] # rotation\n",
    "            Qs = Qs.unsqueeze(1).repeat((1,2,1))\n",
    "            Qs = torch.cat((torch.ones((self.B*self.L,2,1),device=Qs.device),Qs),dim=-1).reshape(self.B,self.L,2,4)\n",
    "            Qs = normQ(Qs)\n",
    "            Rs = Qs2Rs(Qs)\n",
    "            N_C_to_Rot = torch.cat((noised_dict['bb_noised']['N_CA'].reshape(self.B, self.L, 3).to(self.device),\n",
    "                                    noised_dict['bb_noised']['C_CA'].reshape(self.B, self.L, 3).to(self.device)),\n",
    "                                   dim=2).reshape(self.B,self.L,2,1,3)\n",
    "\n",
    "\n",
    "            rot_vecs = einsum('bnkij,bnkhj->bnki',Rs, N_C_to_Rot)\n",
    "            NC_p = CA_p + rot_vecs[:,:,0,:].to(self.device)*self.N_CA_dist\n",
    "            CC_p = CA_p + rot_vecs[:,:,1,:].reshape(self.B, self.L, 3).to(self.device)*self.C_CA_dist\n",
    "\n",
    "            pred = torch.cat((NC_p,CA_p,CC_p),dim=2).reshape(self.B,self.L,3,3)\n",
    "            \n",
    "            fp = convert_pV_to_points(noised_dict, 1, key_in='bb_firstp')\n",
    "            lp = convert_pV_to_points(noised_dict,1,  key_in='bb_lastp')                                             \n",
    "            real_mask = noised_dict['real_nodes_mask'].to(self.device)\n",
    "            score_scales = noised_dict['score_scales'].to(self.device)\n",
    "\n",
    "            nf_pred = out['0']\n",
    "            real_nodes_pred = torch.round(nf_pred).clamp(0,1)\n",
    "            real_nodes_pred_mask = (real_nodes_pred.squeeze().sum(-1)>self.real_threshold).reshape(self.B,self.L)\n",
    "            \n",
    "            lr, lr_d = FAPE_loss_real(pred, true, score_scales, real_mask,  d_clamp=10.0, d_clamp_inter=30.0,\n",
    "                           A=10.0, gamma=1.0, eps=1e-6)\n",
    "\n",
    "            ln, ln_d = FAPE_loss_null(pred, fp, lp, real_mask, score_scales,  d_clamp=10.0,\n",
    "                               d_clamp_inter=30.0, A=10.0, gamma=1.0, eps=1e-6)\n",
    "\n",
    "            ln = ln*self.score_weights['3D_null']\n",
    "            lr = lr*self.score_weights['3D_real']\n",
    "\n",
    "            structure_loss = lr + ln\n",
    "\n",
    "            #score for node feats determining whether node is real or fake\n",
    "            nf_pred = out['0']\n",
    "\n",
    "            nf_feat_dim = noised_dict['real_nodes_noise'].shape[-1]\n",
    "            nf_true = torch.ones(noised_dict['real_nodes_mask'].shape+(nf_feat_dim,) + (1,),\n",
    "                                 dtype=torch.float,device = self.device)\n",
    "\n",
    "            nf_real_mask_mult = real_mask.unsqueeze(-1).unsqueeze(-1).to(self.device)\n",
    "            nf_true = nf_true*nf_real_mask_mult\n",
    "\n",
    "            nf_pred = nf_pred.reshape(self.B,-1,nf_feat_dim)\n",
    "            pred_nf_loss = torch.sum(torch.square(nf_true.squeeze()-nf_pred),dim=-1)\n",
    "\n",
    "            ss_scales = to_cuda(noised_dict['score_scales'])[:,None,None]\n",
    "            pnfloss = (torch.sum((pred_nf_loss*ss_scales))/(self.L*self.nf_dim))*self.score_weights['nf_real']\n",
    "\n",
    "            final_loss = structure_loss + pnfloss\n",
    "            val_loss = {'pnf_loss': pnfloss.detach().cpu(),\n",
    "                        'structure_loss':structure_loss.detach().cpu(),\n",
    "                        'structure_null':ln.detach().cpu(),\n",
    "                        'structure_real':lr.detach().cpu()}\n",
    "\n",
    "        real_nodes_true_mask = noised_dict['real_nodes_mask']\n",
    "        \n",
    "        del nf_real_mask_mult\n",
    "        del ss_scales\n",
    "        del real_mask\n",
    "        del score_scales\n",
    "        del batched_t\n",
    "        del structure_loss\n",
    "        del pnfloss\n",
    "        del nf_pred\n",
    "        del nf_true\n",
    "        del N_C_to_Rot\n",
    "        del CA_n, NC_n, CC_n\n",
    "        \n",
    "        for k,v in out.items():\n",
    "            del v\n",
    "        for k,v in feat_dict.items():\n",
    "            del v\n",
    "        \n",
    "        #needs to be rolled to N-terminal = 0 for pymol output, add coord_scale\n",
    "        return true.detach().cpu(), noise_xyz.detach().cpu(), pred.detach().cpu() , real_nodes_pred_mask.detach().cpu(), real_nodes_true_mask.detach().cpu(), val_loss\n",
    "    \n",
    "    def eval_fn(self, valid_loader, eval_dir, epoch=0, input_t=None, max_cycles=10):\n",
    "        \n",
    "        train_feats = next(iter(valid_loader))\n",
    "        \n",
    "        if input_t is None:\n",
    "            #visualize_T\n",
    "            vis_t = np.array([0.01,0.05,0.1,0.2,0.3,0.5,0.8,1.0])\n",
    "            vis_t = vis_t[None,...].repeat(int(np.ceil(self.B/len(vis_t))),axis=0).flatten()[:self.B]\n",
    "        elif type(input_t) == float:\n",
    "            vis_t = np.ones((self.B,))*input_t\n",
    "        else:\n",
    "            vis_t = input_t\n",
    "\n",
    "        noised_dict = self.fnd.forward(train_feats, t_vec=vis_t)\n",
    "        batched_t = to_cuda( noised_dict['t_vec'] )\n",
    "\n",
    "        true, noise_xyz, pred, real_nodes_pred_mask, real_nodes_true_mask, val_loss = self.eval_model(noised_dict, batched_t)\n",
    "        util.pdb_writer.dump_tnp_null(true, noise_xyz, pred, vis_t, e=epoch, \n",
    "                      numOut=len(vis_t), real_mask=real_nodes_true_mask, \n",
    "                      pred_mask=real_nodes_pred_mask.detach().cpu(), outdir=eval_dir)\n",
    "        \n",
    "        log_lossses = defaultdict(list)\n",
    "        losskeeper = []\n",
    "        eval_steps = 0\n",
    "        \n",
    "        \n",
    "        for i,train_feats in enumerate(valid_loader):\n",
    "            noised_dict = self.fnd.forward(train_feats)\n",
    "            batched_t = to_cuda( noised_dict['t_vec'] )\n",
    "            true, noise_xyz, pred, real_nodes_pred_mask, real_nodes_true_mask, val_loss = self.eval_model(noised_dict, batched_t)\n",
    "            eval_steps += 1\n",
    "            lsum = 0 \n",
    "            for k,v in val_loss.items():\n",
    "                log_lossses[k].append(np.array(v))\n",
    "                lsum+=v.detach().cpu()\n",
    "            losskeeper.append(lsum)   \n",
    "            \n",
    "            del true\n",
    "            del noise_xyz\n",
    "            del pred\n",
    "            del real_nodes_pred_mask\n",
    "            for k,v in val_loss.items():\n",
    "                del v\n",
    "            del batched_t\n",
    "            if i>max_cycles:\n",
    "                break\n",
    "            \n",
    "            \n",
    "        # Logging to terminal\n",
    "        rolling_losses = tree.map_structure(np.mean, log_lossses)\n",
    "        loss_log = ' '.join([\n",
    "            f'{k}={v[0]:.4f}'\n",
    "            for k,v in rolling_losses.items() if 'batch' not in k\n",
    "        ])\n",
    "\n",
    "        log_lossses = defaultdict(list)\n",
    "        print(f'[{eval_steps}]: {loss_log}')\n",
    "        print('eval_loss',np.mean(losskeeper[-1000:]),len(losskeeper))\n",
    "        \n",
    "        for k,v in noised_dict.items():\n",
    "            del v\n",
    "    \n",
    "    \n",
    "    \n",
    "    def loss_fn(self, bb_dict, t_val=None):\n",
    "        \n",
    "        def convert_pV_to_points(dict_in, L, key_in='bb_firstp', return_indiv=False):\n",
    "            \"\"\"Concatenates to xyz from Calpha+atom vectors\"\"\"\n",
    "            CA_fp  = dict_in[key_in]['CA'].reshape(self.B, L, 3).to(self.device)\n",
    "            NC_fp = CA_fp + dict_in[key_in]['N_CA'].reshape(self.B, L, 3).to(self.device)\n",
    "            CC_fp = CA_fp + dict_in[key_in]['C_CA'].reshape(self.B, L, 3).to(self.device)\n",
    "            fp =  torch.cat((NC_fp,CA_fp,CC_fp),dim=2).reshape(self.B,L,3,3)\n",
    "            if return_indiv:\n",
    "                return fp, CA_fp, NC_fp, CC_fp\n",
    "            return fp\n",
    "        \n",
    "        if t_val is not None:\n",
    "            noised_dict = self.fnd.forward(bb_dict, t_vec=t_val)\n",
    "        else:\n",
    "            #generates with random t\n",
    "            noised_dict = self.fnd.forward(bb_dict)\n",
    "        \n",
    "        batched_t = noised_dict['t_vec'].to(self.device)\n",
    "        \n",
    "        #not converted to distance by multiplying by bond distance, seems to work better\n",
    "        true =  convert_pV_to_points(noised_dict, self.L, key_in='bb_shifted')\n",
    "        noise_xyz, CA_n, NC_n, CC_n =   convert_pV_to_points(noised_dict, self.L, key_in='bb_noised', return_indiv=True)\n",
    "                                                                                                            \n",
    "        #prepare graphs\n",
    "        feat_dict = self.mkg.prep_for_network(noised_dict, cuda=True)\n",
    "        out  = self._model(feat_dict,batched_t)\n",
    "                                                                    \n",
    "        #FAPE Loss for the prediction\n",
    "        CA_p = out['1'][:,0,:].reshape(self.B, self.L, 3) + CA_n #translation of Calpha\n",
    "        Qs = out['1'][:,1,:] # rotation , convert from x,y,z (Quat) to rotate input vectors\n",
    "        Qs = Qs.unsqueeze(1).repeat((1,2,1))\n",
    "        Qs = torch.cat((torch.ones((self.B*self.L,2,1), device=Qs.device),Qs),dim=-1).reshape(self.B, self.L, 2, 4)\n",
    "        Qs = normQ(Qs)\n",
    "        Rs = Qs2Rs(Qs)\n",
    "        N_C_to_Rot = torch.cat((noised_dict['bb_noised']['N_CA'].reshape(self.B, self.L, 3).to(self.device),\n",
    "                                noised_dict['bb_noised']['C_CA'].reshape(self.B, self.L, 3).to(self.device)),dim=2).reshape(self.B,self.L,2,1,3)\n",
    "        rot_vecs = einsum('bnkij,bnkhj->bnki',Rs, N_C_to_Rot)\n",
    "        NC_p = CA_p + rot_vecs[:,:,0,:]*self.N_CA_dist #comparable but seems better not have it for true, but have it for pred\n",
    "        CC_p = CA_p + rot_vecs[:,:,1,:]*self.C_CA_dist #maybe this helep prevent \n",
    "\n",
    "        pred = torch.cat((NC_p,CA_p,CC_p),dim=2).reshape(self.B,self.L,3,3)\n",
    "                                                                \n",
    "                                                                                                                     \n",
    "        fp = convert_pV_to_points(noised_dict, 1, key_in='bb_firstp')\n",
    "        lp = convert_pV_to_points(noised_dict,1,  key_in='bb_lastp')                                             \n",
    "        real_mask = noised_dict['real_nodes_mask'].to(self.device)\n",
    "        score_scales = noised_dict['score_scales'].to(self.device)\n",
    "\n",
    "        lr, lr_d = FAPE_loss_real(pred, true, score_scales, real_mask,  d_clamp=10.0, d_clamp_inter=30.0,\n",
    "                       A=10.0, gamma=1.0, eps=1e-6)\n",
    "\n",
    "        ln, ln_d = FAPE_loss_null(pred, fp, lp, real_mask, score_scales,  d_clamp=10.0,\n",
    "                           d_clamp_inter=30.0, A=10.0, gamma=1.0, eps=1e-6)\n",
    "        \n",
    "        ln = ln*self.score_weights['3D_null']\n",
    "        lr = lr*self.score_weights['3D_real']\n",
    "\n",
    "        structure_loss = lr + ln\n",
    "\n",
    "        #score for node feats determining whether node is real or fake\n",
    "        nf_pred = out['0']\n",
    "\n",
    "        nf_feat_dim = noised_dict['real_nodes_noise'].shape[-1]\n",
    "        nf_true = torch.ones(noised_dict['real_nodes_mask'].shape+(nf_feat_dim,) + (1,),\n",
    "                             dtype=torch.float,device = self.device)\n",
    "\n",
    "        nf_real_mask_mult = real_mask.unsqueeze(-1).unsqueeze(-1).to(self.device)\n",
    "        nf_true = nf_true*nf_real_mask_mult\n",
    "\n",
    "        nf_pred = nf_pred.reshape(self.B,-1,nf_feat_dim)\n",
    "        pred_nf_loss = torch.sum(torch.square(nf_true.squeeze()-nf_pred),dim=(-2,-1))\n",
    "\n",
    "        ss_scales = score_scales[:,None,None]\n",
    "        pnfloss = (torch.sum((pred_nf_loss*ss_scales))/(self.L*self.nf_dim))*self.score_weights['nf_real']\n",
    "\n",
    "        final_loss = structure_loss + pnfloss\n",
    "        \n",
    "        aux_loss = {'pnf_loss':pnfloss.detach().cpu(),\n",
    "                    'structure_loss':structure_loss.detach().cpu(),\n",
    "                    'structure_null':ln.detach().cpu(),\n",
    "                    'structure_real':lr.detach().cpu()}\n",
    "        \n",
    "        del nf_real_mask_mult\n",
    "        del ss_scales\n",
    "        del real_mask\n",
    "        del score_scales\n",
    "        del batched_t\n",
    "        del structure_loss\n",
    "        del pnfloss\n",
    "        del nf_pred\n",
    "        del nf_true\n",
    "        del N_C_to_Rot\n",
    "        del noise_xyz, CA_n, NC_n, CC_n\n",
    "        \n",
    "        for k,v in out.items():\n",
    "            del v\n",
    "            \n",
    "        for k,v in feat_dict.items():\n",
    "            del v\n",
    "                                                                \n",
    "        return final_loss, aux_loss\n",
    "    \n",
    "    def reverse_step(self, bb_dict, t_val=None, dt=0.01, frame_diff=None, \n",
    "                 denoise=False, cast = torch.float32):\n",
    "    \n",
    "    \n",
    "        def convert_pV_to_points(dict_in, L, key_in='bb_firstp', return_indiv=False):\n",
    "                \"\"\"Concatenates to xyz from Calpha+atom vectors\"\"\"\n",
    "                CA_fp  = dict_in[key_in]['CA'].reshape(self.B, L, 3).to(self.device)\n",
    "                NC_fp = CA_fp + dict_in[key_in]['N_CA'].reshape(self.B, L, 3).to(self.device)\n",
    "                CC_fp = CA_fp + dict_in[key_in]['C_CA'].reshape(self.B, L, 3).to(self.device)\n",
    "                fp =  torch.cat((NC_fp,CA_fp,CC_fp),dim=2).reshape(self.B,L,3,3)\n",
    "                if return_indiv:\n",
    "                    return fp, CA_fp, NC_fp, CC_fp\n",
    "                return fp\n",
    "\n",
    "        if t_val is not None:\n",
    "            noised_dict = self.fnd.forward(bb_dict, t_vec=t_val)\n",
    "        else:\n",
    "            #generates pure noise\n",
    "            t_val = np.ones((self.B,),dtype=float)\n",
    "            noised_dict = self.fnd.forward(bb_dict, t_vec=t_val)\n",
    "\n",
    "\n",
    "        batched_t = noised_dict['t_vec'].to(self.device)\n",
    "\n",
    "        #not converted to distance by multiplying by bond distance, seems to work better\n",
    "        noise_xyz, CA_n, NC_n, CC_n =   convert_pV_to_points(noised_dict, self.L, key_in='bb_noised', return_indiv=True)\n",
    "        #prepare graphs\n",
    "        feat_dict = self.mkg.prep_for_network(noised_dict, cuda=True)\n",
    "        with torch.no_grad():\n",
    "            out  = self._model(feat_dict, batched_t)\n",
    "\n",
    "        #prepare new coordinates\n",
    "        CA_p = out['1'][:,0,:].reshape(self.B, self.L, 3) + CA_n #translation of Calpha\n",
    "        Qs = out['1'][:,1,:] # rotation , convert from x,y,z (Quat) to rotate input vectors\n",
    "        Qs = Qs.unsqueeze(1).repeat((1,2,1))\n",
    "        Qs = torch.cat((torch.ones((self.B*self.L,2,1), device=Qs.device),Qs),dim=-1).reshape(self.B, self.L, 2, 4)\n",
    "        Qs = normQ(Qs)\n",
    "        Rs = Qs2Rs(Qs)\n",
    "        N_C_to_Rot = torch.cat((noised_dict['bb_noised']['N_CA'].reshape(self.B, self.L, 3).to(self.device),\n",
    "                                noised_dict['bb_noised']['C_CA'].reshape(self.B, self.L, 3).to(self.device)),dim=2).reshape(self.B,self.L,2,1,3)\n",
    "        rot_vecs = einsum('bnkij,bnkhj->bnki',Rs, N_C_to_Rot)\n",
    "        NC_p = CA_p + rot_vecs[:,:,0,:]*self.N_CA_dist \n",
    "        CC_p = CA_p + rot_vecs[:,:,1,:]*self.C_CA_dist \n",
    "\n",
    "        ###WHERE to DO I 10x mult in new code\n",
    "        pred = torch.cat((NC_p,CA_p,CC_p),dim=2).reshape(self.B,self.L,3,3)\n",
    "\n",
    "        nf_pred = out['0']\n",
    "        nf_feat_dim = noised_dict['real_nodes_noise'].shape[-1]\n",
    "        nf_pred = nf_pred.reshape(self.B,-1,nf_feat_dim)\n",
    "\n",
    "        return pred, nf_pred\n",
    "                                                                \n",
    "                                                                \n",
    "            \n",
    "                                                                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e36e2432",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = {'batch_size'  : 4,\n",
    "                          'topk'  : 4,\n",
    "                        'stride'  : 8,\n",
    "                            'KNN' : 30,\n",
    "                      'num_heads' : 16,\n",
    "                       'channels' : 64,\n",
    "                   'channels_div' : 8,\n",
    "                     'num_layers' : 1,\n",
    "                 'num_layers_ca'  : 2,\n",
    "               'edge_feature_dim' : 1,\n",
    "              'latent_pool_type'  : 'avg',\n",
    "                        't_size'  : 12,\n",
    "                         'max_t'  : 1.0,\n",
    "                           'mult' : 2,\n",
    "                       'zero_lin' : True,\n",
    "                      'use_tdeg1' : False,\n",
    "                       'roll': False,\n",
    "                       'circ_pe':False,\n",
    "                            'cuda': True,\n",
    "                  'learning rate' : 0.000025,\n",
    "                   'weight_decay' : 0.000001,\n",
    "                    'sc_nf_real'  : 0.05 ,\n",
    "                    'sc_3D_real'  : 1.0 ,\n",
    "                    'sc_3D_null'  : 0.05 ,\n",
    "                    'device'      : 'cuda',\n",
    "                    'num_epoch'   : 100,\n",
    "                    'log_freq'    : 1000,\n",
    "                    'ckpt_freq'   : 10000,\n",
    "                    'early_chkpt' : 2,\n",
    "                    'coord_scale' : 10.0,\n",
    "                    'nf_threshold_real': 1.99,\n",
    "                    'nf_dim': 5}\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ab2f3e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_file = 'GUN_checkpoints/gu_null/03D_04M_2024Y_09h_16m_56s/step_80000.pth'\n",
    "checkpoint_file = 'GUN_checkpoints/gu_null/04D_04M_2024Y_01h_02m_24s/step_190000.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "19108401",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_model=torch.load(checkpoint_file)['model']\n",
    "ckpt_opt = torch.load(checkpoint_file)['optimizer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b5994b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = Experiment(conf=conf,ckpt_model=ckpt_model,ckpt_opt=ckpt_opt, cur_step=190000, cur_epoch=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "2df8bdfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "train_dL, valid_dL = exp.create_dataset()\n",
    "exp.model_to_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "2c47d32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_iter = iter(train_dL)\n",
    "bb_dict = next(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "cbd94cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nwoodall/miniconda3/envs/se33/lib/python3.9/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
      "/home/nwoodall/miniconda3/envs/se33/lib/python3.9/site-packages/dgl/backend/pytorch/tensor.py:449: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), (\n"
     ]
    }
   ],
   "source": [
    "pred, nf = exp.reverse_step(bb_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a85e429c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp._model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b8a1b833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batched_graph cuda:0\n",
      "0 cuda:0\n",
      "1 cuda:0\n",
      "0 cuda:0\n",
      "batched_graph_mp cuda:0\n",
      "0 cuda:0\n",
      "1 cuda:0\n",
      "0 cuda:0\n",
      "batched_graph_mpself cuda:0\n",
      "0 cuda:0\n",
      "0 cuda:0\n",
      "batched_graph_mprev cuda:0\n"
     ]
    }
   ],
   "source": [
    "for k,v in fd.items():\n",
    "    if type(v) == dict:\n",
    "        for k2,v2 in v.items():\n",
    "            print(k2,v2.device)\n",
    "    else:\n",
    "        print(k,v.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "50386265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bt.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1cfa0446",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[98], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfd\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/se33/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/c/Users/nwood/OneDrive/Desktop/gudiff/gudiff_model/Graph_UNet_Null.py:409\u001b[0m, in \u001b[0;36mGraphUNet_Null.forward\u001b[0;34m(self, feat_dict, batched_t)\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmp_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(b_graph_mp\u001b[38;5;241m.\u001b[39mbatch_num_nodes()[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;66;03m#ca+mp nodes number\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m#SE3 Attention Transformer, c_alphas based on knn real nodes/ null nodes\u001b[39;00m\n\u001b[0;32m--> 409\u001b[0m embed_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_t\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatched_t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m t_nf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconcat_t(nf, embed_t, use_deg1\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_tdeg1) \u001b[38;5;66;03m#concat_t on\u001b[39;00m\n\u001b[1;32m    411\u001b[0m nf_ca_down_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_ca(b_graph, t_nf, ef)\n",
      "File \u001b[0;32m~/miniconda3/envs/se33/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/c/Users/nwood/OneDrive/Desktop/gudiff/gudiff_model/Graph_UNet_Null.py:104\u001b[0m, in \u001b[0;36mGaussianFourierProjection_Linear.forward\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear0(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mGFP(t))), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mGFP(t)))}\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear0(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGFP\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m))}\n",
      "File \u001b[0;32m~/miniconda3/envs/se33/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/c/Users/nwood/OneDrive/Desktop/gudiff/gudiff_model/Graph_UNet_Null.py:84\u001b[0m, in \u001b[0;36mGaussianFourierProjection.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 84\u001b[0m     x_proj \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mpi\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat([torch\u001b[38;5;241m.\u001b[39msin(x_proj), torch\u001b[38;5;241m.\u001b[39mcos(x_proj)], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "exp._model(fd,bt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "54162801",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get rots from sample ref\n",
    "def _extract_trans_rots(rigid: ru.Rigid):\n",
    "    rot = rigid.get_rots().get_rot_mats().cpu().numpy()\n",
    "    rot_shape = rot.shape\n",
    "    num_rots = np.cumprod(rot_shape[:-2])[-1]\n",
    "    rot = rot.reshape((num_rots, 3, 3))\n",
    "    rot = Rotation.from_matrix(rot).as_rotvec().reshape(rot_shape[:-2] +(3,))\n",
    "    tran = rigid.get_trans().cpu().numpy()\n",
    "    return tran, rot\n",
    "\n",
    "def reverse_step(self, bb_dict, t_val=None, dt=0.01, frame_diff=None, \n",
    "                 denoise=False, cast = torch.float32):\n",
    "    \n",
    "    \n",
    "    def convert_pV_to_points(dict_in, L, key_in='bb_firstp', return_indiv=False):\n",
    "            \"\"\"Concatenates to xyz from Calpha+atom vectors\"\"\"\n",
    "            CA_fp  = dict_in[key_in]['CA'].reshape(self.B, L, 3).to(self.device)\n",
    "            NC_fp = CA_fp + dict_in[key_in]['N_CA'].reshape(self.B, L, 3).to(self.device)\n",
    "            CC_fp = CA_fp + dict_in[key_in]['C_CA'].reshape(self.B, L, 3).to(self.device)\n",
    "            fp =  torch.cat((NC_fp,CA_fp,CC_fp),dim=2).reshape(self.B,L,3,3)\n",
    "            if return_indiv:\n",
    "                return fp, CA_fp, NC_fp, CC_fp\n",
    "            return fp\n",
    "        \n",
    "    if t_val is not None:\n",
    "        noised_dict = self.fnd.forward(bb_dict, t_vec=t_val)\n",
    "    else:\n",
    "        #generates pure noise\n",
    "        t_val = torch.ones((self.B,))\n",
    "        noised_dict = self.fnd.forward(bb_dict, t_vec=t_val)\n",
    "    \n",
    "    \n",
    "    batched_t = noised_dict['t_vec'].to(self.device)\n",
    "        \n",
    "    #not converted to distance by multiplying by bond distance, seems to work better\n",
    "    noise_xyz, CA_n, NC_n, CC_n =   convert_pV_to_points(noised_dict, self.L, key_in='bb_noised', return_indiv=True)\n",
    "    #prepare graphs\n",
    "    feat_dict = self.mkg.prep_for_network(noised_dict, cuda=True)\n",
    "    with torch.no_grad():\n",
    "        out  = self._model(feat_dict,batched_t)\n",
    "    \n",
    "    #prepare new coordinates\n",
    "    CA_p = out['1'][:,0,:].reshape(self.B, self.L, 3) + CA_n #translation of Calpha\n",
    "    Qs = out['1'][:,1,:] # rotation , convert from x,y,z (Quat) to rotate input vectors\n",
    "    Qs = Qs.unsqueeze(1).repeat((1,2,1))\n",
    "    Qs = torch.cat((torch.ones((self.B*self.L,2,1), device=Qs.device),Qs),dim=-1).reshape(self.B, self.L, 2, 4)\n",
    "    Qs = normQ(Qs)\n",
    "    Rs = Qs2Rs(Qs)\n",
    "    N_C_to_Rot = torch.cat((noised_dict['bb_noised']['N_CA'].reshape(self.B, self.L, 3).to(self.device),\n",
    "                            noised_dict['bb_noised']['C_CA'].reshape(self.B, self.L, 3).to(self.device)),dim=2).reshape(self.B,self.L,2,1,3)\n",
    "    rot_vecs = einsum('bnkij,bnkhj->bnki',Rs, N_C_to_Rot)\n",
    "    NC_p = CA_p + rot_vecs[:,:,0,:]*self.N_CA_dist \n",
    "    CC_p = CA_p + rot_vecs[:,:,1,:]*self.C_CA_dist \n",
    "        \n",
    "    ###WHERE to DO I 10x mult in new code\n",
    "    pred = torch.cat((NC_p,CA_p,CC_p),dim=2).reshape(self.B,self.L,3,3)\n",
    "    \n",
    "    nf_pred = out['0']\n",
    "    nf_feat_dim = noised_dict['real_nodes_noise'].shape[-1]\n",
    "    nf_pred = nf_pred.reshape(self.B,-1,nf_feat_dim)\n",
    "    \n",
    "    return pred, nf_pred\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23451dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dL, valid_dL = exp.create_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c82f021",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'noised_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m batched_t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(t_vec,dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m noised_bb \u001b[38;5;241m=\u001b[39m exp\u001b[38;5;241m.\u001b[39mfnd(bb_dict,t_vec\u001b[38;5;241m=\u001b[39mt_vec)\n\u001b[0;32m----> 8\u001b[0m nd_2 \u001b[38;5;241m=\u001b[39m \u001b[43mreverse_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoised_bb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m             \u001b[49m\u001b[43mexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmkg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_diff\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfnd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenoise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m ndbb \u001b[38;5;241m=\u001b[39m noised_bb\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m t\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0.01\u001b[39m:\n",
      "Cell \u001b[0;32mIn[14], line 14\u001b[0m, in \u001b[0;36mreverse_step\u001b[0;34m(noise_dict, graph_unet, batched_t, graph_maker, dt, frame_diff, denoise, cast)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreverse_step\u001b[39m(noise_dict, graph_unet, batched_t, \n\u001b[1;32m     12\u001b[0m                  graph_maker,dt\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, frame_diff\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, denoise\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, cast \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat32):\n\u001b[0;32m---> 14\u001b[0m     CA_n \u001b[38;5;241m=\u001b[39m \u001b[43mnoised_dict\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCA\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m     NC_n \u001b[38;5;241m=\u001b[39m CA_n \u001b[38;5;241m+\u001b[39m noised_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN_CA\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(B, L, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m*\u001b[39mN_CA_dist\n\u001b[1;32m     16\u001b[0m     CC_n \u001b[38;5;241m=\u001b[39m CA_n \u001b[38;5;241m+\u001b[39m noised_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC_CA\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(B, L, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m*\u001b[39mC_CA_dist\n",
      "\u001b[0;31mNameError\u001b[0m: name 'noised_dict' is not defined"
     ]
    }
   ],
   "source": [
    "test_iter = iter(train_dL)\n",
    "bb_dict = next(test_iter)\n",
    "\n",
    "t=0.3\n",
    "t_vec = np.ones(exp.B,)*t\n",
    "batched_t = torch.tensor(t_vec,dtype=torch.float32).to('cuda')\n",
    "noised_bb = exp.fnd(bb_dict,t_vec=t_vec)\n",
    "nd_2 = reverse_step(noised_bb, exp._model, batched_t, \n",
    "             exp.mkg, frame_diff=exp.fnd, dt=0.01, denoise=False, cast = torch.float32)\n",
    "ndbb = noised_bb\n",
    "while t>0.01:\n",
    "    \n",
    "    t_vec = np.ones(B,)*t\n",
    "    batched_t = torch.tensor(t_vec,dtype=torch.float32).to('cuda')\n",
    "    ndbb = reverse_step(ndbb, gu, batched_t, \n",
    "                        gm, frame_diff=fdn, dt=0.01, denoise=False, cast = torch.float32)\n",
    "    print(f'{t*100:.0f}')\n",
    "    t = t-.01\n",
    "    xyz = bbdict_to_coords(ndbb)\n",
    "    write_coord_pdb(xyz,name=f'out_{t*100:.0f}',limit=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "488914eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bb_shifted': {'CA': tensor([[[-0.8241, -0.2703, -0.9569],\n",
       "           [-0.8241, -0.2703, -0.9569],\n",
       "           [-0.8241, -0.2703, -0.9569],\n",
       "           ...,\n",
       "           [ 0.1507, -0.9225,  0.4129],\n",
       "           [ 0.1507, -0.9225,  0.4129],\n",
       "           [ 0.1507, -0.9225,  0.4129]],\n",
       "  \n",
       "          [[-0.6088, -0.8041, -0.1776],\n",
       "           [-0.6088, -0.8041, -0.1776],\n",
       "           [-0.6088, -0.8041, -0.1776],\n",
       "           ...,\n",
       "           [ 0.0053, -1.4992,  0.1049],\n",
       "           [ 0.0053, -1.4992,  0.1049],\n",
       "           [ 0.0053, -1.4992,  0.1049]],\n",
       "  \n",
       "          [[-1.2085, -0.2948, -0.3240],\n",
       "           [-1.2085, -0.2948, -0.3240],\n",
       "           [-1.2085, -0.2948, -0.3240],\n",
       "           ...,\n",
       "           [-0.8291, -0.3962,  0.6928],\n",
       "           [-0.8291, -0.3962,  0.6928],\n",
       "           [-0.8291, -0.3962,  0.6928]],\n",
       "  \n",
       "          [[-0.8588, -0.5110, -0.5739],\n",
       "           [-0.8588, -0.5110, -0.5739],\n",
       "           [-0.8588, -0.5110, -0.5739],\n",
       "           ...,\n",
       "           [-0.9205, -0.8206,  0.5710],\n",
       "           [-0.9205, -0.8206,  0.5710],\n",
       "           [-0.9205, -0.8206,  0.5710]]]),\n",
       "  'N_CA': tensor([[-1.0000,  0.0000,  0.0000],\n",
       "          [-1.0000,  0.0000,  0.0000],\n",
       "          [-1.0000,  0.0000,  0.0000],\n",
       "          ...,\n",
       "          [ 0.8255,  0.4025, -0.3956],\n",
       "          [ 0.8255,  0.4025, -0.3956],\n",
       "          [ 0.8255,  0.4025, -0.3956]]),\n",
       "  'C_CA': tensor([[ 0.3623,  0.9320,  0.0000],\n",
       "          [ 0.3623,  0.9320,  0.0000],\n",
       "          [ 0.3623,  0.9320,  0.0000],\n",
       "          ...,\n",
       "          [-0.6864, -0.1839, -0.7035],\n",
       "          [-0.6864, -0.1839, -0.7035],\n",
       "          [-0.6864, -0.1839, -0.7035]])},\n",
       " 'bb_firstp': {'CA': tensor([[[-0.8241, -0.2703, -0.9569]],\n",
       "  \n",
       "          [[-0.6088, -0.8041, -0.1776]],\n",
       "  \n",
       "          [[-1.2085, -0.2948, -0.3240]],\n",
       "  \n",
       "          [[-0.8588, -0.5110, -0.5739]]]),\n",
       "  'N_CA': tensor([[[-1.0000,  0.0000,  0.0000]],\n",
       "  \n",
       "          [[-1.0000,  0.0000,  0.0000]],\n",
       "  \n",
       "          [[-1.0000,  0.0000,  0.0000]],\n",
       "  \n",
       "          [[-1.0000,  0.0000,  0.0000]]]),\n",
       "  'C_CA': tensor([[[0.3623, 0.9320, 0.0000]],\n",
       "  \n",
       "          [[0.3617, 0.9323, 0.0000]],\n",
       "  \n",
       "          [[0.3612, 0.9325, 0.0000]],\n",
       "  \n",
       "          [[0.3617, 0.9323, 0.0000]]])},\n",
       " 'bb_lastp': {'CA': tensor([[[ 0.1507, -0.9225,  0.4129]],\n",
       "  \n",
       "          [[ 0.0053, -1.4992,  0.1049]],\n",
       "  \n",
       "          [[-0.8291, -0.3962,  0.6928]],\n",
       "  \n",
       "          [[-0.9205, -0.8206,  0.5710]]]),\n",
       "  'N_CA': tensor([[[-0.3699,  0.8474,  0.3808]],\n",
       "  \n",
       "          [[ 0.3748,  0.8971, -0.2341]],\n",
       "  \n",
       "          [[ 0.6817,  0.2808, -0.6756]],\n",
       "  \n",
       "          [[ 0.8255,  0.4025, -0.3956]]]),\n",
       "  'C_CA': tensor([[[-0.2495, -0.7880,  0.5628]],\n",
       "  \n",
       "          [[-0.7065, -0.2781, -0.6507]],\n",
       "  \n",
       "          [[-0.8682,  0.4751, -0.1431]],\n",
       "  \n",
       "          [[-0.6864, -0.1839, -0.7035]]])},\n",
       " 'bb_noised': {'CA': tensor([[[-1.4385, -0.9417, -0.8515],\n",
       "           [ 0.1362, -0.2348, -2.0339],\n",
       "           [-1.4237, -1.5665,  0.6218],\n",
       "           ...,\n",
       "           [-0.0790, -1.7967,  1.3246],\n",
       "           [ 0.1972,  1.5291, -1.0320],\n",
       "           [-1.1851, -0.9319, -1.0373]],\n",
       "  \n",
       "          [[ 0.1433, -0.6111, -0.5343],\n",
       "           [-0.5538, -0.4262, -0.7017],\n",
       "           [ 0.5052, -1.4936,  1.0080],\n",
       "           ...,\n",
       "           [ 0.9619, -1.5960, -0.3973],\n",
       "           [-0.8082, -0.1748, -1.2610],\n",
       "           [-0.0936, -0.6942,  0.0075]],\n",
       "  \n",
       "          [[-0.7899,  1.2285,  0.0852],\n",
       "           [-0.0934, -1.6058,  0.7381],\n",
       "           [-1.3569,  0.2937, -2.0337],\n",
       "           ...,\n",
       "           [-1.3809, -0.0421, -1.0410],\n",
       "           [-1.2010, -0.8773,  0.0054],\n",
       "           [-0.2040, -0.6223,  1.4279]],\n",
       "  \n",
       "          [[-1.8724, -0.7918,  0.1361],\n",
       "           [ 0.2042, -0.0304,  0.1585],\n",
       "           [-0.8787, -0.5872,  0.5901],\n",
       "           ...,\n",
       "           [ 0.5866,  0.3337,  2.6042],\n",
       "           [-1.3039, -1.1352,  0.1618],\n",
       "           [-0.2761,  0.5693,  0.4500]]]),\n",
       "  'N_CA': tensor([[[[-0.8723, -0.4880,  0.0315]],\n",
       "  \n",
       "           [[-0.8925, -0.0139,  0.4508]],\n",
       "  \n",
       "           [[-0.9778, -0.1999,  0.0631]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[-0.0208,  0.9964,  0.0816]],\n",
       "  \n",
       "           [[-0.8279, -0.1445,  0.5420]],\n",
       "  \n",
       "           [[-0.5385,  0.8144, -0.2161]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.4462, -0.2132, -0.8692]],\n",
       "  \n",
       "           [[-0.5123,  0.8466, -0.1445]],\n",
       "  \n",
       "           [[-0.2373,  0.1447,  0.9606]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[ 0.9198, -0.0344, -0.3908]],\n",
       "  \n",
       "           [[-0.5904,  0.8016, -0.0944]],\n",
       "  \n",
       "           [[-0.2487,  0.8774,  0.4103]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.8196, -0.5729, -0.0111]],\n",
       "  \n",
       "           [[-0.8386, -0.3445, -0.4220]],\n",
       "  \n",
       "           [[ 0.1556,  0.8028,  0.5756]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[ 0.7957,  0.3125, -0.5188]],\n",
       "  \n",
       "           [[-0.4582, -0.1476, -0.8765]],\n",
       "  \n",
       "           [[-0.2003, -0.1333, -0.9706]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.3145,  0.0740,  0.9464]],\n",
       "  \n",
       "           [[-0.6217, -0.2158, -0.7529]],\n",
       "  \n",
       "           [[-0.8111,  0.4159, -0.4113]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[ 0.2052,  0.8068, -0.5540]],\n",
       "  \n",
       "           [[ 0.7772, -0.1963,  0.5978]],\n",
       "  \n",
       "           [[ 0.1643, -0.4685, -0.8681]]]]),\n",
       "  'C_CA': tensor([[[[-0.1248,  0.9768,  0.1742]],\n",
       "  \n",
       "           [[ 0.0713,  0.7659, -0.6389]],\n",
       "  \n",
       "           [[ 0.1709,  0.7911, -0.5873]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[ 0.4743, -0.4161,  0.7758]],\n",
       "  \n",
       "           [[ 0.7341, -0.6134,  0.2913]],\n",
       "  \n",
       "           [[-0.4207, -0.5253,  0.7396]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.5843,  0.8074, -0.0817]],\n",
       "  \n",
       "           [[ 0.6043,  0.0738,  0.7933]],\n",
       "  \n",
       "           [[-0.2191,  0.8045, -0.5520]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[-0.5575, -0.7667, -0.3185]],\n",
       "  \n",
       "           [[ 0.3108, -0.3277, -0.8922]],\n",
       "  \n",
       "           [[ 0.6465,  0.1228, -0.7530]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.2382,  0.9706,  0.0337]],\n",
       "  \n",
       "           [[ 0.3623,  0.7840, -0.5041]],\n",
       "  \n",
       "           [[ 0.7644, -0.6418,  0.0611]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[-0.6103,  0.7611,  0.2198]],\n",
       "  \n",
       "           [[-0.4754,  0.6906,  0.5450]],\n",
       "  \n",
       "           [[-0.1362,  0.9532,  0.2700]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.7600,  0.0977, -0.6425]],\n",
       "  \n",
       "           [[-0.2061,  0.8957,  0.3939]],\n",
       "  \n",
       "           [[ 0.8363,  0.4470, -0.3176]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[-0.9235,  0.0471,  0.3808]],\n",
       "  \n",
       "           [[-0.3046,  0.9471,  0.1009]],\n",
       "  \n",
       "           [[-0.9701,  0.2106,  0.1202]]]])},\n",
       " 't_vec': tensor([0.3000, 0.3000, 0.3000, 0.3000]),\n",
       " 'score_scales': tensor([1.2871, 1.2871, 1.2871, 1.2871]),\n",
       " 'real_nodes_mask': tensor([[False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False],\n",
       "         [False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True, False, False, False, False, False],\n",
       "         [False, False, False, False, False,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False],\n",
       "         [False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False]]),\n",
       " 'real_nodes_noise': tensor([[[-0.7751,  1.2359,  0.7474, -0.2748, -0.1171],\n",
       "          [ 0.1158, -0.2835, -0.1587,  0.6818,  0.4339],\n",
       "          [ 1.1096,  0.5734,  0.5180,  0.9647, -0.2724],\n",
       "          ...,\n",
       "          [-0.8758,  0.0948, -1.4757,  0.0176,  0.4923],\n",
       "          [ 0.5984, -0.1174,  0.9456, -0.7639, -0.7923],\n",
       "          [-1.0403, -0.4017,  0.5719, -0.9786, -0.0333]],\n",
       " \n",
       "         [[-0.8289,  0.0950,  0.4090, -1.0693,  0.5378],\n",
       "          [ 0.2493,  0.9494,  0.2069,  0.1645, -0.0971],\n",
       "          [-0.2755,  0.5662, -0.7080, -0.9288, -0.3421],\n",
       "          ...,\n",
       "          [-0.1337, -1.2340,  0.3550,  0.3481,  0.2991],\n",
       "          [-0.3815,  1.3541,  0.8729, -1.2723, -0.2915],\n",
       "          [ 0.3817,  0.7903,  0.0182, -0.5930,  0.2676]],\n",
       " \n",
       "         [[ 0.0865,  0.6848,  0.0336, -0.0584, -0.6449],\n",
       "          [ 0.3251, -1.1720, -0.1414, -1.0522, -0.1787],\n",
       "          [ 0.6722,  0.0400, -0.8630,  0.1427, -0.9330],\n",
       "          ...,\n",
       "          [ 1.0491,  0.1129,  0.9535,  0.4243, -0.1799],\n",
       "          [ 1.2215,  0.8465,  0.0687, -0.2056,  0.1977],\n",
       "          [ 0.2714, -0.8237, -0.1980,  1.0674,  1.4735]],\n",
       " \n",
       "         [[ 1.4639,  0.8462,  0.1748,  1.2145, -0.2081],\n",
       "          [ 0.6071,  0.8929,  0.4305, -0.3739,  1.5798],\n",
       "          [ 0.2916,  0.8356, -0.3674, -0.4060, -0.6858],\n",
       "          ...,\n",
       "          [ 1.0039,  1.3716, -0.2874,  0.9524, -1.6557],\n",
       "          [-0.8034, -0.3568, -0.4855,  1.1397,  0.5017],\n",
       "          [ 0.2648, -0.9255,  0.9256, -2.1172,  0.0446]]], dtype=torch.float64),\n",
       " 'edge_cons': tensor([[[[[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           ...,\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]]],\n",
       " \n",
       " \n",
       "          [[[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           ...,\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]]],\n",
       " \n",
       " \n",
       "          [[[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           ...,\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]]],\n",
       " \n",
       " \n",
       "          ...,\n",
       " \n",
       " \n",
       "          [[[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           ...,\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]]],\n",
       " \n",
       " \n",
       "          [[[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           ...,\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]]],\n",
       " \n",
       " \n",
       "          [[[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           ...,\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]]]],\n",
       " \n",
       " \n",
       " \n",
       "         [[[[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           ...,\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]]],\n",
       " \n",
       " \n",
       "          [[[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           ...,\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]]],\n",
       " \n",
       " \n",
       "          [[[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           ...,\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]]],\n",
       " \n",
       " \n",
       "          ...,\n",
       " \n",
       " \n",
       "          [[[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           ...,\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]]],\n",
       " \n",
       " \n",
       "          [[[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           ...,\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]]],\n",
       " \n",
       " \n",
       "          [[[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           ...,\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]]]],\n",
       " \n",
       " \n",
       " \n",
       "         [[[[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           ...,\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]]],\n",
       " \n",
       " \n",
       "          [[[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           ...,\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]]],\n",
       " \n",
       " \n",
       "          [[[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           ...,\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]]],\n",
       " \n",
       " \n",
       "          ...,\n",
       " \n",
       " \n",
       "          [[[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           ...,\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]]],\n",
       " \n",
       " \n",
       "          [[[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           ...,\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]]],\n",
       " \n",
       " \n",
       "          [[[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           ...,\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]]]],\n",
       " \n",
       " \n",
       " \n",
       "         [[[[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           ...,\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]]],\n",
       " \n",
       " \n",
       "          [[[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           ...,\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]]],\n",
       " \n",
       " \n",
       "          [[[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           ...,\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]]],\n",
       " \n",
       " \n",
       "          ...,\n",
       " \n",
       " \n",
       "          [[[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           ...,\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]]],\n",
       " \n",
       " \n",
       "          [[[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           ...,\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]]],\n",
       " \n",
       " \n",
       "          [[[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           ...,\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]],\n",
       " \n",
       "           [[0., 1.],\n",
       "            [0., 1.],\n",
       "            [0., 1.]]]]])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp.fnd(bb_dict,t_vec=t_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3bea4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52947ef1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fac141f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065cecde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "number of epochs 100\n",
      "epoch 63\n",
      "mem_used 267491328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nwoodall/miniconda3/envs/se33/lib/python3.9/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
      "/home/nwoodall/miniconda3/envs/se33/lib/python3.9/site-packages/dgl/backend/pytorch/tensor.py:449: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[81000]: pnf_loss=0.4483 structure_loss=0.8490 structure_null=0.0206 structure_real=0.8284, steps/sec=1.66816\n",
      "0.8808940314650535\n",
      "epoch 64\n",
      "mem_used 330741760\n",
      "[82000]: pnf_loss=0.1350 structure_loss=0.5938 structure_null=0.0166 structure_real=0.5772, steps/sec=2.28443\n",
      "0.8999913890782147\n",
      "epoch 65\n",
      "mem_used 330993152\n",
      "[83000]: pnf_loss=0.0787 structure_loss=0.5832 structure_null=0.0142 structure_real=0.5689, steps/sec=3.50332\n",
      "0.8855836835417727\n",
      "epoch 66\n",
      "mem_used 331804672\n",
      "[84000]: pnf_loss=0.1103 structure_loss=0.5765 structure_null=0.0193 structure_real=0.5572, steps/sec=7.65129\n",
      "0.864283752753745\n",
      "[85000]: pnf_loss=0.0828 structure_loss=0.8002 structure_null=0.0154 structure_real=0.7848, steps/sec=1.71908\n",
      "0.9182922410964965\n",
      "epoch 67\n",
      "mem_used 330069504\n",
      "[86000]: pnf_loss=0.1639 structure_loss=0.6246 structure_null=0.0143 structure_real=0.6103, steps/sec=1.76321\n",
      "0.9361494648358459\n",
      "epoch 68\n",
      "mem_used 330591744\n",
      "[87000]: pnf_loss=0.2127 structure_loss=0.9131 structure_null=0.0167 structure_real=0.8964, steps/sec=2.39208\n",
      "0.8806299889004314\n",
      "epoch 69\n",
      "mem_used 331545088\n",
      "[88000]: pnf_loss=0.1876 structure_loss=0.8883 structure_null=0.0147 structure_real=0.8736, steps/sec=3.77890\n",
      "0.8541014754095453\n",
      "epoch 70\n",
      "mem_used 330689024\n",
      "[89000]: pnf_loss=0.1315 structure_loss=0.8173 structure_null=0.0136 structure_real=0.8037, steps/sec=8.49516\n",
      "0.8670991540548221\n",
      "[90000]: pnf_loss=0.0594 structure_loss=0.6603 structure_null=0.0161 structure_real=0.6441, steps/sec=1.72614\n",
      "0.9040187652111054\n",
      "eval Eval_Direc/gu_null_04D_04M_2024Y/04D_04M_2024Y_01h_02m_24s/step_90000\n",
      "[12]: pnf_loss=0.0980 structure_loss=0.6090 structure_null=0.0142 structure_real=0.5948\n",
      "eval_loss 1.4186584 12\n",
      "epoch 71\n",
      "mem_used 331769344\n",
      "[91000]: pnf_loss=0.1453 structure_loss=0.7228 structure_null=0.0141 structure_real=0.7086, steps/sec=1.82627\n",
      "0.8889703924246763\n",
      "epoch 72\n",
      "mem_used 332299264\n",
      "[92000]: pnf_loss=0.0638 structure_loss=0.5982 structure_null=0.0124 structure_real=0.5857, steps/sec=2.51175\n",
      "0.9000396744132563\n",
      "epoch 73\n",
      "mem_used 330740736\n",
      "[93000]: pnf_loss=0.1586 structure_loss=0.7125 structure_null=0.0165 structure_real=0.6960, steps/sec=3.99972\n",
      "0.8841025743373605\n",
      "epoch 74\n",
      "mem_used 332270080\n",
      "[94000]: pnf_loss=0.1015 structure_loss=0.6128 structure_null=0.0170 structure_real=0.5958, steps/sec=9.85099\n",
      "0.8617596409224362\n",
      "[95000]: pnf_loss=0.1108 structure_loss=0.5966 structure_null=0.0148 structure_real=0.5818, steps/sec=1.72150\n",
      "0.8771297914385796\n",
      "epoch 75\n",
      "mem_used 330213376\n",
      "[96000]: pnf_loss=0.1626 structure_loss=1.0177 structure_null=0.0174 structure_real=1.0003, steps/sec=1.87863\n",
      "0.8776905659634994\n",
      "epoch 76\n",
      "mem_used 331284992\n",
      "[97000]: pnf_loss=0.1466 structure_loss=0.5816 structure_null=0.0141 structure_real=0.5675, steps/sec=2.60153\n",
      "0.902734192170214\n",
      "epoch 77\n",
      "mem_used 331326464\n",
      "[98000]: pnf_loss=0.0754 structure_loss=0.6035 structure_null=0.0151 structure_real=0.5885, steps/sec=4.25367\n",
      "0.9358467787355926\n",
      "epoch 78\n",
      "mem_used 329953280\n",
      "[99000]: pnf_loss=0.8442 structure_loss=1.6337 structure_null=0.0198 structure_real=1.6139, steps/sec=11.82080\n",
      "0.8730048101523827\n",
      "[100000]: pnf_loss=0.1464 structure_loss=0.6536 structure_null=0.0203 structure_real=0.6333, steps/sec=1.71298\n",
      "0.8886596411466599\n",
      "eval Eval_Direc/gu_null_04D_04M_2024Y/04D_04M_2024Y_01h_02m_24s/step_100000\n",
      "[12]: pnf_loss=0.0657 structure_loss=0.8596 structure_null=0.0173 structure_real=0.8423\n",
      "eval_loss 1.515896 12\n",
      "epoch 79\n",
      "mem_used 330469888\n",
      "[101000]: pnf_loss=0.1105 structure_loss=0.6024 structure_null=0.0160 structure_real=0.5864, steps/sec=1.93601\n",
      "0.8747764605659623\n",
      "epoch 80\n",
      "mem_used 331731968\n",
      "[102000]: pnf_loss=0.0514 structure_loss=0.6148 structure_null=0.0114 structure_real=0.6034, steps/sec=2.63488\n",
      "0.8745768372494899\n",
      "epoch 81\n",
      "mem_used 332000256\n",
      "[103000]: pnf_loss=0.0911 structure_loss=0.5877 structure_null=0.0143 structure_real=0.5734, steps/sec=4.57951\n",
      "0.8514586816178286\n",
      "epoch 82\n",
      "mem_used 330229760\n",
      "[104000]: pnf_loss=0.1063 structure_loss=0.5750 structure_null=0.0125 structure_real=0.5625, steps/sec=14.54402\n",
      "0.8830920208213676\n",
      "[105000]: pnf_loss=0.1189 structure_loss=0.6900 structure_null=0.0170 structure_real=0.6730, steps/sec=1.71608\n",
      "0.8747328839302063\n",
      "epoch 83\n",
      "mem_used 330668544\n",
      "[106000]: pnf_loss=0.2124 structure_loss=0.8728 structure_null=0.0196 structure_real=0.8532, steps/sec=2.00014\n",
      "0.872446591563003\n",
      "epoch 84\n",
      "mem_used 331034624\n",
      "[107000]: pnf_loss=0.1032 structure_loss=0.5933 structure_null=0.0173 structure_real=0.5761, steps/sec=2.85872\n",
      "0.8730712016028156\n",
      "epoch 85\n",
      "mem_used 330681344\n",
      "[108000]: pnf_loss=0.1087 structure_loss=0.6619 structure_null=0.0144 structure_real=0.6476, steps/sec=5.00510\n",
      "0.8766090818223237\n",
      "epoch 86\n",
      "mem_used 331502080\n",
      "[109000]: pnf_loss=0.1286 structure_loss=0.5642 structure_null=0.0147 structure_real=0.5494, steps/sec=19.23440\n",
      "0.8925287569506785\n",
      "[110000]: pnf_loss=0.0823 structure_loss=0.5990 structure_null=0.0121 structure_real=0.5869, steps/sec=1.71638\n",
      "0.8864085480570794\n",
      "eval Eval_Direc/gu_null_04D_04M_2024Y/04D_04M_2024Y_01h_02m_24s/step_110000\n",
      "[12]: pnf_loss=0.2337 structure_loss=0.8543 structure_null=0.0170 structure_real=0.8373\n",
      "eval_loss 1.460795 12\n",
      "epoch 87\n",
      "mem_used 329598464\n",
      "[111000]: pnf_loss=0.0266 structure_loss=0.7502 structure_null=0.0164 structure_real=0.7338, steps/sec=2.06398\n",
      "0.8485694026861054\n",
      "epoch 88\n",
      "mem_used 330774016\n",
      "[112000]: pnf_loss=0.2069 structure_loss=0.6977 structure_null=0.0168 structure_real=0.6810, steps/sec=2.99182\n",
      "0.8545079810723014\n",
      "epoch 89\n",
      "mem_used 329635840\n",
      "[113000]: pnf_loss=0.1050 structure_loss=0.5785 structure_null=0.0183 structure_real=0.5602, steps/sec=5.40635\n",
      "0.8426308748107286\n",
      "epoch 90\n",
      "mem_used 329852416\n",
      "[114000]: pnf_loss=0.1521 structure_loss=0.7987 structure_null=0.0169 structure_real=0.7818, steps/sec=28.12927\n",
      "1.0862594041668001\n",
      "[115000]: pnf_loss=0.2420 structure_loss=1.0183 structure_null=0.0143 structure_real=1.0040, steps/sec=1.71637\n",
      "0.9034566218256951\n",
      "epoch 91\n",
      "mem_used 330892288\n",
      "[116000]: pnf_loss=0.1014 structure_loss=0.7100 structure_null=0.0155 structure_real=0.6945, steps/sec=2.14653\n",
      "0.8814556400722532\n",
      "epoch 92\n",
      "mem_used 330463232\n",
      "[117000]: pnf_loss=0.1244 structure_loss=0.5767 structure_null=0.0135 structure_real=0.5632, steps/sec=3.14807\n",
      "0.8455095736810451\n",
      "epoch 93\n",
      "mem_used 329718784\n",
      "[118000]: pnf_loss=0.1187 structure_loss=0.6246 structure_null=0.0129 structure_real=0.6117, steps/sec=5.92040\n",
      "0.8487858916151112\n",
      "epoch 94\n",
      "mem_used 331182080\n",
      "[119000]: pnf_loss=0.3801 structure_loss=0.7723 structure_null=0.0161 structure_real=0.7562, steps/sec=51.42525\n",
      "0.8680289080648711\n",
      "[120000]: pnf_loss=0.3243 structure_loss=0.7964 structure_null=0.0148 structure_real=0.7815, steps/sec=1.71851\n",
      "0.8607045551538467\n",
      "eval Eval_Direc/gu_null_04D_04M_2024Y/04D_04M_2024Y_01h_02m_24s/step_120000\n",
      "[12]: pnf_loss=0.9852 structure_loss=1.5799 structure_null=0.0161 structure_real=1.5639\n",
      "eval_loss 1.7082084 12\n",
      "epoch 95\n",
      "mem_used 329567232\n",
      "[121000]: pnf_loss=0.2438 structure_loss=0.7233 structure_null=0.0161 structure_real=0.7071, steps/sec=2.22096\n",
      "0.8899423564524994\n",
      "epoch 96\n",
      "mem_used 330768384\n",
      "[122000]: pnf_loss=0.4001 structure_loss=0.9210 structure_null=0.0117 structure_real=0.9093, steps/sec=3.29804\n",
      "0.9380381384107189\n",
      "epoch 97\n",
      "mem_used 330056192\n",
      "[123000]: pnf_loss=0.0375 structure_loss=0.7030 structure_null=0.0168 structure_real=0.6862, steps/sec=6.43646\n",
      "0.8700579987227461\n",
      "epoch 98\n",
      "mem_used 331697152\n",
      "[124000]: pnf_loss=0.0502 structure_loss=0.6573 structure_null=0.0159 structure_real=0.6414, steps/sec=327.09828\n",
      "0.7465714693069458\n",
      "[125000]: pnf_loss=0.0829 structure_loss=0.6545 structure_null=0.0154 structure_real=0.6392, steps/sec=1.70842\n",
      "0.8532199128866196\n",
      "epoch 99\n",
      "mem_used 330920448\n",
      "[126000]: pnf_loss=0.0501 structure_loss=0.9177 structure_null=0.0148 structure_real=0.9029, steps/sec=2.30468\n",
      "0.8560238610455059\n",
      "epoch 100\n",
      "mem_used 330128896\n",
      "[127000]: pnf_loss=0.1853 structure_loss=0.5639 structure_null=0.0147 structure_real=0.5492, steps/sec=3.48624\n",
      "0.8442553490582406\n",
      "epoch 101\n",
      "mem_used 332159488\n",
      "[128000]: pnf_loss=0.0684 structure_loss=0.6963 structure_null=0.0152 structure_real=0.6810, steps/sec=7.29538\n",
      "0.8567197575018957\n",
      "[129000]: pnf_loss=0.0919 structure_loss=0.6457 structure_null=0.0148 structure_real=0.6310, steps/sec=1.71897\n",
      "0.9212903183102608\n",
      "epoch 102\n",
      "mem_used 330351104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[130000]: pnf_loss=0.1425 structure_loss=0.5702 structure_null=0.0125 structure_real=0.5576, steps/sec=1.76782\n",
      "0.8586553715878645\n",
      "eval Eval_Direc/gu_null_04D_04M_2024Y/04D_04M_2024Y_01h_02m_24s/step_130000\n",
      "[12]: pnf_loss=0.0658 structure_loss=0.7547 structure_null=0.0159 structure_real=0.7388\n",
      "eval_loss 1.5288572 12\n",
      "epoch 103\n",
      "mem_used 331210240\n",
      "[131000]: pnf_loss=0.1254 structure_loss=0.5604 structure_null=0.0167 structure_real=0.5436, steps/sec=2.38237\n",
      "0.8560249694519573\n",
      "epoch 104\n",
      "mem_used 330103808\n",
      "[132000]: pnf_loss=0.0691 structure_loss=0.8250 structure_null=0.0135 structure_real=0.8115, steps/sec=3.69422\n",
      "0.8336531939568324\n",
      "epoch 105\n",
      "mem_used 330509312\n",
      "[133000]: pnf_loss=0.0930 structure_loss=0.5662 structure_null=0.0113 structure_real=0.5549, steps/sec=8.34046\n",
      "0.8845334278727041\n",
      "[134000]: pnf_loss=0.0812 structure_loss=0.6891 structure_null=0.0168 structure_real=0.6722, steps/sec=1.71743\n",
      "0.8535720794796944\n",
      "epoch 106\n",
      "mem_used 331548672\n",
      "[135000]: pnf_loss=0.1223 structure_loss=0.6077 structure_null=0.0143 structure_real=0.5933, steps/sec=1.80815\n",
      "0.8416835282574966\n",
      "epoch 107\n",
      "mem_used 331772416\n",
      "[136000]: pnf_loss=0.0569 structure_loss=0.7987 structure_null=0.0145 structure_real=0.7842, steps/sec=2.48097\n",
      "0.8538476631820546\n",
      "epoch 108\n",
      "mem_used 330625024\n",
      "[137000]: pnf_loss=0.0487 structure_loss=0.6318 structure_null=0.0140 structure_real=0.6178, steps/sec=3.94217\n",
      "0.8549792900852773\n",
      "epoch 109\n",
      "mem_used 329633792\n",
      "[138000]: pnf_loss=0.0599 structure_loss=0.5956 structure_null=0.0131 structure_real=0.5824, steps/sec=9.58039\n",
      "0.8096825296289465\n",
      "[139000]: pnf_loss=0.0688 structure_loss=0.6058 structure_null=0.0198 structure_real=0.5859, steps/sec=1.71710\n",
      "0.8743262741565704\n",
      "epoch 110\n",
      "mem_used 331521536\n",
      "[140000]: pnf_loss=0.2525 structure_loss=0.7735 structure_null=0.0138 structure_real=0.7597, steps/sec=1.86835\n",
      "0.8636040894656435\n",
      "eval Eval_Direc/gu_null_04D_04M_2024Y/04D_04M_2024Y_01h_02m_24s/step_140000\n",
      "[12]: pnf_loss=0.0766 structure_loss=0.5680 structure_null=0.0157 structure_real=0.5522\n",
      "eval_loss 1.5103997 12\n",
      "epoch 111\n",
      "mem_used 332980224\n",
      "[141000]: pnf_loss=0.1156 structure_loss=0.6569 structure_null=0.0178 structure_real=0.6391, steps/sec=2.61098\n",
      "0.8437520024108599\n",
      "epoch 112\n",
      "mem_used 330027520\n",
      "[142000]: pnf_loss=0.5415 structure_loss=1.6259 structure_null=0.0167 structure_real=1.6092, steps/sec=4.28625\n",
      "0.8380194876059267\n",
      "epoch 113\n",
      "mem_used 329900544\n",
      "[143000]: pnf_loss=0.2213 structure_loss=0.7756 structure_null=0.0117 structure_real=0.7639, steps/sec=11.55403\n",
      "0.9971925596396128\n",
      "[144000]: pnf_loss=0.1365 structure_loss=0.5758 structure_null=0.0160 structure_real=0.5598, steps/sec=1.72205\n",
      "0.8612310608029365\n",
      "epoch 114\n",
      "mem_used 331865088\n",
      "[145000]: pnf_loss=0.0598 structure_loss=0.6476 structure_null=0.0166 structure_real=0.6310, steps/sec=1.95111\n",
      "0.8410852514749678\n",
      "epoch 115\n",
      "mem_used 330647040\n",
      "[146000]: pnf_loss=0.0936 structure_loss=0.6121 structure_null=0.0147 structure_real=0.5973, steps/sec=2.71015\n",
      "0.8711125420511894\n",
      "epoch 116\n",
      "mem_used 330858496\n",
      "[147000]: pnf_loss=0.1781 structure_loss=0.5699 structure_null=0.0129 structure_real=0.5571, steps/sec=4.59361\n",
      "0.8326272961332175\n",
      "epoch 117\n",
      "mem_used 329889792\n",
      "[148000]: pnf_loss=0.1484 structure_loss=0.6344 structure_null=0.0145 structure_real=0.6199, steps/sec=14.23922\n",
      "0.809707302539075\n",
      "[149000]: pnf_loss=0.1097 structure_loss=0.6016 structure_null=0.0118 structure_real=0.5897, steps/sec=1.72207\n",
      "0.8427244549989701\n",
      "epoch 118\n",
      "mem_used 329680384\n",
      "[150000]: pnf_loss=0.1075 structure_loss=0.5999 structure_null=0.0118 structure_real=0.5881, steps/sec=1.99699\n",
      "0.8540519305736344\n",
      "eval Eval_Direc/gu_null_04D_04M_2024Y/04D_04M_2024Y_01h_02m_24s/step_150000\n",
      "[12]: pnf_loss=0.0301 structure_loss=0.6154 structure_null=0.0146 structure_real=0.6007\n",
      "eval_loss 1.4457932 12\n",
      "epoch 119\n",
      "mem_used 331625472\n",
      "[151000]: pnf_loss=0.3600 structure_loss=0.8202 structure_null=0.0158 structure_real=0.8045, steps/sec=2.83378\n",
      "0.8355350151265922\n",
      "epoch 120\n",
      "mem_used 330355712\n",
      "[152000]: pnf_loss=0.1000 structure_loss=0.6726 structure_null=0.0126 structure_real=0.6599, steps/sec=4.89119\n",
      "0.8335003221136892\n",
      "epoch 121\n",
      "mem_used 329880064\n",
      "[153000]: pnf_loss=0.3325 structure_loss=0.8682 structure_null=0.0147 structure_real=0.8535, steps/sec=18.19595\n",
      "0.8038025048184902\n",
      "[154000]: pnf_loss=0.1067 structure_loss=0.5724 structure_null=0.0185 structure_real=0.5539, steps/sec=1.70730\n",
      "0.8301515757441521\n",
      "epoch 122\n",
      "mem_used 330443264\n",
      "[155000]: pnf_loss=0.0965 structure_loss=0.6259 structure_null=0.0156 structure_real=0.6104, steps/sec=2.03805\n",
      "0.844253183336953\n",
      "epoch 123\n",
      "mem_used 331208704\n",
      "[156000]: pnf_loss=0.0416 structure_loss=0.6275 structure_null=0.0162 structure_real=0.6113, steps/sec=2.94407\n",
      "0.8411417597326739\n",
      "epoch 124\n",
      "mem_used 331243008\n",
      "[157000]: pnf_loss=0.4048 structure_loss=0.8561 structure_null=0.0154 structure_real=0.8406, steps/sec=5.31698\n",
      "0.8351193122081343\n",
      "epoch 125\n",
      "mem_used 329567232\n",
      "[158000]: pnf_loss=0.1281 structure_loss=0.5637 structure_null=0.0147 structure_real=0.5490, steps/sec=25.85642\n",
      "0.8580875080643278\n",
      "[159000]: pnf_loss=0.0912 structure_loss=0.5748 structure_null=0.0126 structure_real=0.5623, steps/sec=1.72659\n",
      "0.8428841716647149\n",
      "epoch 126\n",
      "mem_used 330228224\n",
      "[160000]: pnf_loss=0.0488 structure_loss=0.6720 structure_null=0.0151 structure_real=0.6569, steps/sec=2.12419\n",
      "0.858325759397891\n",
      "eval Eval_Direc/gu_null_04D_04M_2024Y/04D_04M_2024Y_01h_02m_24s/step_160000\n",
      "[12]: pnf_loss=0.0849 structure_loss=0.6323 structure_null=0.0200 structure_real=0.6122\n",
      "eval_loss 1.4464049 12\n",
      "epoch 127\n",
      "mem_used 330144768\n",
      "[161000]: pnf_loss=0.0556 structure_loss=0.6921 structure_null=0.0131 structure_real=0.6789, steps/sec=3.10686\n",
      "0.8771230340867803\n",
      "epoch 128\n",
      "mem_used 329805312\n",
      "[162000]: pnf_loss=0.0699 structure_loss=0.5967 structure_null=0.0150 structure_real=0.5817, steps/sec=5.78076\n",
      "0.8290600239220313\n",
      "epoch 129\n",
      "mem_used 330507776\n",
      "[163000]: pnf_loss=0.1309 structure_loss=0.5860 structure_null=0.0170 structure_real=0.5690, steps/sec=45.29257\n",
      "0.7880857351579165\n",
      "[164000]: pnf_loss=0.1498 structure_loss=0.6872 structure_null=0.0140 structure_real=0.6732, steps/sec=1.71267\n",
      "0.8577004034519196\n",
      "epoch 130\n",
      "mem_used 331405824\n",
      "[165000]: pnf_loss=0.0659 structure_loss=0.6743 structure_null=0.0164 structure_real=0.6579, steps/sec=2.21450\n",
      "0.8311217774502294\n",
      "epoch 131\n",
      "mem_used 330181120\n",
      "[166000]: pnf_loss=0.2128 structure_loss=0.5723 structure_null=0.0103 structure_real=0.5619, steps/sec=3.28296\n",
      "0.8338587538886616\n",
      "epoch 132\n",
      "mem_used 330898432\n",
      "[167000]: pnf_loss=0.1347 structure_loss=0.5606 structure_null=0.0156 structure_real=0.5450, steps/sec=6.38822\n",
      "0.7952315945750319\n",
      "epoch 133\n",
      "mem_used 330547200\n",
      "[168000]: pnf_loss=0.0956 structure_loss=0.6021 structure_null=0.0125 structure_real=0.5896, steps/sec=171.49080\n",
      "1.00418119430542\n",
      "[169000]: pnf_loss=0.0924 structure_loss=0.6483 structure_null=0.0117 structure_real=0.6366, steps/sec=1.70555\n",
      "0.906505625963211\n",
      "epoch 134\n",
      "mem_used 330401792\n",
      "[170000]: pnf_loss=0.0584 structure_loss=0.8597 structure_null=0.0157 structure_real=0.8440, steps/sec=2.27317\n",
      "0.8448091868068751\n",
      "eval Eval_Direc/gu_null_04D_04M_2024Y/04D_04M_2024Y_01h_02m_24s/step_170000\n",
      "[12]: pnf_loss=0.1009 structure_loss=0.6303 structure_null=0.0150 structure_real=0.6153\n",
      "eval_loss 1.780499 12\n",
      "epoch 135\n",
      "mem_used 330696192\n",
      "[171000]: pnf_loss=0.1487 structure_loss=0.7246 structure_null=0.0173 structure_real=0.7073, steps/sec=3.45032\n",
      "0.8715658840392867\n",
      "epoch 136\n",
      "mem_used 329907200\n",
      "[172000]: pnf_loss=0.1700 structure_loss=0.6910 structure_null=0.0156 structure_real=0.6754, steps/sec=7.11870\n",
      "0.8277012788102218\n",
      "[173000]: pnf_loss=0.0778 structure_loss=0.6046 structure_null=0.0140 structure_real=0.5906, steps/sec=1.70881\n",
      "0.8344318888783455\n",
      "epoch 137\n",
      "mem_used 330506240\n",
      "[174000]: pnf_loss=0.0612 structure_loss=0.5804 structure_null=0.0167 structure_real=0.5637, steps/sec=1.73716\n",
      "1.0448994209834368\n",
      "epoch 138\n",
      "mem_used 332332544\n",
      "[175000]: pnf_loss=0.0871 structure_loss=0.6168 structure_null=0.0184 structure_real=0.5984, steps/sec=2.35592\n",
      "0.8484690217314095\n",
      "epoch 139\n",
      "mem_used 330536448\n",
      "[176000]: pnf_loss=0.1279 structure_loss=0.6145 structure_null=0.0176 structure_real=0.5969, steps/sec=3.66089\n",
      "0.832813531287715\n",
      "epoch 140\n",
      "mem_used 329718784\n",
      "[177000]: pnf_loss=0.1560 structure_loss=0.6573 structure_null=0.0100 structure_real=0.6473, steps/sec=8.07232\n",
      "0.8016188280277342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[178000]: pnf_loss=0.0354 structure_loss=0.7121 structure_null=0.0144 structure_real=0.6977, steps/sec=1.72554\n",
      "0.9029266076683998\n",
      "epoch 141\n",
      "mem_used 329800704\n",
      "[179000]: pnf_loss=0.5797 structure_loss=2.4319 structure_null=0.0159 structure_real=2.4161, steps/sec=1.78582\n",
      "0.8302507763513729\n",
      "epoch 142\n",
      "mem_used 330312192\n",
      "[180000]: pnf_loss=0.1269 structure_loss=0.6935 structure_null=0.0141 structure_real=0.6794, steps/sec=2.45129\n",
      "0.8464319181750118\n",
      "eval Eval_Direc/gu_null_04D_04M_2024Y/04D_04M_2024Y_01h_02m_24s/step_180000\n",
      "[12]: pnf_loss=0.0973 structure_loss=0.5615 structure_null=0.0143 structure_real=0.5472\n",
      "eval_loss 1.460657 12\n",
      "epoch 143\n",
      "mem_used 330071552\n",
      "[181000]: pnf_loss=0.0862 structure_loss=0.6205 structure_null=0.0176 structure_real=0.6029, steps/sec=3.88938\n",
      "0.8416264672170986\n",
      "epoch 144\n",
      "mem_used 331028480\n",
      "[182000]: pnf_loss=0.0691 structure_loss=0.7584 structure_null=0.0156 structure_real=0.7428, steps/sec=9.33368\n",
      "0.8128469824139538\n",
      "[183000]: pnf_loss=0.1256 structure_loss=0.6132 structure_null=0.0164 structure_real=0.5968, steps/sec=1.71365\n",
      "0.8473318105936051\n",
      "epoch 145\n",
      "mem_used 329800704\n",
      "[184000]: pnf_loss=0.0898 structure_loss=0.6366 structure_null=0.0117 structure_real=0.6249, steps/sec=1.84626\n",
      "0.8240950583896678\n",
      "epoch 146\n",
      "mem_used 330331136\n",
      "[185000]: pnf_loss=0.1261 structure_loss=0.5635 structure_null=0.0151 structure_real=0.5484, steps/sec=2.55887\n",
      "0.8175331024489239\n",
      "epoch 147\n",
      "mem_used 330411520\n",
      "[186000]: pnf_loss=0.1008 structure_loss=0.6672 structure_null=0.0114 structure_real=0.6558, steps/sec=4.10590\n",
      "0.8118433176892476\n",
      "epoch 148\n",
      "mem_used 331048960\n",
      "[187000]: pnf_loss=0.1527 structure_loss=0.5914 structure_null=0.0135 structure_real=0.5779, steps/sec=11.01248\n",
      "0.8404593448485097\n",
      "[188000]: pnf_loss=0.0522 structure_loss=0.7274 structure_null=0.0175 structure_real=0.7100, steps/sec=1.70859\n",
      "0.8395643698573112\n",
      "epoch 149\n",
      "mem_used 331199488\n",
      "[189000]: pnf_loss=0.0330 structure_loss=0.7720 structure_null=0.0117 structure_real=0.7603, steps/sec=1.90393\n",
      "0.8461330134778352\n",
      "epoch 150\n",
      "mem_used 331170304\n",
      "[190000]: pnf_loss=0.0648 structure_loss=0.6011 structure_null=0.0137 structure_real=0.5873, steps/sec=2.66599\n",
      "0.844730022173032\n",
      "eval Eval_Direc/gu_null_04D_04M_2024Y/04D_04M_2024Y_01h_02m_24s/step_190000\n",
      "[12]: pnf_loss=0.1020 structure_loss=0.6560 structure_null=0.0163 structure_real=0.6397\n",
      "eval_loss 1.7299557 12\n",
      "epoch 151\n",
      "mem_used 330773504\n",
      "[191000]: pnf_loss=0.0470 structure_loss=0.7267 structure_null=0.0152 structure_real=0.7115, steps/sec=4.44881\n",
      "0.8181642621445159\n",
      "epoch 152\n",
      "mem_used 330699264\n",
      "[192000]: pnf_loss=0.1400 structure_loss=0.6653 structure_null=0.0130 structure_real=0.6522, steps/sec=13.45167\n",
      "0.8148233740348515\n",
      "[193000]: pnf_loss=0.0774 structure_loss=0.5692 structure_null=0.0144 structure_real=0.5548, steps/sec=1.71029\n",
      "0.8434819069504738\n",
      "epoch 153\n",
      "mem_used 330193408\n",
      "[194000]: pnf_loss=0.0302 structure_loss=0.7510 structure_null=0.0144 structure_real=0.7366, steps/sec=1.96091\n",
      "0.8404928528714454\n",
      "epoch 154\n",
      "mem_used 331235328\n",
      "[195000]: pnf_loss=0.0769 structure_loss=0.6672 structure_null=0.0154 structure_real=0.6517, steps/sec=2.78410\n",
      "0.8017417410264007\n",
      "epoch 155\n",
      "mem_used 330105344\n",
      "[196000]: pnf_loss=0.6120 structure_loss=1.6586 structure_null=0.0196 structure_real=1.6390, steps/sec=4.85005\n",
      "0.8429858535528183\n",
      "epoch 156\n",
      "mem_used 330216448\n",
      "[197000]: pnf_loss=0.1337 structure_loss=0.7067 structure_null=0.0161 structure_real=0.6906, steps/sec=13.88338\n",
      "0.8567828776860478\n",
      "[198000]: pnf_loss=0.0853 structure_loss=0.6758 structure_null=0.0129 structure_real=0.6629, steps/sec=0.42664\n",
      "0.8631963772773743\n"
     ]
    }
   ],
   "source": [
    "exp.start_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "903b6477",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to add eval code, / dump conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c1bbc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_file = 'GUN_checkpoints/gu_null/01D_04M_2024Y_00h_20m_09s/step_120000.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "367cdc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_model=torch.load(checkpoint_file)['model']\n",
    "ckpt_opt = torch.load(checkpoint_file)['optimizer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1816e0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = {'batch_size'  : 4,\n",
    "                          'topk'  : 4,\n",
    "                        'stride'  : 8,\n",
    "                            'KNN' : 30,\n",
    "                      'num_heads' : 16,\n",
    "                       'channels' : 64,\n",
    "                   'channels_div' : 8,\n",
    "                     'num_layers' : 1,\n",
    "                 'num_layers_ca'  : 2,\n",
    "               'edge_feature_dim' : 1,\n",
    "              'latent_pool_type'  : 'avg',\n",
    "                        't_size'  : 12,\n",
    "                         'max_t'  : 1.0,\n",
    "                           'mult' : 2,\n",
    "                       'zero_lin' : True,\n",
    "                      'use_tdeg1' : False,\n",
    "                       'roll': False,\n",
    "                       'circ_pe':False,\n",
    "                            'cuda': True,\n",
    "                  'learning rate' : 0.0001,\n",
    "                   'weight_decay' : 0.000001,\n",
    "                    'sc_nf_real'  : 0.05 ,\n",
    "                    'sc_3D_real'  : 1.0 ,\n",
    "                    'sc_3D_null'  : 0.05 ,\n",
    "                    'device'      : 'cuda',\n",
    "                    'num_epoch'   : 100,\n",
    "                    'log_freq'    : 1000,\n",
    "                    'ckpt_freq'   : 10000,\n",
    "                    'early_chkpt' : 2,\n",
    "                    'coord_scale' : 10.0,\n",
    "                    'nf_threshold_real': 1.99,\n",
    "                    'nf_dim': 5}\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7970fc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = Experiment(conf=conf,ckpt_model=ckpt_model,ckpt_opt=ckpt_opt, cur_step=120000, cur_epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2cf6e82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "number of epochs 100\n",
      "epoch 100\n",
      "mem_used 401532928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nwoodall/miniconda3/envs/se33/lib/python3.9/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
      "/home/nwoodall/miniconda3/envs/se33/lib/python3.9/site-packages/dgl/backend/pytorch/tensor.py:449: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[121000]: pnf_loss=0.1735 structure_loss=0.7181 structure_null=0.0209 structure_real=0.6973, steps/sec=1.56234\n",
      "0.9263865057229995\n",
      "epoch 101\n",
      "mem_used 465145856\n",
      "[122000]: pnf_loss=0.1695 structure_loss=0.5928 structure_null=0.0203 structure_real=0.5725, steps/sec=2.13181\n",
      "0.8799873555171859\n",
      "epoch 102\n",
      "mem_used 463135232\n",
      "[123000]: pnf_loss=0.1060 structure_loss=0.6315 structure_null=0.0227 structure_real=0.6088, steps/sec=3.27743\n",
      "0.9120104858659422\n",
      "epoch 103\n",
      "mem_used 463700992\n",
      "[124000]: pnf_loss=0.3352 structure_loss=0.6810 structure_null=0.0172 structure_real=0.6638, steps/sec=6.88224\n",
      "0.8736528991091199\n",
      "[125000]: pnf_loss=0.1463 structure_loss=0.6399 structure_null=0.0204 structure_real=0.6196, steps/sec=1.57897\n",
      "0.8571856165528298\n",
      "epoch 104\n",
      "mem_used 463290368\n",
      "[126000]: pnf_loss=0.1206 structure_loss=0.5740 structure_null=0.0209 structure_real=0.5531, steps/sec=1.64577\n",
      "0.8636106863433932\n",
      "epoch 105\n",
      "mem_used 463852544\n",
      "[127000]: pnf_loss=0.1338 structure_loss=0.5935 structure_null=0.0173 structure_real=0.5761, steps/sec=2.29254\n",
      "0.8832465587796031\n",
      "epoch 106\n",
      "mem_used 463404032\n",
      "[128000]: pnf_loss=0.1968 structure_loss=0.6924 structure_null=0.0137 structure_real=0.6788, steps/sec=3.55027\n",
      "0.9176685787965116\n",
      "epoch 107\n",
      "mem_used 464150528\n",
      "[129000]: pnf_loss=0.1641 structure_loss=0.6195 structure_null=0.0175 structure_real=0.6020, steps/sec=8.01236\n",
      "0.8610062747452389\n",
      "[130000]: pnf_loss=0.1370 structure_loss=0.5678 structure_null=0.0172 structure_real=0.5506, steps/sec=1.61114\n",
      "0.8814027747511863\n",
      "eval Eval_Direc/gu_null_02D_04M_2024Y/02D_04M_2024Y_00h_05m_26s/step_130000\n",
      "[12]: pnf_loss=0.1953 structure_loss=0.7170 structure_null=0.0177 structure_real=0.6993\n",
      "eval_loss 1.5195813 12\n",
      "epoch 108\n",
      "mem_used 463120384\n",
      "[131000]: pnf_loss=0.0687 structure_loss=0.7196 structure_null=0.0224 structure_real=0.6973, steps/sec=1.72070\n",
      "0.8577337952614841\n",
      "epoch 109\n",
      "mem_used 463644160\n",
      "[132000]: pnf_loss=0.1858 structure_loss=0.6018 structure_null=0.0216 structure_real=0.5802, steps/sec=2.36123\n",
      "0.8708144377759798\n",
      "epoch 110\n",
      "mem_used 465406976\n",
      "[133000]: pnf_loss=0.1233 structure_loss=0.5788 structure_null=0.0199 structure_real=0.5589, steps/sec=3.82631\n",
      "0.8322939881058626\n",
      "epoch 111\n",
      "mem_used 463967744\n",
      "[134000]: pnf_loss=0.0920 structure_loss=0.6512 structure_null=0.0192 structure_real=0.6320, steps/sec=9.49194\n",
      "0.8791375546097067\n",
      "[135000]: pnf_loss=0.0883 structure_loss=0.6740 structure_null=0.0176 structure_real=0.6564, steps/sec=1.62016\n",
      "0.8486765618920327\n",
      "epoch 112\n",
      "mem_used 463968256\n",
      "[136000]: pnf_loss=0.2028 structure_loss=0.6171 structure_null=0.0211 structure_real=0.5960, steps/sec=1.76786\n",
      "0.859101896267791\n",
      "epoch 113\n",
      "mem_used 463110656\n",
      "[137000]: pnf_loss=0.1046 structure_loss=0.6789 structure_null=0.0189 structure_real=0.6600, steps/sec=2.45865\n",
      "0.8476295559829573\n",
      "epoch 114\n",
      "mem_used 465403392\n",
      "[138000]: pnf_loss=0.1690 structure_loss=0.6341 structure_null=0.0190 structure_real=0.6151, steps/sec=4.00304\n",
      "0.8335090119447282\n",
      "epoch 115\n",
      "mem_used 463333888\n",
      "[139000]: pnf_loss=0.0840 structure_loss=0.5694 structure_null=0.0147 structure_real=0.5547, steps/sec=11.13195\n",
      "0.9421860629114611\n",
      "[140000]: pnf_loss=0.1700 structure_loss=0.6520 structure_null=0.0178 structure_real=0.6342, steps/sec=1.55974\n",
      "0.8314857862591744\n",
      "eval Eval_Direc/gu_null_02D_04M_2024Y/02D_04M_2024Y_00h_05m_26s/step_140000\n",
      "[12]: pnf_loss=0.0962 structure_loss=0.6189 structure_null=0.0169 structure_real=0.6019\n",
      "eval_loss 1.4880633 12\n",
      "epoch 116\n",
      "mem_used 463317504\n",
      "[141000]: pnf_loss=0.0565 structure_loss=0.7952 structure_null=0.0178 structure_real=0.7774, steps/sec=1.70792\n",
      "0.8322291525231825\n",
      "epoch 117\n",
      "mem_used 462991360\n",
      "[142000]: pnf_loss=0.0862 structure_loss=0.6083 structure_null=0.0199 structure_real=0.5884, steps/sec=2.38543\n",
      "0.8210867422318496\n",
      "epoch 118\n",
      "mem_used 464412672\n",
      "[143000]: pnf_loss=0.1085 structure_loss=0.6455 structure_null=0.0175 structure_real=0.6280, steps/sec=3.98169\n",
      "0.8097870298885407\n",
      "epoch 119\n",
      "mem_used 462773248\n",
      "[144000]: pnf_loss=0.0775 structure_loss=0.6250 structure_null=0.0165 structure_real=0.6085, steps/sec=12.70277\n",
      "0.8519817779206822\n",
      "[145000]: pnf_loss=0.1406 structure_loss=0.5850 structure_null=0.0187 structure_real=0.5662, steps/sec=1.52997\n",
      "0.8312387509942055\n",
      "epoch 120\n",
      "mem_used 464705536\n",
      "[146000]: pnf_loss=0.1293 structure_loss=0.5899 structure_null=0.0169 structure_real=0.5729, steps/sec=1.78682\n",
      "0.8149659760469614\n",
      "epoch 121\n",
      "mem_used 464461824\n",
      "[147000]: pnf_loss=0.1804 structure_loss=0.6590 structure_null=0.0159 structure_real=0.6430, steps/sec=2.52361\n",
      "0.8204948784502387\n",
      "epoch 122\n",
      "mem_used 465816064\n",
      "[148000]: pnf_loss=0.1712 structure_loss=0.5723 structure_null=0.0154 structure_real=0.5570, steps/sec=4.41570\n",
      "0.8565193313050132\n",
      "epoch 123\n",
      "mem_used 463828480\n",
      "[149000]: pnf_loss=0.1129 structure_loss=0.5862 structure_null=0.0175 structure_real=0.5686, steps/sec=17.11868\n",
      "0.8022588585199935\n",
      "[150000]: pnf_loss=0.1932 structure_loss=0.6379 structure_null=0.0149 structure_real=0.6230, steps/sec=1.53537\n",
      "0.8226616426110268\n",
      "eval Eval_Direc/gu_null_02D_04M_2024Y/02D_04M_2024Y_00h_05m_26s/step_150000\n",
      "[12]: pnf_loss=0.1275 structure_loss=0.6633 structure_null=0.0161 structure_real=0.6473\n",
      "eval_loss 1.4168538 12\n",
      "epoch 124\n",
      "mem_used 464559104\n",
      "[151000]: pnf_loss=0.1214 structure_loss=0.6208 structure_null=0.0184 structure_real=0.6024, steps/sec=1.83699\n",
      "0.8132018962015326\n",
      "epoch 125\n",
      "mem_used 463544320\n",
      "[152000]: pnf_loss=0.0819 structure_loss=0.6530 structure_null=0.0152 structure_real=0.6379, steps/sec=2.64793\n",
      "0.7838818382180255\n",
      "epoch 126\n",
      "mem_used 463682048\n",
      "[153000]: pnf_loss=0.2722 structure_loss=0.7163 structure_null=0.0138 structure_real=0.7024, steps/sec=4.78319\n",
      "0.8116797651134947\n",
      "epoch 127\n",
      "mem_used 464945664\n",
      "[154000]: pnf_loss=0.0996 structure_loss=0.6007 structure_null=0.0177 structure_real=0.5829, steps/sec=25.16561\n",
      "0.7899391162590902\n",
      "[155000]: pnf_loss=0.0792 structure_loss=0.6437 structure_null=0.0173 structure_real=0.6264, steps/sec=1.52497\n",
      "0.8207108935713768\n",
      "epoch 128\n",
      "mem_used 464507904\n",
      "[156000]: pnf_loss=0.1293 structure_loss=0.5604 structure_null=0.0164 structure_real=0.5440, steps/sec=1.89295\n",
      "0.8331557295808745\n",
      "epoch 129\n",
      "mem_used 464578560\n",
      "[157000]: pnf_loss=0.0717 structure_loss=0.6502 structure_null=0.0205 structure_real=0.6297, steps/sec=2.80240\n",
      "0.8395044294746092\n",
      "epoch 130\n",
      "mem_used 463600128\n",
      "[158000]: pnf_loss=0.0856 structure_loss=0.5988 structure_null=0.0167 structure_real=0.5821, steps/sec=5.24992\n",
      "0.8104630772409768\n",
      "epoch 131\n",
      "mem_used 464102912\n",
      "[159000]: pnf_loss=0.0820 structure_loss=0.6340 structure_null=0.0195 structure_real=0.6144, steps/sec=45.13452\n",
      "0.850203275680542\n",
      "[160000]: pnf_loss=0.0702 structure_loss=0.7685 structure_null=0.0169 structure_real=0.7516, steps/sec=1.49156\n",
      "0.8237156074047088\n",
      "eval Eval_Direc/gu_null_02D_04M_2024Y/02D_04M_2024Y_00h_05m_26s/step_160000\n",
      "[12]: pnf_loss=0.1417 structure_loss=0.6114 structure_null=0.0154 structure_real=0.5960\n",
      "eval_loss 1.4240495 12\n",
      "epoch 132\n",
      "mem_used 464038912\n",
      "[161000]: pnf_loss=0.0468 structure_loss=0.6828 structure_null=0.0146 structure_real=0.6681, steps/sec=1.97990\n",
      "0.8403958514640012\n",
      "epoch 133\n",
      "mem_used 465006592\n",
      "[162000]: pnf_loss=0.1238 structure_loss=0.6208 structure_null=0.0166 structure_real=0.6042, steps/sec=2.97080\n",
      "0.7946849389572364\n",
      "epoch 134\n",
      "mem_used 463678976\n",
      "[163000]: pnf_loss=0.0447 structure_loss=0.6819 structure_null=0.0183 structure_real=0.6636, steps/sec=5.83714\n",
      "0.7913874119292689\n",
      "epoch 135\n",
      "mem_used 464215040\n",
      "[164000]: pnf_loss=0.1430 structure_loss=0.6843 structure_null=0.0182 structure_real=0.6661, steps/sec=310.56296\n",
      "0.7433449983596802\n",
      "[165000]: pnf_loss=0.1301 structure_loss=0.6314 structure_null=0.0185 structure_real=0.6129, steps/sec=1.51691\n",
      "0.7939557795524597\n",
      "epoch 136\n",
      "mem_used 463127552\n",
      "[166000]: pnf_loss=0.0984 structure_loss=0.6182 structure_null=0.0146 structure_real=0.6036, steps/sec=2.04201\n",
      "0.8267436707242924\n",
      "epoch 137\n",
      "mem_used 463867904\n",
      "[167000]: pnf_loss=0.2824 structure_loss=0.6651 structure_null=0.0162 structure_real=0.6489, steps/sec=3.07868\n",
      "0.9215983843123597\n",
      "epoch 138\n",
      "mem_used 465748480\n",
      "[168000]: pnf_loss=0.0865 structure_loss=0.6716 structure_null=0.0177 structure_real=0.6539, steps/sec=6.52660\n",
      "0.7849864995377696\n",
      "[169000]: pnf_loss=0.2233 structure_loss=0.8113 structure_null=0.0135 structure_real=0.7979, steps/sec=1.51309\n",
      "0.7873495411276817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 139\n",
      "mem_used 465440256\n",
      "[170000]: pnf_loss=0.0883 structure_loss=0.6641 structure_null=0.0147 structure_real=0.6494, steps/sec=1.54717\n",
      "0.8603988191610471\n",
      "eval Eval_Direc/gu_null_02D_04M_2024Y/02D_04M_2024Y_00h_05m_26s/step_170000\n",
      "[12]: pnf_loss=0.0660 structure_loss=0.5943 structure_null=0.0159 structure_real=0.5784\n",
      "eval_loss 1.3883824 12\n",
      "epoch 140\n",
      "mem_used 465761280\n",
      "[171000]: pnf_loss=0.0746 structure_loss=0.6959 structure_null=0.0156 structure_real=0.6803, steps/sec=2.11030\n",
      "0.7934505792127715\n",
      "epoch 141\n",
      "mem_used 465282048\n",
      "[172000]: pnf_loss=0.0336 structure_loss=0.6674 structure_null=0.0144 structure_real=0.6530, steps/sec=3.30337\n",
      "0.8067492026489703\n",
      "epoch 142\n",
      "mem_used 463734272\n",
      "[173000]: pnf_loss=0.1136 structure_loss=0.7646 structure_null=0.0161 structure_real=0.7484, steps/sec=7.35566\n",
      "0.8288495410414576\n",
      "[174000]: pnf_loss=0.1523 structure_loss=0.6747 structure_null=0.0156 structure_real=0.6591, steps/sec=1.52376\n",
      "0.8045203531384468\n",
      "epoch 143\n",
      "mem_used 463595520\n",
      "[175000]: pnf_loss=0.1101 structure_loss=0.5620 structure_null=0.0152 structure_real=0.5469, steps/sec=1.59921\n",
      "0.7983800386352459\n",
      "epoch 144\n",
      "mem_used 463009280\n",
      "[176000]: pnf_loss=0.1282 structure_loss=0.5818 structure_null=0.0184 structure_real=0.5634, steps/sec=2.18367\n",
      "0.8114226835139225\n",
      "epoch 145\n",
      "mem_used 464025088\n",
      "[177000]: pnf_loss=0.2502 structure_loss=0.6325 structure_null=0.0166 structure_real=0.6160, steps/sec=3.48262\n",
      "0.8108262810213812\n",
      "epoch 146\n",
      "mem_used 464438784\n",
      "[178000]: pnf_loss=0.0889 structure_loss=0.6476 structure_null=0.0160 structure_real=0.6316, steps/sec=8.50791\n",
      "0.7976430209165208\n",
      "[179000]: pnf_loss=0.5645 structure_loss=0.7546 structure_null=0.0183 structure_real=0.7363, steps/sec=1.52933\n",
      "0.801581798017025\n",
      "epoch 147\n",
      "mem_used 465326592\n",
      "[180000]: pnf_loss=0.1487 structure_loss=0.6367 structure_null=0.0132 structure_real=0.6235, steps/sec=1.63679\n",
      "0.8265695651501709\n",
      "eval Eval_Direc/gu_null_02D_04M_2024Y/02D_04M_2024Y_00h_05m_26s/step_180000\n",
      "[12]: pnf_loss=0.0544 structure_loss=0.7066 structure_null=0.0134 structure_real=0.6932\n",
      "eval_loss 1.5306884 12\n",
      "epoch 148\n",
      "mem_used 463821824\n",
      "[181000]: pnf_loss=0.0918 structure_loss=0.6383 structure_null=0.0134 structure_real=0.6249, steps/sec=2.27259\n",
      "0.7968654973679278\n",
      "epoch 149\n",
      "mem_used 464461824\n",
      "[182000]: pnf_loss=0.0474 structure_loss=0.6715 structure_null=0.0163 structure_real=0.6553, steps/sec=3.72101\n",
      "0.7955185735840762\n",
      "epoch 150\n",
      "mem_used 464546816\n",
      "[183000]: pnf_loss=0.0793 structure_loss=0.5940 structure_null=0.0167 structure_real=0.5773, steps/sec=10.18013\n",
      "0.8523088490962982\n",
      "[184000]: pnf_loss=0.0744 structure_loss=0.6211 structure_null=0.0149 structure_real=0.6062, steps/sec=1.51503\n",
      "0.8093266533613205\n",
      "epoch 151\n",
      "mem_used 464615424\n",
      "[185000]: pnf_loss=0.0886 structure_loss=0.6027 structure_null=0.0165 structure_real=0.5863, steps/sec=1.71170\n",
      "0.7961342950817719\n",
      "epoch 152\n",
      "mem_used 463093248\n",
      "[186000]: pnf_loss=0.0941 structure_loss=0.6505 structure_null=0.0155 structure_real=0.6350, steps/sec=2.39188\n",
      "0.8157707238159839\n",
      "epoch 153\n",
      "mem_used 464563200\n",
      "[187000]: pnf_loss=0.1272 structure_loss=0.7306 structure_null=0.0201 structure_real=0.7105, steps/sec=4.06756\n",
      "0.8581572197043487\n",
      "epoch 154\n",
      "mem_used 464619520\n",
      "[188000]: pnf_loss=0.2201 structure_loss=0.6831 structure_null=0.0209 structure_real=0.6622, steps/sec=12.37940\n",
      "0.8684870991550508\n",
      "[189000]: pnf_loss=0.1457 structure_loss=0.5970 structure_null=0.0199 structure_real=0.5771, steps/sec=1.52020\n",
      "0.8553249552249909\n",
      "epoch 155\n",
      "mem_used 464515584\n",
      "[190000]: pnf_loss=0.1290 structure_loss=0.7425 structure_null=0.0198 structure_real=0.7227, steps/sec=1.76605\n",
      "0.8454739564416036\n",
      "eval Eval_Direc/gu_null_02D_04M_2024Y/02D_04M_2024Y_00h_05m_26s/step_190000\n",
      "[12]: pnf_loss=0.2745 structure_loss=0.8537 structure_null=0.0178 structure_real=0.8360\n",
      "eval_loss 1.6107745 12\n",
      "epoch 156\n",
      "mem_used 464857088\n",
      "[191000]: pnf_loss=0.1275 structure_loss=0.6705 structure_null=0.0162 structure_real=0.6543, steps/sec=2.48279\n",
      "0.8679665316288409\n",
      "epoch 157\n",
      "mem_used 466103808\n",
      "[192000]: pnf_loss=0.1408 structure_loss=0.6809 structure_null=0.0156 structure_real=0.6653, steps/sec=4.32547\n",
      "0.817716664907939\n",
      "epoch 158\n",
      "mem_used 464778752\n",
      "[193000]: pnf_loss=0.0804 structure_loss=0.6602 structure_null=0.0169 structure_real=0.6433, steps/sec=16.10165\n",
      "0.7743278691109191\n",
      "[194000]: pnf_loss=0.1378 structure_loss=0.7136 structure_null=0.0187 structure_real=0.6949, steps/sec=1.51403\n",
      "0.8285081014037132\n",
      "epoch 159\n",
      "mem_used 465312256\n",
      "[195000]: pnf_loss=0.1167 structure_loss=0.6306 structure_null=0.0182 structure_real=0.6124, steps/sec=1.81883\n",
      "0.8673459015866761\n",
      "epoch 160\n",
      "mem_used 465792000\n",
      "[196000]: pnf_loss=0.1091 structure_loss=0.5690 structure_null=0.0186 structure_real=0.5504, steps/sec=2.62457\n",
      "0.8205276960956639\n",
      "epoch 161\n",
      "mem_used 463808512\n",
      "[197000]: pnf_loss=0.0548 structure_loss=0.6436 structure_null=0.0153 structure_real=0.6283, steps/sec=4.70583\n",
      "0.8372728295001453\n",
      "epoch 162\n",
      "mem_used 464518144\n",
      "[198000]: pnf_loss=0.0982 structure_loss=0.7409 structure_null=0.0191 structure_real=0.7218, steps/sec=23.07001\n",
      "0.8038251779296182\n",
      "[199000]: pnf_loss=0.0679 structure_loss=0.6563 structure_null=0.0163 structure_real=0.6400, steps/sec=1.52234\n",
      "0.815210806787014\n",
      "epoch 163\n",
      "mem_used 462920192\n",
      "[200000]: pnf_loss=0.0720 structure_loss=0.6645 structure_null=0.0184 structure_real=0.6461, steps/sec=1.86685\n",
      "0.8310989705829597\n",
      "eval Eval_Direc/gu_null_02D_04M_2024Y/02D_04M_2024Y_00h_05m_26s/step_200000\n",
      "[12]: pnf_loss=0.0643 structure_loss=0.6750 structure_null=0.0187 structure_real=0.6563\n",
      "eval_loss 1.6900864 12\n",
      "epoch 164\n",
      "mem_used 463317504\n",
      "[201000]: pnf_loss=0.1676 structure_loss=0.6550 structure_null=0.0188 structure_real=0.6363, steps/sec=2.73936\n",
      "0.8183085212241048\n",
      "epoch 165\n",
      "mem_used 463619072\n",
      "[202000]: pnf_loss=0.1064 structure_loss=0.5918 structure_null=0.0212 structure_real=0.5706, steps/sec=5.16359\n",
      "0.7957202260777102\n",
      "epoch 166\n",
      "mem_used 464452608\n",
      "[203000]: pnf_loss=0.2091 structure_loss=0.6802 structure_null=0.0169 structure_real=0.6633, steps/sec=40.02227\n",
      "0.8185882364448748\n",
      "[204000]: pnf_loss=0.1493 structure_loss=0.6889 structure_null=0.0149 structure_real=0.6740, steps/sec=1.52145\n",
      "0.8563663786649705\n",
      "epoch 167\n",
      "mem_used 463539712\n",
      "[205000]: pnf_loss=0.1523 structure_loss=0.6409 structure_null=0.0183 structure_real=0.6226, steps/sec=1.95825\n",
      "0.8362492256524774\n",
      "epoch 168\n",
      "mem_used 464846336\n",
      "[206000]: pnf_loss=0.1022 structure_loss=0.6423 structure_null=0.0209 structure_real=0.6214, steps/sec=2.90307\n",
      "0.7865343862817488\n",
      "epoch 169\n",
      "mem_used 464018432\n",
      "[207000]: pnf_loss=0.2044 structure_loss=0.6363 structure_null=0.0168 structure_real=0.6195, steps/sec=5.71011\n",
      "0.8334475085976418\n",
      "epoch 170\n",
      "mem_used 464566784\n",
      "[208000]: pnf_loss=0.0998 structure_loss=0.5946 structure_null=0.0176 structure_real=0.5770, steps/sec=151.13372\n",
      "0.7222501575946808\n",
      "[209000]: pnf_loss=0.0790 structure_loss=0.5921 structure_null=0.0178 structure_real=0.5742, steps/sec=1.52250\n",
      "0.8262935616374015\n",
      "epoch 171\n",
      "mem_used 463729664\n",
      "[210000]: pnf_loss=0.1184 structure_loss=0.6726 structure_null=0.0169 structure_real=0.6557, steps/sec=2.01413\n",
      "0.8071256262531001\n",
      "eval Eval_Direc/gu_null_02D_04M_2024Y/02D_04M_2024Y_00h_05m_26s/step_210000\n",
      "[12]: pnf_loss=0.1339 structure_loss=0.5621 structure_null=0.0192 structure_real=0.5429\n",
      "eval_loss 1.3384476 12\n",
      "epoch 172\n",
      "mem_used 463600128\n",
      "[211000]: pnf_loss=0.1054 structure_loss=0.6615 structure_null=0.0192 structure_real=0.6423, steps/sec=3.04499\n",
      "0.822173148033119\n",
      "epoch 173\n",
      "mem_used 463588352\n",
      "[212000]: pnf_loss=0.0841 structure_loss=0.6099 structure_null=0.0194 structure_real=0.5906, steps/sec=6.38553\n",
      "0.8017502736346991\n",
      "[213000]: pnf_loss=0.1003 structure_loss=0.6532 structure_null=0.0147 structure_real=0.6384, steps/sec=1.51142\n",
      "0.8142780311107636\n",
      "epoch 174\n",
      "mem_used 465149952\n",
      "[214000]: pnf_loss=0.1186 structure_loss=0.6493 structure_null=0.0155 structure_real=0.6338, steps/sec=1.54355\n",
      "0.8230696198532392\n",
      "epoch 175\n",
      "mem_used 463790592\n",
      "[215000]: pnf_loss=0.0672 structure_loss=0.5882 structure_null=0.0159 structure_real=0.5723, steps/sec=2.10486\n",
      "0.8347503393271873\n",
      "epoch 176\n",
      "mem_used 464253952\n",
      "[216000]: pnf_loss=0.1139 structure_loss=0.7242 structure_null=0.0171 structure_real=0.7071, steps/sec=3.22862\n",
      "0.7965427325067357\n",
      "epoch 177\n",
      "mem_used 465261056\n",
      "[217000]: pnf_loss=0.1290 structure_loss=0.6041 structure_null=0.0166 structure_real=0.5876, steps/sec=7.17495\n",
      "0.8093543877533826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[218000]: pnf_loss=0.1366 structure_loss=0.5777 structure_null=0.0138 structure_real=0.5640, steps/sec=1.51383\n",
      "0.8092578684091568\n",
      "epoch 178\n",
      "mem_used 463614976\n",
      "[219000]: pnf_loss=0.1338 structure_loss=0.6328 structure_null=0.0162 structure_real=0.6166, steps/sec=1.53751\n",
      "0.8285984063923234\n",
      "epoch 179\n",
      "mem_used 464165376\n",
      "[220000]: pnf_loss=0.1568 structure_loss=0.5549 structure_null=0.0166 structure_real=0.5383, steps/sec=1.90808\n",
      "0.8333477709693581\n",
      "eval Eval_Direc/gu_null_02D_04M_2024Y/02D_04M_2024Y_00h_05m_26s/step_220000\n",
      "[12]: pnf_loss=0.0953 structure_loss=0.5964 structure_null=0.0184 structure_real=0.5780\n",
      "eval_loss 1.413291 12\n",
      "epoch 180\n",
      "mem_used 464682496\n",
      "[221000]: pnf_loss=0.0568 structure_loss=0.6520 structure_null=0.0157 structure_real=0.6363, steps/sec=0.47278\n",
      "0.8276724804531445\n",
      "epoch 181\n",
      "mem_used 463725568\n",
      "[222000]: pnf_loss=0.0565 structure_loss=0.6616 structure_null=0.0187 structure_real=0.6429, steps/sec=1.63972\n",
      "0.8349676917159492\n",
      "[223000]: pnf_loss=0.1424 structure_loss=1.0798 structure_null=0.0166 structure_real=1.0632, steps/sec=0.30296\n",
      "0.8070046724081039\n",
      "epoch 182\n",
      "mem_used 463566336\n",
      "[224000]: pnf_loss=0.0751 structure_loss=0.7005 structure_null=0.0146 structure_real=0.6859, steps/sec=0.32808\n",
      "0.8159535325757885\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 251\u001b[0m, in \u001b[0;36mExperiment.start_training\u001b[0;34m(self, return_logs)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m, epoch)\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmem_used\u001b[39m\u001b[38;5;124m'\u001b[39m,torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmemory_allocated(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m--> 251\u001b[0m epoch_log \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_logs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_logs\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_logs:\n\u001b[1;32m    258\u001b[0m     logs\u001b[38;5;241m.\u001b[39mappend(epoch_log)\n",
      "Cell \u001b[0;32mIn[13], line 277\u001b[0m, in \u001b[0;36mExperiment.train_epoch\u001b[0;34m(self, train_loader, valid_loader, epoch, return_logs)\u001b[0m\n\u001b[1;32m    273\u001b[0m losskeeper \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_feats \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m    275\u001b[0m     \n\u001b[1;32m    276\u001b[0m     \u001b[38;5;66;03m#train_feats = tree.map_structure(lambda x: x.to(device), train_feats)\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m     loss, aux_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_feats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m     losskeeper\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(loss))\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m aux_data\u001b[38;5;241m.\u001b[39mitems():\n",
      "Cell \u001b[0;32mIn[13], line 355\u001b[0m, in \u001b[0;36mExperiment.update_fn\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    353\u001b[0m loss, aux_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(data)\n\u001b[1;32m    354\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 355\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    356\u001b[0m loss_out \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m    357\u001b[0m \u001b[38;5;66;03m#del loss\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/se33/lib/python3.9/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/se33/lib/python3.9/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/miniconda3/envs/se33/lib/python3.9/site-packages/torch/optim/adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    130\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    133\u001b[0m         group,\n\u001b[1;32m    134\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    139\u001b[0m         state_steps)\n\u001b[0;32m--> 141\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniconda3/envs/se33/lib/python3.9/site-packages/torch/optim/adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 281\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/se33/lib/python3.9/site-packages/torch/optim/adam.py:489\u001b[0m, in \u001b[0;36m_multi_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    487\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_foreach_addcdiv_(params_, device_exp_avgs, denom)\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 489\u001b[0m     bias_correction1 \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m _get_value(step) \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m device_state_steps]\n\u001b[1;32m    490\u001b[0m     bias_correction2 \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m _get_value(step) \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m device_state_steps]\n\u001b[1;32m    492\u001b[0m     step_size \u001b[38;5;241m=\u001b[39m _stack_if_compiling([(lr \u001b[38;5;241m/\u001b[39m bc) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m bc \u001b[38;5;129;01min\u001b[39;00m bias_correction1])\n",
      "File \u001b[0;32m~/miniconda3/envs/se33/lib/python3.9/site-packages/torch/optim/adam.py:489\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    487\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_foreach_addcdiv_(params_, device_exp_avgs, denom)\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 489\u001b[0m     bias_correction1 \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m device_state_steps]\n\u001b[1;32m    490\u001b[0m     bias_correction2 \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m _get_value(step) \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m device_state_steps]\n\u001b[1;32m    492\u001b[0m     step_size \u001b[38;5;241m=\u001b[39m _stack_if_compiling([(lr \u001b[38;5;241m/\u001b[39m bc) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m bc \u001b[38;5;129;01min\u001b[39;00m bias_correction1])\n",
      "File \u001b[0;32m~/miniconda3/envs/se33/lib/python3.9/site-packages/torch/optim/optimizer.py:44\u001b[0m, in \u001b[0;36m_get_value\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "exp.start_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f599909a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a8e64ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = Experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ce2c47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "number of epochs 100\n",
      "epoch 0\n",
      "mem_used 44206080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nwoodall/miniconda3/envs/se33/lib/python3.9/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
      "/home/nwoodall/miniconda3/envs/se33/lib/python3.9/site-packages/dgl/backend/pytorch/tensor.py:449: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]: pnf_loss=2.8846 structure_loss=1.8591 structure_null=0.0316 structure_real=1.8275, steps/sec=142.13944\n",
      "4.74365758895874\n",
      "eval Eval_Direc/gu_null_01D_04M_2024Y/01D_04M_2024Y_00h_20m_09s/step_2\n",
      "[12]: pnf_loss=0.7785 structure_loss=1.2240 structure_null=0.0314 structure_real=1.1926\n",
      "eval_loss 4.446444 12\n",
      "[1000]: pnf_loss=2.8060 structure_loss=1.8154 structure_null=0.0316 structure_real=1.7837, steps/sec=2.15333\n",
      "1.6399432535767555\n",
      "epoch 1\n",
      "mem_used 196537856\n",
      "[2000]: pnf_loss=0.0629 structure_loss=1.1616 structure_null=0.0309 structure_real=1.1307, steps/sec=2.94169\n",
      "1.4086577666880786\n",
      "epoch 2\n",
      "mem_used 196421120\n",
      "[3000]: pnf_loss=0.0293 structure_loss=2.6763 structure_null=0.0252 structure_real=2.6511, steps/sec=4.57495\n",
      "1.4042774274516008\n",
      "epoch 3\n",
      "mem_used 196994560\n",
      "[4000]: pnf_loss=0.0047 structure_loss=1.7067 structure_null=0.0253 structure_real=1.6813, steps/sec=9.59559\n",
      "1.2987553669896188\n",
      "[5000]: pnf_loss=0.1094 structure_loss=1.1695 structure_null=0.0251 structure_real=1.1444, steps/sec=2.26084\n",
      "1.400515117108822\n",
      "epoch 4\n",
      "mem_used 197085184\n",
      "[6000]: pnf_loss=0.1011 structure_loss=1.2626 structure_null=0.0266 structure_real=1.2361, steps/sec=2.33408\n",
      "1.3363610553275411\n",
      "epoch 5\n",
      "mem_used 195948544\n",
      "[7000]: pnf_loss=0.1252 structure_loss=1.2613 structure_null=0.0246 structure_real=1.2367, steps/sec=3.17979\n",
      "1.2888272195429236\n",
      "epoch 6\n",
      "mem_used 196795392\n",
      "[8000]: pnf_loss=0.0305 structure_loss=1.0395 structure_null=0.0232 structure_real=1.0163, steps/sec=4.93187\n",
      "1.4735249670051591\n",
      "epoch 7\n",
      "mem_used 196991488\n",
      "[9000]: pnf_loss=0.1029 structure_loss=1.3839 structure_null=0.0221 structure_real=1.3617, steps/sec=11.37138\n",
      "1.3032759822423186\n",
      "[10000]: pnf_loss=0.0493 structure_loss=1.0640 structure_null=0.0221 structure_real=1.0419, steps/sec=2.25414\n",
      "1.3457162082791327\n",
      "eval Eval_Direc/gu_null_01D_04M_2024Y/01D_04M_2024Y_00h_20m_09s/step_10000\n",
      "[12]: pnf_loss=0.0280 structure_loss=1.1060 structure_null=0.0204 structure_real=1.0855\n",
      "eval_loss 2.2553747 12\n",
      "epoch 8\n",
      "mem_used 196092928\n",
      "[11000]: pnf_loss=0.0612 structure_loss=1.1743 structure_null=0.0212 structure_real=1.1531, steps/sec=2.40220\n",
      "1.3047160409643488\n",
      "epoch 9\n",
      "mem_used 197145600\n",
      "[12000]: pnf_loss=0.0458 structure_loss=1.1022 structure_null=0.0218 structure_real=1.0804, steps/sec=3.31150\n",
      "1.3097495987599246\n",
      "epoch 10\n",
      "mem_used 195415552\n",
      "[13000]: pnf_loss=0.0553 structure_loss=1.1878 structure_null=0.0221 structure_real=1.1657, steps/sec=5.34172\n",
      "1.2945949142755464\n",
      "epoch 11\n",
      "mem_used 196546048\n",
      "[14000]: pnf_loss=0.0469 structure_loss=0.8593 structure_null=0.0207 structure_real=0.8387, steps/sec=13.19844\n",
      "1.3488123930258558\n",
      "[15000]: pnf_loss=0.0744 structure_loss=1.0564 structure_null=0.0218 structure_real=1.0345, steps/sec=2.27616\n",
      "1.2885296308994294\n",
      "epoch 12\n",
      "mem_used 196451328\n",
      "[16000]: pnf_loss=0.0200 structure_loss=1.2201 structure_null=0.0175 structure_real=1.2026, steps/sec=2.50224\n",
      "1.2794683761732026\n",
      "epoch 13\n",
      "mem_used 195422208\n",
      "[17000]: pnf_loss=0.0189 structure_loss=1.2334 structure_null=0.0186 structure_real=1.2148, steps/sec=3.44386\n",
      "1.288150546040629\n",
      "epoch 14\n",
      "mem_used 197566464\n",
      "[18000]: pnf_loss=0.0777 structure_loss=0.9929 structure_null=0.0191 structure_real=0.9739, steps/sec=5.67492\n",
      "1.254150642091362\n",
      "epoch 15\n",
      "mem_used 197412864\n",
      "[19000]: pnf_loss=0.0413 structure_loss=1.0357 structure_null=0.0143 structure_real=1.0214, steps/sec=15.69827\n",
      "1.2285896938422631\n",
      "[20000]: pnf_loss=0.3984 structure_loss=2.1973 structure_null=0.0207 structure_real=2.1765, steps/sec=2.27567\n",
      "1.2635746412277222\n",
      "eval Eval_Direc/gu_null_01D_04M_2024Y/01D_04M_2024Y_00h_20m_09s/step_20000\n",
      "[12]: pnf_loss=0.0359 structure_loss=0.9654 structure_null=0.0196 structure_real=0.9458\n",
      "eval_loss 2.1004534 12\n",
      "epoch 16\n",
      "mem_used 197069824\n",
      "[21000]: pnf_loss=0.0436 structure_loss=0.8450 structure_null=0.0197 structure_real=0.8253, steps/sec=2.55805\n",
      "1.2082669059032793\n",
      "epoch 17\n",
      "mem_used 197408256\n",
      "[22000]: pnf_loss=0.0701 structure_loss=1.2225 structure_null=0.0182 structure_real=1.2043, steps/sec=3.60559\n",
      "1.207883100691008\n",
      "epoch 18\n",
      "mem_used 197753856\n",
      "[23000]: pnf_loss=0.0005 structure_loss=1.3749 structure_null=0.0180 structure_real=1.3570, steps/sec=5.72473\n",
      "1.1652391796762294\n",
      "epoch 19\n",
      "mem_used 196413952\n",
      "[24000]: pnf_loss=0.0403 structure_loss=0.8647 structure_null=0.0158 structure_real=0.8488, steps/sec=19.52414\n",
      "1.1746276943092673\n",
      "[25000]: pnf_loss=0.0406 structure_loss=0.9115 structure_null=0.0209 structure_real=0.8907, steps/sec=2.27857\n",
      "1.188285204052925\n",
      "epoch 20\n",
      "mem_used 196105728\n",
      "[26000]: pnf_loss=0.0242 structure_loss=1.4147 structure_null=0.0190 structure_real=1.3957, steps/sec=2.64835\n",
      "1.2220646045928778\n",
      "epoch 21\n",
      "mem_used 197413376\n",
      "[27000]: pnf_loss=0.0799 structure_loss=1.0703 structure_null=0.0180 structure_real=1.0523, steps/sec=3.76810\n",
      "1.3223466602328602\n",
      "epoch 22\n",
      "mem_used 195393024\n",
      "[28000]: pnf_loss=0.0899 structure_loss=1.1074 structure_null=0.0214 structure_real=1.0860, steps/sec=6.58135\n",
      "1.1720262517818827\n",
      "epoch 23\n",
      "mem_used 195579392\n",
      "[29000]: pnf_loss=0.0382 structure_loss=1.1629 structure_null=0.0200 structure_real=1.1429, steps/sec=25.38116\n",
      "1.088647461339329\n",
      "[30000]: pnf_loss=0.0381 structure_loss=1.0369 structure_null=0.0173 structure_real=1.0195, steps/sec=2.27509\n",
      "1.1985441460609436\n",
      "eval Eval_Direc/gu_null_01D_04M_2024Y/01D_04M_2024Y_00h_20m_09s/step_30000\n",
      "[12]: pnf_loss=0.1444 structure_loss=1.2130 structure_null=0.0175 structure_real=1.1954\n",
      "eval_loss 2.1023357 12\n",
      "epoch 24\n",
      "mem_used 197424128\n",
      "[31000]: pnf_loss=0.0689 structure_loss=0.9273 structure_null=0.0183 structure_real=0.9089, steps/sec=2.74166\n",
      "1.179455725977627\n",
      "epoch 25\n",
      "mem_used 195600384\n",
      "[32000]: pnf_loss=0.0321 structure_loss=0.9349 structure_null=0.0162 structure_real=0.9186, steps/sec=3.94262\n",
      "1.422202869705532\n",
      "epoch 26\n",
      "mem_used 196409344\n",
      "[33000]: pnf_loss=0.0314 structure_loss=0.9847 structure_null=0.0176 structure_real=0.9671, steps/sec=7.17119\n",
      "1.1801798538591877\n",
      "epoch 27\n",
      "mem_used 197649408\n",
      "[34000]: pnf_loss=0.0309 structure_loss=0.9817 structure_null=0.0173 structure_real=0.9644, steps/sec=37.54654\n",
      "1.2205915998239987\n",
      "[35000]: pnf_loss=0.0808 structure_loss=1.4810 structure_null=0.0193 structure_real=1.4616, steps/sec=2.28278\n",
      "1.181704049706459\n",
      "epoch 28\n",
      "mem_used 196203520\n",
      "[36000]: pnf_loss=0.0618 structure_loss=0.9198 structure_null=0.0192 structure_real=0.9006, steps/sec=2.84159\n",
      "1.1601125459321102\n",
      "epoch 29\n",
      "mem_used 196657152\n",
      "[37000]: pnf_loss=0.0443 structure_loss=0.9206 structure_null=0.0170 structure_real=0.9036, steps/sec=4.13250\n",
      "1.2403950696671466\n",
      "epoch 30\n",
      "mem_used 195990016\n",
      "[38000]: pnf_loss=0.0199 structure_loss=0.9611 structure_null=0.0173 structure_real=0.9439, steps/sec=7.82128\n",
      "1.2368809210843053\n",
      "epoch 31\n",
      "mem_used 198001152\n",
      "[39000]: pnf_loss=0.0415 structure_loss=0.9007 structure_null=0.0148 structure_real=0.8859, steps/sec=69.11113\n",
      "1.1121240077596721\n",
      "[40000]: pnf_loss=0.0449 structure_loss=0.8844 structure_null=0.0149 structure_real=0.8694, steps/sec=2.27242\n",
      "1.180408309519291\n",
      "eval Eval_Direc/gu_null_01D_04M_2024Y/01D_04M_2024Y_00h_20m_09s/step_40000\n",
      "[12]: pnf_loss=0.0386 structure_loss=0.9961 structure_null=0.0188 structure_real=0.9773\n",
      "eval_loss 2.3787205 12\n",
      "epoch 32\n",
      "mem_used 196363264\n",
      "[41000]: pnf_loss=0.0518 structure_loss=0.9216 structure_null=0.0158 structure_real=0.9059, steps/sec=2.93016\n",
      "1.1809872166090405\n",
      "epoch 33\n",
      "mem_used 195609088\n",
      "[42000]: pnf_loss=0.0284 structure_loss=1.0341 structure_null=0.0150 structure_real=1.0191, steps/sec=4.37495\n",
      "1.1510393353096553\n",
      "epoch 34\n",
      "mem_used 196516864\n",
      "[43000]: pnf_loss=0.0410 structure_loss=1.1812 structure_null=0.0165 structure_real=1.1646, steps/sec=8.70419\n",
      "1.1163665190452838\n",
      "epoch 35\n",
      "mem_used 196083200\n",
      "[44000]: pnf_loss=0.0339 structure_loss=1.0086 structure_null=0.0157 structure_real=0.9929, steps/sec=445.92489\n",
      "0.9646596670150757\n",
      "[45000]: pnf_loss=0.0214 structure_loss=1.4359 structure_null=0.0176 structure_real=1.4184, steps/sec=2.26770\n",
      "1.129015704870224\n",
      "epoch 36\n",
      "mem_used 197114880\n",
      "[46000]: pnf_loss=0.0137 structure_loss=1.0186 structure_null=0.0161 structure_real=1.0025, steps/sec=3.04470\n",
      "1.1137830864458798\n",
      "epoch 37\n",
      "mem_used 196418048\n",
      "[47000]: pnf_loss=0.0849 structure_loss=1.5000 structure_null=0.0176 structure_real=1.4824, steps/sec=4.64329\n",
      "1.1279716000051945\n",
      "epoch 38\n",
      "mem_used 197701632\n",
      "[48000]: pnf_loss=0.0283 structure_loss=0.9108 structure_null=0.0158 structure_real=0.8950, steps/sec=9.72475\n",
      "1.0805276082112238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[49000]: pnf_loss=0.0003 structure_loss=3.2116 structure_null=0.0191 structure_real=3.1925, steps/sec=2.27050\n",
      "1.1563784728050233\n",
      "epoch 39\n",
      "mem_used 196229120\n",
      "[50000]: pnf_loss=0.0196 structure_loss=0.8673 structure_null=0.0151 structure_real=0.8522, steps/sec=2.32234\n",
      "1.1142327558542835\n",
      "eval Eval_Direc/gu_null_01D_04M_2024Y/01D_04M_2024Y_00h_20m_09s/step_50000\n",
      "[12]: pnf_loss=0.0979 structure_loss=1.0699 structure_null=0.0177 structure_real=1.0522\n",
      "eval_loss 2.1719978 12\n",
      "epoch 40\n",
      "mem_used 196002816\n",
      "[51000]: pnf_loss=0.0988 structure_loss=2.3887 structure_null=0.0194 structure_real=2.3694, steps/sec=3.16707\n",
      "1.1755057214034927\n",
      "epoch 41\n",
      "mem_used 196826112\n",
      "[52000]: pnf_loss=0.0554 structure_loss=0.9170 structure_null=0.0144 structure_real=0.9026, steps/sec=4.91259\n",
      "1.1106835808403805\n",
      "epoch 42\n",
      "mem_used 197424128\n",
      "[53000]: pnf_loss=0.0132 structure_loss=0.9085 structure_null=0.0138 structure_real=0.8947, steps/sec=11.04416\n",
      "1.1679852165064766\n",
      "[54000]: pnf_loss=0.0646 structure_loss=1.2299 structure_null=0.0157 structure_real=1.2142, steps/sec=2.27226\n",
      "1.1101143036484717\n",
      "epoch 43\n",
      "mem_used 196847104\n",
      "[55000]: pnf_loss=0.0457 structure_loss=0.9373 structure_null=0.0162 structure_real=0.9211, steps/sec=2.39952\n",
      "1.1367774526613152\n",
      "epoch 44\n",
      "mem_used 195593216\n",
      "[56000]: pnf_loss=0.0193 structure_loss=0.9125 structure_null=0.0151 structure_real=0.8974, steps/sec=3.30398\n",
      "1.1405613381910875\n",
      "epoch 45\n",
      "mem_used 195588608\n",
      "[57000]: pnf_loss=0.0538 structure_loss=0.9186 structure_null=0.0164 structure_real=0.9021, steps/sec=5.21976\n",
      "1.111681913644418\n",
      "epoch 46\n",
      "mem_used 196080128\n",
      "[58000]: pnf_loss=0.2060 structure_loss=1.3727 structure_null=0.0165 structure_real=1.3562, steps/sec=12.69872\n",
      "1.110244729545679\n",
      "[59000]: pnf_loss=0.0112 structure_loss=0.9656 structure_null=0.0170 structure_real=0.9486, steps/sec=2.28983\n",
      "1.2148071895241737\n",
      "epoch 47\n",
      "mem_used 197352448\n",
      "[60000]: pnf_loss=0.0000 structure_loss=1.2272 structure_null=0.0184 structure_real=1.2088, steps/sec=2.46067\n",
      "1.1040367475679462\n",
      "eval Eval_Direc/gu_null_01D_04M_2024Y/01D_04M_2024Y_00h_20m_09s/step_60000\n",
      "[12]: pnf_loss=0.0863 structure_loss=1.5474 structure_null=0.0179 structure_real=1.5294\n",
      "eval_loss 2.407183 12\n",
      "epoch 48\n",
      "mem_used 196335104\n",
      "[61000]: pnf_loss=0.0495 structure_loss=0.8470 structure_null=0.0175 structure_real=0.8296, steps/sec=3.43398\n",
      "1.109151857714337\n",
      "epoch 49\n",
      "mem_used 196193280\n",
      "[62000]: pnf_loss=0.0463 structure_loss=0.9191 structure_null=0.0173 structure_real=0.9018, steps/sec=5.63273\n",
      "1.1518280885436318\n",
      "epoch 50\n",
      "mem_used 196652032\n",
      "[63000]: pnf_loss=0.0008 structure_loss=1.1475 structure_null=0.0184 structure_real=1.1291, steps/sec=15.20528\n",
      "1.0870420396327973\n",
      "[64000]: pnf_loss=0.0030 structure_loss=1.1060 structure_null=0.0154 structure_real=1.0906, steps/sec=2.28372\n",
      "1.0896770619750022\n",
      "epoch 51\n",
      "mem_used 196373504\n",
      "[65000]: pnf_loss=0.0412 structure_loss=0.8047 structure_null=0.0138 structure_real=0.7909, steps/sec=2.54750\n",
      "1.102661209837987\n",
      "epoch 52\n",
      "mem_used 197193728\n",
      "[66000]: pnf_loss=0.0537 structure_loss=0.8909 structure_null=0.0164 structure_real=0.8745, steps/sec=3.57982\n",
      "1.155308594025156\n",
      "epoch 53\n",
      "mem_used 196796416\n",
      "[67000]: pnf_loss=0.0226 structure_loss=0.9772 structure_null=0.0143 structure_real=0.9629, steps/sec=6.01779\n",
      "1.1164923893744838\n",
      "epoch 54\n",
      "mem_used 195937280\n",
      "[68000]: pnf_loss=0.0604 structure_loss=1.5290 structure_null=0.0172 structure_real=1.5118, steps/sec=18.62613\n",
      "1.1276044088308927\n",
      "[69000]: pnf_loss=0.0152 structure_loss=0.9545 structure_null=0.0148 structure_real=0.9397, steps/sec=2.27215\n",
      "1.0974933931827544\n",
      "epoch 55\n",
      "mem_used 197254144\n",
      "[70000]: pnf_loss=0.0177 structure_loss=1.0001 structure_null=0.0164 structure_real=0.9837, steps/sec=2.63053\n",
      "1.1626544487958699\n",
      "eval Eval_Direc/gu_null_01D_04M_2024Y/01D_04M_2024Y_00h_20m_09s/step_70000\n",
      "[12]: pnf_loss=0.0644 structure_loss=0.9492 structure_null=0.0159 structure_real=0.9333\n",
      "eval_loss 2.200461 12\n",
      "epoch 56\n",
      "mem_used 196402688\n",
      "[71000]: pnf_loss=0.0049 structure_loss=1.1783 structure_null=0.0180 structure_real=1.1603, steps/sec=3.72597\n",
      "1.107975263834784\n",
      "epoch 57\n",
      "mem_used 196723200\n",
      "[72000]: pnf_loss=0.0693 structure_loss=0.9208 structure_null=0.0162 structure_real=0.9046, steps/sec=6.49716\n",
      "1.0932710544336215\n",
      "epoch 58\n",
      "mem_used 196111872\n",
      "[73000]: pnf_loss=0.0098 structure_loss=0.9402 structure_null=0.0122 structure_real=0.9280, steps/sec=24.00337\n",
      "1.119505007850363\n",
      "[74000]: pnf_loss=0.0040 structure_loss=0.9653 structure_null=0.0153 structure_real=0.9500, steps/sec=2.28061\n",
      "1.1110178750753403\n",
      "epoch 59\n",
      "mem_used 196229120\n",
      "[75000]: pnf_loss=0.0209 structure_loss=0.8461 structure_null=0.0152 structure_real=0.8309, steps/sec=2.71925\n",
      "1.1262738528787164\n",
      "epoch 60\n",
      "mem_used 198227456\n",
      "[76000]: pnf_loss=0.0343 structure_loss=0.8646 structure_null=0.0152 structure_real=0.8495, steps/sec=3.93048\n",
      "1.097976532681235\n",
      "epoch 61\n",
      "mem_used 198216192\n",
      "[77000]: pnf_loss=0.0947 structure_loss=1.0206 structure_null=0.0176 structure_real=1.0030, steps/sec=7.07147\n",
      "1.0600356199423964\n",
      "epoch 62\n",
      "mem_used 196172800\n",
      "[78000]: pnf_loss=0.0061 structure_loss=1.0966 structure_null=0.0155 structure_real=1.0811, steps/sec=34.56646\n",
      "1.0686228175957997\n",
      "[79000]: pnf_loss=0.0050 structure_loss=0.9767 structure_null=0.0148 structure_real=0.9619, steps/sec=2.28091\n",
      "1.1074479328989983\n",
      "epoch 63\n",
      "mem_used 196955136\n",
      "[80000]: pnf_loss=0.0373 structure_loss=0.8894 structure_null=0.0161 structure_real=0.8733, steps/sec=2.80773\n",
      "1.0851945159461795\n",
      "eval Eval_Direc/gu_null_01D_04M_2024Y/01D_04M_2024Y_00h_20m_09s/step_80000\n",
      "[12]: pnf_loss=0.1082 structure_loss=1.0734 structure_null=0.0168 structure_real=1.0566\n",
      "eval_loss 1.9992489 12\n",
      "epoch 64\n",
      "mem_used 196838912\n",
      "[81000]: pnf_loss=0.0500 structure_loss=0.9315 structure_null=0.0139 structure_real=0.9176, steps/sec=4.14368\n",
      "1.1440736075890237\n",
      "epoch 65\n",
      "mem_used 198277120\n",
      "[82000]: pnf_loss=0.0131 structure_loss=0.9909 structure_null=0.0142 structure_real=0.9767, steps/sec=7.74418\n",
      "1.0585208571563334\n",
      "epoch 66\n",
      "mem_used 196080128\n",
      "[83000]: pnf_loss=0.0071 structure_loss=1.5888 structure_null=0.0167 structure_real=1.5721, steps/sec=60.20155\n",
      "1.1466400419410907\n",
      "[84000]: pnf_loss=0.1622 structure_loss=1.3798 structure_null=0.0151 structure_real=1.3646, steps/sec=2.28425\n",
      "1.1154895223379135\n",
      "epoch 67\n",
      "mem_used 196417024\n",
      "[85000]: pnf_loss=0.0330 structure_loss=0.8680 structure_null=0.0160 structure_real=0.8519, steps/sec=2.92150\n",
      "1.06564020767102\n",
      "epoch 68\n",
      "mem_used 198434304\n",
      "[86000]: pnf_loss=0.0993 structure_loss=1.4903 structure_null=0.0159 structure_real=1.4744, steps/sec=4.34673\n",
      "1.1083814715611116\n",
      "epoch 69\n",
      "mem_used 196096000\n",
      "[87000]: pnf_loss=0.0111 structure_loss=0.9864 structure_null=0.0137 structure_real=0.9728, steps/sec=8.49670\n",
      "1.08522753679797\n",
      "epoch 70\n",
      "mem_used 196360704\n",
      "[88000]: pnf_loss=0.0547 structure_loss=0.9530 structure_null=0.0157 structure_real=0.9373, steps/sec=226.23770\n",
      "1.0181895434856414\n",
      "[89000]: pnf_loss=0.0163 structure_loss=0.9075 structure_null=0.0158 structure_real=0.8917, steps/sec=2.27504\n",
      "1.098803404390812\n",
      "epoch 71\n",
      "mem_used 197319168\n",
      "[90000]: pnf_loss=0.0315 structure_loss=1.7692 structure_null=0.0166 structure_real=1.7526, steps/sec=3.03167\n",
      "1.1029758406666963\n",
      "eval Eval_Direc/gu_null_01D_04M_2024Y/01D_04M_2024Y_00h_20m_09s/step_90000\n",
      "[12]: pnf_loss=0.0246 structure_loss=0.9948 structure_null=0.0158 structure_real=0.9789\n",
      "eval_loss 2.1689208 12\n",
      "epoch 72\n",
      "mem_used 197096960\n",
      "[91000]: pnf_loss=0.0485 structure_loss=0.9732 structure_null=0.0151 structure_real=0.9581, steps/sec=4.58763\n",
      "1.1545571711274885\n",
      "epoch 73\n",
      "mem_used 196373504\n",
      "[92000]: pnf_loss=0.0271 structure_loss=1.1277 structure_null=0.0156 structure_real=1.1120, steps/sec=9.47239\n",
      "1.0478955517254116\n",
      "[93000]: pnf_loss=0.0565 structure_loss=1.0338 structure_null=0.0153 structure_real=1.0185, steps/sec=2.27460\n",
      "1.0811973004341124\n",
      "epoch 74\n",
      "mem_used 196132352\n",
      "[94000]: pnf_loss=0.0425 structure_loss=0.8738 structure_null=0.0172 structure_real=0.8566, steps/sec=2.32650\n",
      "1.0647664663747956\n",
      "epoch 75\n",
      "mem_used 196374016\n",
      "[95000]: pnf_loss=0.0303 structure_loss=0.9726 structure_null=0.0145 structure_real=0.9581, steps/sec=3.14809\n",
      "1.0765283273006307\n",
      "epoch 76\n",
      "mem_used 196406272\n",
      "[96000]: pnf_loss=0.0181 structure_loss=0.9582 structure_null=0.0162 structure_real=0.9420, steps/sec=4.84168\n",
      "1.0811389237642288\n",
      "epoch 77\n",
      "mem_used 197090816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[97000]: pnf_loss=0.1815 structure_loss=1.4976 structure_null=0.0171 structure_real=1.4805, steps/sec=10.73258\n",
      "1.062409329470865\n",
      "[98000]: pnf_loss=0.0159 structure_loss=0.9219 structure_null=0.0188 structure_real=0.9030, steps/sec=2.27601\n",
      "1.0728412869572639\n",
      "epoch 78\n",
      "mem_used 196739072\n",
      "[99000]: pnf_loss=0.0418 structure_loss=0.9321 structure_null=0.0173 structure_real=0.9149, steps/sec=2.39268\n",
      "1.0509303537054882\n",
      "epoch 79\n",
      "mem_used 197154304\n",
      "[100000]: pnf_loss=0.0275 structure_loss=0.8685 structure_null=0.0136 structure_real=0.8549, steps/sec=3.26025\n",
      "1.0556270132961028\n",
      "eval Eval_Direc/gu_null_01D_04M_2024Y/01D_04M_2024Y_00h_20m_09s/step_100000\n",
      "[12]: pnf_loss=0.0516 structure_loss=0.9482 structure_null=0.0152 structure_real=0.9330\n",
      "eval_loss 2.1908886 12\n",
      "epoch 80\n",
      "mem_used 195325440\n",
      "[101000]: pnf_loss=0.0072 structure_loss=0.8929 structure_null=0.0147 structure_real=0.8782, steps/sec=5.18464\n",
      "1.0273501550609416\n",
      "epoch 81\n",
      "mem_used 197421056\n",
      "[102000]: pnf_loss=0.0561 structure_loss=0.8540 structure_null=0.0176 structure_real=0.8364, steps/sec=12.42966\n",
      "0.999033210381784\n",
      "[103000]: pnf_loss=0.0026 structure_loss=0.9580 structure_null=0.0145 structure_real=0.9435, steps/sec=2.27971\n",
      "1.0412738659381866\n",
      "epoch 82\n",
      "mem_used 196399616\n",
      "[104000]: pnf_loss=0.0138 structure_loss=0.9265 structure_null=0.0138 structure_real=0.9127, steps/sec=2.46420\n",
      "1.0371985439347913\n",
      "epoch 83\n",
      "mem_used 196225024\n",
      "[105000]: pnf_loss=0.0254 structure_loss=1.0325 structure_null=0.0162 structure_real=1.0163, steps/sec=3.42099\n",
      "1.0429713643185226\n",
      "epoch 84\n",
      "mem_used 195685376\n",
      "[106000]: pnf_loss=0.0840 structure_loss=2.3886 structure_null=0.0176 structure_real=2.3710, steps/sec=5.55524\n",
      "1.0433645542096166\n",
      "epoch 85\n",
      "mem_used 195900416\n",
      "[107000]: pnf_loss=0.0320 structure_loss=1.0646 structure_null=0.0172 structure_real=1.0474, steps/sec=14.80416\n",
      "1.0344849574950434\n",
      "[108000]: pnf_loss=0.0387 structure_loss=0.8555 structure_null=0.0149 structure_real=0.8406, steps/sec=2.27864\n",
      "1.0275144590735434\n",
      "epoch 86\n",
      "mem_used 195593216\n",
      "[109000]: pnf_loss=0.0467 structure_loss=0.8086 structure_null=0.0145 structure_real=0.7942, steps/sec=2.53777\n",
      "1.0493575383002616\n",
      "epoch 87\n",
      "mem_used 196417536\n",
      "[110000]: pnf_loss=0.0193 structure_loss=0.8329 structure_null=0.0151 structure_real=0.8177, steps/sec=3.56463\n",
      "1.0896564113926404\n",
      "eval Eval_Direc/gu_null_01D_04M_2024Y/01D_04M_2024Y_00h_20m_09s/step_110000\n",
      "[12]: pnf_loss=0.0601 structure_loss=0.9125 structure_null=0.0145 structure_real=0.8980\n",
      "eval_loss 1.8810018 12\n",
      "epoch 88\n",
      "mem_used 196839936\n",
      "[111000]: pnf_loss=0.0417 structure_loss=0.8674 structure_null=0.0140 structure_real=0.8535, steps/sec=5.92162\n",
      "1.0154179205807548\n",
      "epoch 89\n",
      "mem_used 195908096\n",
      "[112000]: pnf_loss=0.0700 structure_loss=0.9175 structure_null=0.0145 structure_real=0.9030, steps/sec=17.90729\n",
      "0.9995543121352909\n",
      "[113000]: pnf_loss=0.0033 structure_loss=1.2033 structure_null=0.0178 structure_real=1.1854, steps/sec=2.28234\n",
      "1.03431552618742\n",
      "epoch 90\n",
      "mem_used 196102144\n",
      "[114000]: pnf_loss=0.0495 structure_loss=0.8626 structure_null=0.0138 structure_real=0.8488, steps/sec=2.61937\n",
      "0.9927676118653396\n",
      "epoch 91\n",
      "mem_used 197423616\n",
      "[115000]: pnf_loss=0.0124 structure_loss=0.8286 structure_null=0.0144 structure_real=0.8142, steps/sec=3.72735\n",
      "1.0277807115536142\n",
      "epoch 92\n",
      "mem_used 196840960\n",
      "[116000]: pnf_loss=0.0012 structure_loss=0.9379 structure_null=0.0135 structure_real=0.9243, steps/sec=6.42039\n",
      "1.0802857132440202\n",
      "epoch 93\n",
      "mem_used 196222464\n",
      "[117000]: pnf_loss=0.0149 structure_loss=0.8512 structure_null=0.0121 structure_real=0.8391, steps/sec=22.54763\n",
      "1.046389715840118\n",
      "[118000]: pnf_loss=0.0144 structure_loss=0.9434 structure_null=0.0115 structure_real=0.9318, steps/sec=2.28156\n",
      "0.9822931187748909\n",
      "epoch 94\n",
      "mem_used 196303872\n",
      "[119000]: pnf_loss=0.0140 structure_loss=0.8729 structure_null=0.0100 structure_real=0.8629, steps/sec=2.70350\n",
      "0.9901081788709781\n",
      "epoch 95\n",
      "mem_used 196365312\n",
      "[120000]: pnf_loss=0.0059 structure_loss=0.8309 structure_null=0.0141 structure_real=0.8167, steps/sec=3.89926\n",
      "0.9602810209632939\n",
      "eval Eval_Direc/gu_null_01D_04M_2024Y/01D_04M_2024Y_00h_20m_09s/step_120000\n",
      "[12]: pnf_loss=0.0031 structure_loss=0.8862 structure_null=0.0158 structure_real=0.8704\n",
      "eval_loss 1.7391372 12\n",
      "epoch 96\n",
      "mem_used 195385856\n",
      "[121000]: pnf_loss=0.0092 structure_loss=0.9209 structure_null=0.0160 structure_real=0.9049, steps/sec=6.93306\n",
      "0.9701720238095377\n",
      "epoch 97\n",
      "mem_used 197438464\n",
      "[122000]: pnf_loss=0.0281 structure_loss=0.8376 structure_null=0.0147 structure_real=0.8229, steps/sec=32.04199\n",
      "0.9204977247076975\n",
      "[123000]: pnf_loss=0.0284 structure_loss=0.8223 structure_null=0.0174 structure_real=0.8049, steps/sec=2.27403\n",
      "0.9596472699642181\n",
      "epoch 98\n",
      "mem_used 195896832\n",
      "[124000]: pnf_loss=0.0508 structure_loss=0.8253 structure_null=0.0187 structure_real=0.8065, steps/sec=2.79783\n",
      "0.9392562174679899\n",
      "epoch 99\n",
      "mem_used 195831808\n",
      "[125000]: pnf_loss=0.0134 structure_loss=0.8781 structure_null=0.0156 structure_real=0.8625, steps/sec=4.09870\n",
      "0.9539122423106935\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp.start_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6518214f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f538488",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(self, train_loader, valid_loader, device, return_logs=False):\n",
    "        log_lossses = defaultdict(list)\n",
    "        global_logs = []\n",
    "        log_time = time.time()\n",
    "        step_time = time.time()\n",
    "        losskeeper = []\n",
    "        for train_feats in train_loader:\n",
    "            train_feats = tree.map_structure(lambda x: x.to(device), train_feats)\n",
    "            loss, aux_data = self.update_fn(train_feats)\n",
    "            losskeeper.append(float(loss.detach().cpu()))\n",
    "            if return_logs:\n",
    "                global_logs.append(loss)\n",
    "            for k,v in aux_data.items():\n",
    "                log_lossses[k].append(du.move_to_np(v))\n",
    "            self.trained_steps += 1\n",
    "\n",
    "            # Logging to terminal\n",
    "            if self.trained_steps == 1 or self.trained_steps % self._exp_conf.log_freq == 0:\n",
    "                elapsed_time = time.time() - log_time\n",
    "                log_time = time.time()\n",
    "                step_per_sec = self._exp_conf.log_freq / elapsed_time\n",
    "                rolling_losses = tree.map_structure(np.mean, log_lossses)\n",
    "                loss_log = ' '.join([\n",
    "                    f'{k}={v[0]:.4f}'\n",
    "                    for k,v in rolling_losses.items() if 'batch' not in k\n",
    "                ])\n",
    "                self._log.info(\n",
    "                    f'[{self.trained_steps}]: {loss_log}, steps/sec={step_per_sec:.5f}')\n",
    "                log_lossses = defaultdict(list)\n",
    "                \n",
    "                print(f'[{self.trained_steps}]: {loss_log}, steps/sec={step_per_sec:.5f}')\n",
    "                print(np.mean(losskeeper[-1000:]))\n",
    "\n",
    "            # Take checkpoint\n",
    "            if self._exp_conf.ckpt_dir is not None and (\n",
    "                    (self.trained_steps % self._exp_conf.ckpt_freq) == 0\n",
    "                    or (self._exp_conf.early_ckpt and self.trained_steps == 2)\n",
    "                ):\n",
    "                ckpt_path = os.path.join(\n",
    "                    self._exp_conf.ckpt_dir, f'step_{self.trained_steps}.pth')\n",
    "                du.write_checkpoint(\n",
    "                    ckpt_path,\n",
    "                    copy.deepcopy(self.model.state_dict()),\n",
    "                    self._conf,\n",
    "                    copy.deepcopy(self._optimizer.state_dict()),\n",
    "                    self.trained_epochs,\n",
    "                    self.trained_steps,\n",
    "                    logger=self._log,\n",
    "                    use_torch=True\n",
    "                )\n",
    "                \n",
    "\n",
    "                # Run evaluation\n",
    "                self._log.info(f'Running evaluation of {ckpt_path}')\n",
    "                start_time = time.time()\n",
    "                self._exp_conf.eval_dir = 'output/'\n",
    "                eval_dir = os.path.join(\n",
    "                    self._exp_conf.eval_dir, f'step_{self.trained_steps}')\n",
    "                print(eval_dir)\n",
    "                os.makedirs(eval_dir, exist_ok=True)\n",
    "                ckpt_metrics = self.eval_fn(\n",
    "                    eval_dir, valid_loader, device,\n",
    "                    noise_scale=self._exp_conf.noise_scale\n",
    "                )\n",
    "                eval_time = time.time() - start_time\n",
    "                self._log.info(f'Finished evaluation in {eval_time:.2f}s')\n",
    "            else:\n",
    "                ckpt_metrics = None\n",
    "                eval_time = None\n",
    "\n",
    "\n",
    "            if torch.isnan(loss):\n",
    "                if self._use_wandb:\n",
    "                    wandb.alert(\n",
    "                        title=\"Encountered NaN loss\",\n",
    "                        text=f\"Loss NaN after {self.trained_epochs} epochs, {self.trained_steps} steps\"\n",
    "                    )\n",
    "                raise Exception(f'NaN encountered')\n",
    "\n",
    "        if return_logs:\n",
    "            return global_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79ced54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b717659a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48458147",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def model_step_null(noised_dict, batched_t, graph_maker, graph_unet, train=True):\n",
    "    #prep coordinates for output display from and comparison via FAPE\n",
    "    \n",
    "    CA_t  = noised_dict['bb_shifted']['CA'].reshape(B, L, 3).to(device)\n",
    "    NC_t = CA_t + noised_dict['bb_shifted']['N_CA'].reshape(B, L, 3).to(device)#not mult by bond distance, seems to help?\n",
    "    CC_t = CA_t + noised_dict['bb_shifted']['C_CA'].reshape(B, L, 3).to(device)#not mult \n",
    "    true =  torch.cat((NC_t,CA_t,CC_t),dim=2).reshape(B,L,3,3)\n",
    "\n",
    "    CA_n  = noised_dict['bb_noised']['CA'].reshape(B, L, 3).to(device)\n",
    "    NC_n = CA_n + noised_dict['bb_noised']['N_CA'].reshape(B, L, 3).to(device)\n",
    "    CC_n = CA_n + noised_dict['bb_noised']['C_CA'].reshape(B, L, 3).to(device)\n",
    "    noise_xyz =  torch.cat((NC_n,CA_n,CC_n),dim=2).reshape(B,L,3,3)\n",
    "\n",
    "    #prepare graphs\n",
    "    feat_dict = graph_maker.prep_for_network(noised_dict, cuda=True)\n",
    "    out =graph_unet(feat_dict,batched_t)\n",
    "    \n",
    "    \n",
    "    #FAPE Loss for the prediction\n",
    "    CA_p = out['1'][:,0,:].reshape(B, L, 3)+CA_n #translation of Calpha\n",
    "    Qs = out['1'][:,1,:] # rotation , convert from x,y,z (Quat) to rotate input vectors\n",
    "    Qs = Qs.unsqueeze(1).repeat((1,2,1))\n",
    "    Qs = torch.cat((torch.ones((B*L,2,1),device=Qs.device),Qs),dim=-1).reshape(B,L,2,4)\n",
    "    Qs = normQ(Qs)\n",
    "    Rs = Qs2Rs(Qs)\n",
    "    N_C_to_Rot = torch.cat((noised_dict['bb_noised']['N_CA'].reshape(B, L, 3).to(device),\n",
    "                            noised_dict['bb_noised']['C_CA'].reshape(B, L, 3).to(device)),dim=2).reshape(B,L,2,1,3)\n",
    "    rot_vecs = einsum('bnkij,bnkhj->bnki',Rs, N_C_to_Rot)\n",
    "    NC_p = CA_p + rot_vecs[:,:,0,:]*N_CA_dist #comparable but seems better not have it for true, but have it for pred\n",
    "    CC_p = CA_p + rot_vecs[:,:,1,:]*C_CA_dist #maybe this helep prevent \n",
    "\n",
    "    pred = torch.cat((NC_p,CA_p,CC_p),dim=2).reshape(B,L,3,3)\n",
    "    \n",
    "    #divide loss by real and null nodes\n",
    "    \n",
    "    fp, lp  = convert_pV_to_points(noised_dict)\n",
    "\n",
    "    real_mask = noised_dict['real_nodes_mask'].to('cuda')\n",
    "    score_scales = noised_dict['score_scales'].to('cuda')\n",
    "\n",
    "    lr, lr_d = FAPE_loss_real(pred, true, score_scales, real_mask,  d_clamp=10.0, d_clamp_inter=30.0,\n",
    "                   A=10.0, gamma=1.0, eps=1e-6)\n",
    "    \n",
    "    ln, ln_d = FAPE_loss_null(pred, fp, lp, real_mask, score_scales,  d_clamp=10.0,\n",
    "                       d_clamp_inter=30.0, A=10.0, gamma=1.0, eps=1e-6)\n",
    "    \n",
    "    structure_loss = lr*score_weights['3D_real']+ln*score_weights['3D_null']\n",
    "    \n",
    "    #score for node feats determining whether node is real or fake\n",
    "    nf_pred = out['0']\n",
    "\n",
    "    nf_feat_dim = noised_dict['real_nodes_noise'].shape[-1]\n",
    "    nf_true = torch.ones(noised_dict['real_nodes_mask'].shape+(nf_feat_dim,) + (1,),\n",
    "                         dtype=torch.float,device=device)\n",
    "\n",
    "    nf_real_mask_mult = real_mask.unsqueeze(-1).unsqueeze(-1).to(device)\n",
    "    nf_true = nf_true*nf_real_mask_mult\n",
    "\n",
    "    nf_pred = nf_pred.reshape(B,-1,nf_feat_dim)\n",
    "    pred_nf_loss = torch.sum(torch.abs(nf_true.squeeze()-nf_pred),dim=-1) #absolute value loss\n",
    "    pred_nf_loss = pred_nf_loss.to(device)\n",
    "    \n",
    "    ss_scales = to_cuda(noised_dict['score_scales'])[:,None,None]\n",
    "    pnfloss = (torch.sum((pred_nf_loss*ss_scales/L)))*score_weights['nf_real']\n",
    "    \n",
    "    final_loss = structure_loss + pnfloss\n",
    "    \n",
    "    \n",
    "    return final_loss, pnfloss.detach().cpu(), structure_loss.detach().cpu(), lr.detach().cpu(), ln.detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2999b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(self, train_loader, valid_loader, device, return_logs=False):\n",
    "        log_lossses = defaultdict(list)\n",
    "        global_logs = []\n",
    "        log_time = time.time()\n",
    "        step_time = time.time()\n",
    "        losskeeper = []\n",
    "        for train_feats in train_loader:\n",
    "            train_feats = tree.map_structure(lambda x: x.to(device), train_feats)\n",
    "            loss, aux_data = self.update_fn(train_feats)\n",
    "            losskeeper.append(float(loss.detach().cpu()))\n",
    "            if return_logs:\n",
    "                global_logs.append(loss)\n",
    "            for k,v in aux_data.items():\n",
    "                log_lossses[k].append(du.move_to_np(v))\n",
    "            self.trained_steps += 1\n",
    "\n",
    "            # Logging to terminal\n",
    "            if self.trained_steps == 1 or self.trained_steps % self._exp_conf.log_freq == 0:\n",
    "                elapsed_time = time.time() - log_time\n",
    "                log_time = time.time()\n",
    "                step_per_sec = self._exp_conf.log_freq / elapsed_time\n",
    "                rolling_losses = tree.map_structure(np.mean, log_lossses)\n",
    "                loss_log = ' '.join([\n",
    "                    f'{k}={v[0]:.4f}'\n",
    "                    for k,v in rolling_losses.items() if 'batch' not in k\n",
    "                ])\n",
    "                self._log.info(\n",
    "                    f'[{self.trained_steps}]: {loss_log}, steps/sec={step_per_sec:.5f}')\n",
    "                log_lossses = defaultdict(list)\n",
    "                \n",
    "                print(f'[{self.trained_steps}]: {loss_log}, steps/sec={step_per_sec:.5f}')\n",
    "                print(np.mean(losskeeper[-1000:]))\n",
    "\n",
    "            # Take checkpoint\n",
    "            if self._exp_conf.ckpt_dir is not None and (\n",
    "                    (self.trained_steps % self._exp_conf.ckpt_freq) == 0\n",
    "                    or (self._exp_conf.early_ckpt and self.trained_steps == 2)\n",
    "                ):\n",
    "                ckpt_path = os.path.join(\n",
    "                    self._exp_conf.ckpt_dir, f'step_{self.trained_steps}.pth')\n",
    "                du.write_checkpoint(\n",
    "                    ckpt_path,\n",
    "                    copy.deepcopy(self.model.state_dict()),\n",
    "                    self._conf,\n",
    "                    copy.deepcopy(self._optimizer.state_dict()),\n",
    "                    self.trained_epochs,\n",
    "                    self.trained_steps,\n",
    "                    logger=self._log,\n",
    "                    use_torch=True\n",
    "                )\n",
    "                \n",
    "\n",
    "                # Run evaluation\n",
    "                self._log.info(f'Running evaluation of {ckpt_path}')\n",
    "                start_time = time.time()\n",
    "                self._exp_conf.eval_dir = 'output/'\n",
    "                eval_dir = os.path.join(\n",
    "                    self._exp_conf.eval_dir, f'step_{self.trained_steps}')\n",
    "                print(eval_dir)\n",
    "                os.makedirs(eval_dir, exist_ok=True)\n",
    "                ckpt_metrics = self.eval_fn(\n",
    "                    eval_dir, valid_loader, device,\n",
    "                    noise_scale=self._exp_conf.noise_scale\n",
    "                )\n",
    "                eval_time = time.time() - start_time\n",
    "                self._log.info(f'Finished evaluation in {eval_time:.2f}s')\n",
    "            else:\n",
    "                ckpt_metrics = None\n",
    "                eval_time = None\n",
    "\n",
    "\n",
    "            if torch.isnan(loss):\n",
    "                if self._use_wandb:\n",
    "                    wandb.alert(\n",
    "                        title=\"Encountered NaN loss\",\n",
    "                        text=f\"Loss NaN after {self.trained_epochs} epochs, {self.trained_steps} steps\"\n",
    "                    )\n",
    "                raise Exception(f'NaN encountered')\n",
    "\n",
    "        if return_logs:\n",
    "            return global_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75573c13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62506e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    \n",
    "# class EMA(nn.Module):\n",
    "#     def __init__(self, mu):\n",
    "#         super(EMA, self).__init__()\n",
    "#         self.mu = mu\n",
    "#         self.shadow = {}\n",
    "\n",
    "#     def register(self, name, val):\n",
    "#         self.shadow[name] = val.clone()\n",
    "\n",
    "#     def forward(self, name, x):\n",
    "#         assert name in self.shadow\n",
    "#         new_average = (1.0 - self.mu) * x + self.mu * self.shadow[name]\n",
    "#         self.shadow[name] = new_average.clone()\n",
    "#         return new_average\n",
    "\n",
    "class Experiment:\n",
    "\n",
    "    def __init__(self,*,config_path= 'config/base_static_4H_norot.yaml', simple=True, limit=5028):\n",
    "        \"\"\"Initialize experiment.\n",
    "        Currently does not incorporate psi pred (just taken from starting points)\n",
    "        Args:\n",
    "            exp_cfg: Experiment configuration.\n",
    "        \"\"\"\n",
    "        with open(config_path, 'r') as file:\n",
    "            config = yaml.safe_load(file)\n",
    "        conf = Struct(config)\n",
    "        \n",
    "        logging.basicConfig(filename='test.log', level=logging.INFO)\n",
    "        self._log = logging.getLogger(__name__)\n",
    "        \n",
    "        self.simple = simple\n",
    "        self.limit = limit\n",
    "        \n",
    "        self._use_wandb = False\n",
    "        \n",
    "        self.rot=False\n",
    "        self.bbloss=False\n",
    "        self.device='cuda'\n",
    "        \n",
    "        # Configs\n",
    "        self._conf = conf\n",
    "        self._exp_conf = conf.experiment\n",
    "        self._diff_conf = conf.diffuser\n",
    "        self._model_conf = conf.model\n",
    "        self._data_conf = conf.data\n",
    "        self.ipa_conf = conf.model.ipa\n",
    "        \n",
    "        \n",
    "        self.B=self._exp_conf.batch_size\n",
    "        L=65 #HARDDDCODE\n",
    "        \n",
    "        self._diffuser = se3_diffuser.SE3Diffuser(self._diff_conf)\n",
    "        self._model = ScoreNetwork(\n",
    "            self._model_conf, self._diffuser,B=self.B,L=L)\n",
    "        num_parameters = sum(p.numel() for p in self._model.parameters())\n",
    "        self._exp_conf.num_parameters = num_parameters\n",
    "        self._log.info(f'Number of model parameters {num_parameters}')\n",
    "#         self._optimizer = EMA(0.980)\n",
    "#         for name, param in self._model.named_parameters():\n",
    "#             if param.requires_grad:\n",
    "#                 self._optimizer.register(name, param.data)\n",
    "        \n",
    "        \n",
    "        self._optimizer = torch.optim.Adam(\n",
    "            self._model.score_model.parameters(), lr=self._exp_conf.learning_rate)\n",
    "        dt_string = datetime.now().strftime(\"%dD_%mM_%YY_%Hh_%Mm_%Ss\")\n",
    "        dt_string_short = datetime.now().strftime(\"%dD_%mM_%YY\")\n",
    "        self._exp_conf.ckpt_dir = 'log/'\n",
    "        if self._exp_conf.ckpt_dir is not None:\n",
    "            # Set-up checkpoint location\n",
    "            ckpt_dir = os.path.join(\n",
    "                self._exp_conf.ckpt_dir,\n",
    "                self._exp_conf.name,\n",
    "                dt_string)\n",
    "            if not os.path.exists(ckpt_dir):\n",
    "                os.makedirs(ckpt_dir, exist_ok=True)\n",
    "            self._exp_conf.ckpt_dir = ckpt_dir\n",
    "            self._log.info(f'Checkpoints saved to: {ckpt_dir}')\n",
    "        else:  \n",
    "            self._log.info('Checkpoint not being saved.')\n",
    "            \n",
    "        if self._exp_conf.eval_dir is not None :\n",
    "            eval_dir = os.path.join(\n",
    "                self._exp_conf.eval_dir,\n",
    "                self._exp_conf.name,\n",
    "                dt_string)\n",
    "            self._exp_conf.eval_dir = eval_dir\n",
    "            self._log.info(f'Evaluation saved to: {eval_dir}')\n",
    "        else:\n",
    "            self._exp_conf.eval_dir = os.devnull\n",
    "            self._log.info(f'Evaluation will not be saved.')\n",
    "    #         self._aux_data_history = deque(maxlen=100)\n",
    "\n",
    "        # Warm starting\n",
    "        ckpt_model = None\n",
    "        ckpt_opt = None\n",
    "        self.trained_epochs = 0\n",
    "        self.trained_steps = 0\n",
    "\n",
    "        # Initialize experiment objects\n",
    "\n",
    "        if ckpt_model is not None:\n",
    "            ckpt_model = {k.replace('module.', ''):v for k,v in ckpt_model.items()}\n",
    "            self._model.load_state_dict(ckpt_model, strict=True)\n",
    "\n",
    "        num_parameters = sum(p.numel() for p in self._model.parameters())\n",
    "        self._exp_conf.num_parameters = num_parameters\n",
    "        self._log.info(f'Number of model parameters {num_parameters}')\n",
    "        self._optimizer = torch.optim.Adam(\n",
    "            self._model.parameters(), lr=self._exp_conf.learning_rate)\n",
    "        if ckpt_opt is not None:\n",
    "            self._optimizer.load_state_dict(ckpt_opt)\n",
    "\n",
    "        dt_string = datetime.now().strftime(\"%dD_%mM_%YY_%Hh_%Mm_%Ss\")\n",
    "        if self._exp_conf.ckpt_dir is not None:\n",
    "            # Set-up checkpoint location\n",
    "            ckpt_dir = os.path.join(\n",
    "                self._exp_conf.ckpt_dir,\n",
    "                self._exp_conf.name,\n",
    "                dt_string)\n",
    "            if not os.path.exists(ckpt_dir):\n",
    "                os.makedirs(ckpt_dir, exist_ok=True)\n",
    "            self._exp_conf.ckpt_dir = ckpt_dir\n",
    "            self._log.info(f'Checkpoints saved to: {ckpt_dir}')\n",
    "        else:  \n",
    "            self._log.info('Checkpoint not being saved.')\n",
    "        if self._exp_conf.eval_dir is not None :\n",
    "            eval_dir = os.path.join(\n",
    "                self._exp_conf.eval_dir,\n",
    "                self._exp_conf.name,\n",
    "                dt_string)\n",
    "            self._exp_conf.eval_dir = eval_dir\n",
    "            self._log.info(f'Evaluation saved to: {eval_dir}')\n",
    "        else:\n",
    "            self._exp_conf.eval_dir = os.devnull\n",
    "            self._log.info(f'Evaluation will not be saved.')\n",
    "        self._aux_data_history = deque(maxlen=100)\n",
    "        \n",
    "    @property\n",
    "    def diffuser(self):\n",
    "        return self._diffuser\n",
    "\n",
    "    @property\n",
    "    def model(self):\n",
    "        return self._model\n",
    "\n",
    "    @property\n",
    "    def conf(self):\n",
    "        return self._conf\n",
    "    \n",
    "    def create_dataset(self):\n",
    "        self._exp_conf.eval_batch_size=self.B\n",
    "        #valid limit set\n",
    "        num_workers = self._exp_conf.num_loader_workers\n",
    "        train_dataset = pdb_data_loader.PdbDataset(\n",
    "                    data_conf=self._data_conf,\n",
    "                    diffuser=self._diffuser,\n",
    "                    is_training=True,\n",
    "                    simple = self.simple\n",
    "                )\n",
    "        valid_dataset = pdb_data_loader.PdbDataset(\n",
    "                    data_conf=self._data_conf,\n",
    "                    diffuser=self._diffuser,\n",
    "                    is_training=False,\n",
    "                    simple = self.simple\n",
    "                )\n",
    "\n",
    "        train_dataset.csv = train_dataset.csv.iloc[:self.limit]\n",
    "        valid_dataset.csv = valid_dataset.csv.iloc[:256] #valid\n",
    "\n",
    "        valid_sampler=None\n",
    "        train_sampler = pdb_data_loader.TrainSampler(\n",
    "                data_conf=self._data_conf,\n",
    "                dataset=train_dataset,\n",
    "                batch_size=self.B,\n",
    "                sample_mode=self._exp_conf.sample_mode,\n",
    "            )\n",
    "\n",
    "\n",
    "        train_loader = du.create_data_loader(\n",
    "            train_dataset,\n",
    "            sampler=train_sampler,\n",
    "            np_collate=False,\n",
    "            length_batch=False,\n",
    "            batch_size=self.B,\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "            drop_last=False,\n",
    "            max_squared_res=100*2,\n",
    "        )\n",
    "        \n",
    "        # Loaders\n",
    "        \n",
    "        train_loader = du.create_data_loader(\n",
    "            train_dataset,\n",
    "            sampler=train_sampler,\n",
    "            np_collate=False,\n",
    "            length_batch=False,\n",
    "            batch_size=self.B,\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "            drop_last=False,\n",
    "            max_squared_res=self._exp_conf.max_squared_res,\n",
    "        )\n",
    "        \n",
    "        #####TRAIN DATASET again##########\n",
    "        valid_loader = du.create_data_loader(\n",
    "            valid_dataset,\n",
    "            sampler=valid_sampler,\n",
    "            np_collate=False,\n",
    "            length_batch=False,\n",
    "            batch_size=self.B,\n",
    "            shuffle=False,\n",
    "            num_workers=1,\n",
    "            drop_last=False,\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "        return train_loader, train_sampler, valid_loader, valid_sampler\n",
    "    \n",
    "    def start_training(self, return_logs=False):\n",
    "\n",
    "        # GPU mode\n",
    "        if torch.cuda.is_available():\n",
    "            device = f\"cuda\"\n",
    "            self._model = self.model.to(device)\n",
    "            print(f\"Using device: {device}\")\n",
    "            self._log.info(f\"Using device: {device}\")\n",
    "        else:\n",
    "            device = 'cpu'\n",
    "            self._model = self.model.to(device)\n",
    "            self._log.info(f\"Using device: {device}\")\n",
    "\n",
    "        self._model.train()\n",
    "        #removed valid\n",
    "        (train_loader, train_sampler, valid_loader, valid_sampler) = self.create_dataset()\n",
    "\n",
    "        logs = []\n",
    "        print(self._exp_conf.num_epoch)\n",
    "        for epoch in range(self.trained_epochs, self._exp_conf.num_epoch):\n",
    "            if train_sampler is not None:\n",
    "                train_sampler.set_epoch(epoch)\n",
    "            print(epoch)\n",
    "            epoch_log = self.train_epoch(\n",
    "                train_loader,\n",
    "                valid_loader,\n",
    "                device,\n",
    "                return_logs=return_logs\n",
    "            )\n",
    "            if return_logs:\n",
    "                logs.append(epoch_log)\n",
    "\n",
    "        self._log.info('Done')\n",
    "        return logs\n",
    "    \n",
    "    def train_epoch(self, train_loader, valid_loader, device, return_logs=False):\n",
    "        log_lossses = defaultdict(list)\n",
    "        global_logs = []\n",
    "        log_time = time.time()\n",
    "        step_time = time.time()\n",
    "        losskeeper = []\n",
    "        for train_feats in train_loader:\n",
    "            train_feats = tree.map_structure(lambda x: x.to(device), train_feats)\n",
    "            loss, aux_data = self.update_fn(train_feats)\n",
    "            losskeeper.append(float(loss.detach().cpu()))\n",
    "            for k,v in aux_data.items():\n",
    "                log_lossses[k].append(v.detach().cpu())\n",
    "            self.trained_steps += 1\n",
    "\n",
    "            # Logging to terminal\n",
    "            if self.trained_steps == 1 or self.trained_steps % self._exp_conf.log_freq == 0:\n",
    "                elapsed_time = time.time() - log_time\n",
    "                log_time = time.time()\n",
    "                step_per_sec = self._exp_conf.log_freq / elapsed_time\n",
    "                rolling_losses = tree.map_structure(np.mean, log_lossses)\n",
    "                loss_log = ' '.join([\n",
    "                    f'{k}={v[0]:.4f}'\n",
    "                    for k,v in rolling_losses.items() if 'batch' not in k\n",
    "                ])\n",
    "                self._log.info(\n",
    "                    f'[{self.trained_steps}]: {loss_log}, steps/sec={step_per_sec:.5f}')\n",
    "                log_lossses = defaultdict(list)\n",
    "                \n",
    "                print(f'[{self.trained_steps}]: {loss_log}, steps/sec={step_per_sec:.5f}')\n",
    "                print(np.mean(losskeeper[-1000:]))\n",
    "\n",
    "            # Take checkpoint\n",
    "            if self._exp_conf.ckpt_dir is not None and (\n",
    "                    (self.trained_steps % self._exp_conf.ckpt_freq) == 0\n",
    "                    or (self._exp_conf.early_ckpt and self.trained_steps == 2)\n",
    "                ):\n",
    "                ckpt_path = os.path.join(\n",
    "                    self._exp_conf.ckpt_dir, f'step_{self.trained_steps}.pth')\n",
    "                du.write_checkpoint(\n",
    "                    ckpt_path,\n",
    "                    copy.deepcopy(self.model.state_dict()),\n",
    "                    self._conf,\n",
    "                    copy.deepcopy(self._optimizer.state_dict()),\n",
    "                    self.trained_epochs,\n",
    "                    self.trained_steps,\n",
    "                    logger=self._log,\n",
    "                    use_torch=True\n",
    "                )\n",
    "                \n",
    "\n",
    "                # Run evaluation\n",
    "                self._log.info(f'Running evaluation of {ckpt_path}')\n",
    "                start_time = time.time()\n",
    "                self._exp_conf.eval_dir = 'output/'\n",
    "                eval_dir = os.path.join(\n",
    "                    self._exp_conf.eval_dir, f'step_{self.trained_steps}')\n",
    "                print(eval_dir)\n",
    "                os.makedirs(eval_dir, exist_ok=True)\n",
    "                ckpt_metrics = self.eval_fn(\n",
    "                    eval_dir, valid_loader, device,\n",
    "                    noise_scale=self._exp_conf.noise_scale\n",
    "                )\n",
    "                eval_time = time.time() - start_time\n",
    "                self._log.info(f'Finished evaluation in {eval_time:.2f}s')\n",
    "            else:\n",
    "                ckpt_metrics = None\n",
    "                eval_time = None\n",
    "\n",
    "\n",
    "            if torch.isnan(loss):\n",
    "                if self._use_wandb:\n",
    "                    wandb.alert(\n",
    "                        title=\"Encountered NaN loss\",\n",
    "                        text=f\"Loss NaN after {self.trained_epochs} epochs, {self.trained_steps} steps\"\n",
    "                    )\n",
    "                raise Exception(f'NaN encountered')\n",
    "\n",
    "        if return_logs:\n",
    "            return global_logs\n",
    "        \n",
    "    def eval_fn(self, eval_dir, valid_loader, device, min_t=None, num_t=None, noise_scale=1.0):\n",
    "        ckpt_eval_metrics = []\n",
    "        self._exp_conf.eval_batch_size=self.B\n",
    "        for valid_feats, pdb_names in valid_loader:\n",
    "            res_mask = du.move_to_np(valid_feats['res_mask'].bool())\n",
    "            fixed_mask = du.move_to_np(valid_feats['fixed_mask'].bool())\n",
    "            aatype = du.move_to_np(valid_feats['aatype'])\n",
    "            gt_prot = du.move_to_np(valid_feats['atom37_pos'])\n",
    "            batch_size = res_mask.shape[0]\n",
    "            valid_feats = tree.map_structure(\n",
    "                lambda x: x.to(device), valid_feats)\n",
    "            \n",
    "            # Run inference\n",
    "            infer_out = self.inference_fn(\n",
    "                valid_feats, min_t=min_t, num_t=num_t, noise_scale=noise_scale)\n",
    "            final_prot = infer_out['prot_traj'][0]\n",
    "            for i in range(batch_size):\n",
    "                num_res = int(np.sum(res_mask[i]).item())\n",
    "                unpad_fixed_mask = fixed_mask[i][res_mask[i]]\n",
    "                unpad_diffused_mask = 1 - unpad_fixed_mask\n",
    "                unpad_prot = final_prot[i][res_mask[i]]\n",
    "                unpad_gt_prot = gt_prot[i][res_mask[i]]\n",
    "                unpad_gt_aatype = aatype[i][res_mask[i]]\n",
    "                percent_diffused = np.sum(unpad_diffused_mask) / num_res\n",
    "\n",
    "                # Extract argmax predicted aatype\n",
    "                saved_path = au.write_prot_to_pdb(\n",
    "                    unpad_prot,\n",
    "                    os.path.join(\n",
    "                        eval_dir,\n",
    "                        f'len_{num_res}_sample_{i}_diffused_{percent_diffused:.2f}.pdb'\n",
    "                    ),\n",
    "                    no_indexing=True,\n",
    "                    b_factors=np.tile(1 - unpad_fixed_mask[..., None], 37) * 100\n",
    "                )\n",
    "                try:\n",
    "                    sample_metrics = metrics.protein_metrics(\n",
    "                        pdb_path=saved_path,\n",
    "                        atom37_pos=unpad_prot,\n",
    "                        gt_atom37_pos=unpad_gt_prot,\n",
    "                        gt_aatype=unpad_gt_aatype,\n",
    "                        diffuse_mask=unpad_diffused_mask,\n",
    "                         )\n",
    "                except ValueError as e:\n",
    "                    self._log.warning(\n",
    "                        f'Failed evaluation of length {num_res} sample {i}: {e}')\n",
    "                    continue\n",
    "                sample_metrics['step'] = self.trained_steps\n",
    "                sample_metrics['num_res'] = num_res\n",
    "                sample_metrics['fixed_residues'] = np.sum(unpad_fixed_mask)\n",
    "                sample_metrics['diffused_percentage'] = percent_diffused\n",
    "                sample_metrics['sample_path'] = saved_path\n",
    "                sample_metrics['gt_pdb'] = pdb_names[i]\n",
    "                ckpt_eval_metrics.append(sample_metrics)\n",
    "\n",
    "        # Save metrics as CSV.\n",
    "        eval_metrics_csv_path = os.path.join(eval_dir, 'metrics.csv')\n",
    "        ckpt_eval_metrics = pd.DataFrame(ckpt_eval_metrics)\n",
    "        ckpt_eval_metrics.to_csv(eval_metrics_csv_path, index=False)\n",
    "        return ckpt_eval_metrics\n",
    "    \n",
    "    \n",
    "    def _self_conditioning(self, batch):\n",
    "        model_sc = self.model(batch)\n",
    "        batch['sc_ca_t'] = model_sc['rigids'][..., 4:]\n",
    "        return batch\n",
    "    \n",
    "    def loss_fn(self,batch):\n",
    "        \n",
    "        \n",
    "        \n",
    "        model_out = self._model(batch) #score network calls graph maker/gu_net\n",
    "    \n",
    "        #loss\n",
    "        bb_mask = batch['res_mask']\n",
    "        diffuse_mask = 1 - batch['fixed_mask']\n",
    "        loss_mask = bb_mask * diffuse_mask\n",
    "        batch_size, num_res = bb_mask.shape\n",
    "\n",
    "        gt_rot_score = batch['rot_score']\n",
    "        gt_trans_score = batch['trans_score']\n",
    "        rot_score_scaling = batch['rot_score_scaling']\n",
    "        trans_score_scaling = batch['trans_score_scaling']\n",
    "        batch_loss_mask = torch.any(bb_mask, dim=-1)\n",
    "\n",
    "        pred_rot_score = model_out['rot_score'] * diffuse_mask[..., None]\n",
    "        pred_trans_score = model_out['trans_score'] * diffuse_mask[..., None]\n",
    "\n",
    "        # Translation score loss\n",
    "        trans_score_mse = (gt_trans_score - pred_trans_score)**2 * loss_mask[..., None]\n",
    "        trans_score_loss = torch.sum(\n",
    "            trans_score_mse / trans_score_scaling[:, None, None]**2,\n",
    "            dim=(-1, -2)\n",
    "        ) / (loss_mask.sum(dim=-1) + 1e-10)\n",
    "\n",
    "        # Translation x0 loss\n",
    "        gt_trans_x0 = batch['rigids_0'][..., 4:] * self._exp_conf.coordinate_scaling\n",
    "        pred_trans_x0 = model_out['rigids'][..., 4:] * self._exp_conf.coordinate_scaling\n",
    "        trans_x0_loss = torch.sum(\n",
    "            (gt_trans_x0 - pred_trans_x0)**2 * loss_mask[..., None],\n",
    "            dim=(-1, -2)\n",
    "        ) / (loss_mask.sum(dim=-1) + 1e-10)\n",
    "\n",
    "        trans_loss = (\n",
    "            trans_score_loss * (batch['t'] > self._exp_conf.trans_x0_threshold)\n",
    "            + trans_x0_loss * (batch['t'] <= self._exp_conf.trans_x0_threshold)\n",
    "        )\n",
    "        if self.rot:\n",
    "            # Rotation loss\n",
    "            if self._exp_conf.separate_rot_loss:\n",
    "                gt_rot_angle = torch.norm(gt_rot_score, dim=-1, keepdim=True)\n",
    "                gt_rot_axis = gt_rot_score / (gt_rot_angle + 1e-6)\n",
    "\n",
    "                pred_rot_angle = torch.norm(pred_rot_score, dim=-1, keepdim=True)\n",
    "                pred_rot_axis = pred_rot_score / (pred_rot_angle + 1e-6)\n",
    "\n",
    "                # Separate loss on the axis\n",
    "                axis_loss = (gt_rot_axis - pred_rot_axis)**2 * loss_mask[..., None]\n",
    "                axis_loss = torch.sum(\n",
    "                    axis_loss, dim=(-1, -2)\n",
    "                ) / (loss_mask.sum(dim=-1) + 1e-10)\n",
    "\n",
    "                # Separate loss on the angle\n",
    "                angle_loss = (gt_rot_angle - pred_rot_angle)**2 * loss_mask[..., None]\n",
    "                angle_loss = torch.sum(\n",
    "                    angle_loss / rot_score_scaling[:, None, None]**2,\n",
    "                    dim=(-1, -2)\n",
    "                ) / (loss_mask.sum(dim=-1) + 1e-10)\n",
    "                angle_loss *= self._exp_conf.rot_loss_weight\n",
    "                angle_loss *= batch['t'] > self._exp_conf.rot_loss_t_threshold\n",
    "                rot_loss = angle_loss + axis_loss\n",
    "            else:\n",
    "                rot_mse = (gt_rot_score - pred_rot_score)**2 * loss_mask[..., None]\n",
    "                rot_loss = torch.sum(\n",
    "                    rot_mse / rot_score_scaling[:, None, None]**2,\n",
    "                    dim=(-1, -2)\n",
    "                ) / (loss_mask.sum(dim=-1) + 1e-10)\n",
    "                rot_loss *= self._exp_conf.rot_loss_weight\n",
    "                rot_loss *= batch['t'] > self._exp_conf.rot_loss_t_threshold\n",
    "            rot_loss *= int(self._diff_conf.diffuse_rot)\n",
    "            \n",
    "        else:\n",
    "            rot_loss = torch.zeros_like(trans_loss,device=self.device)\n",
    "        \n",
    "\n",
    "        if self.bbloss:\n",
    "            # Backbone atom loss\n",
    "            pred_atom37 = model_out['atom37'][:, :, :5]\n",
    "            gt_rigids = ru.Rigid.from_tensor_7(batch['rigids_0'].type(torch.float32))\n",
    "            gt_psi = batch['torsion_angles_sin_cos'][..., 2, :]\n",
    "            gt_atom37, atom37_mask, _, _ = all_atom.compute_backbone(\n",
    "                gt_rigids, gt_psi)\n",
    "            gt_atom37 = gt_atom37[:, :, :5]\n",
    "            atom37_mask = atom37_mask[:, :, :5]\n",
    "\n",
    "            gt_atom37 = gt_atom37.to(pred_atom37.device)\n",
    "            atom37_mask = atom37_mask.to(pred_atom37.device)\n",
    "            bb_atom_loss_mask = atom37_mask * loss_mask[..., None]\n",
    "            bb_atom_loss = torch.sum(\n",
    "                (pred_atom37 - gt_atom37)**2 * bb_atom_loss_mask[..., None],\n",
    "                dim=(-1, -2, -3)\n",
    "            ) / (bb_atom_loss_mask.sum(dim=(-1, -2)) + 1e-10)\n",
    "            bb_atom_loss *= self._exp_conf.bb_atom_loss_weight\n",
    "            bb_atom_loss *= batch['t'] < self._exp_conf.bb_atom_loss_t_filter\n",
    "            bb_atom_loss *= self._exp_conf.aux_loss_weight\n",
    "\n",
    "\n",
    "            # Pairwise distance loss\n",
    "            gt_flat_atoms = gt_atom37.reshape([batch_size, num_res*5, 3])\n",
    "            gt_pair_dists = torch.linalg.norm(\n",
    "                gt_flat_atoms[:, :, None, :] - gt_flat_atoms[:, None, :, :], dim=-1)\n",
    "            pred_flat_atoms = pred_atom37.reshape([batch_size, num_res*5, 3])\n",
    "            pred_pair_dists = torch.linalg.norm(\n",
    "                pred_flat_atoms[:, :, None, :] - pred_flat_atoms[:, None, :, :], dim=-1)\n",
    "\n",
    "            flat_loss_mask = torch.tile(loss_mask[:, :, None], (1, 1, 5))\n",
    "            flat_loss_mask = flat_loss_mask.reshape([batch_size, num_res*5])\n",
    "            flat_res_mask = torch.tile(bb_mask[:, :, None], (1, 1, 5))\n",
    "            flat_res_mask = flat_res_mask.reshape([batch_size, num_res*5])\n",
    "\n",
    "            gt_pair_dists = gt_pair_dists * flat_loss_mask[..., None]\n",
    "            pred_pair_dists = pred_pair_dists * flat_loss_mask[..., None]\n",
    "            pair_dist_mask = flat_loss_mask[..., None] * flat_res_mask[:, None, :]\n",
    "\n",
    "            # No loss on anything >6A\n",
    "            proximity_mask = gt_pair_dists < 6\n",
    "            pair_dist_mask  = pair_dist_mask * proximity_mask\n",
    "\n",
    "            dist_mat_loss = torch.sum(\n",
    "                (gt_pair_dists - pred_pair_dists)**2 * pair_dist_mask,\n",
    "                dim=(1, 2))\n",
    "            dist_mat_loss /= (torch.sum(pair_dist_mask, dim=(1, 2)) - num_res)\n",
    "            dist_mat_loss *= self._exp_conf.dist_mat_loss_weight\n",
    "            dist_mat_loss *= batch['t'] < self._exp_conf.dist_mat_loss_t_filter\n",
    "            dist_mat_loss *= self._exp_conf.aux_loss_weight\n",
    "            \n",
    "        else:\n",
    "            bb_atom_loss = torch.zeros_like(trans_loss,device=self.device)\n",
    "            dist_mat_loss = torch.zeros_like(trans_loss,device=self.device)\n",
    "\n",
    "         \n",
    "        final_loss = (\n",
    "            rot_loss\n",
    "            + trans_loss\n",
    "            + bb_atom_loss\n",
    "            + dist_mat_loss\n",
    "        )\n",
    "\n",
    "        def normalize_loss(x):\n",
    "            return x.sum() /  (batch_loss_mask.sum() + 1e-9)\n",
    "\n",
    "        aux_data = {\n",
    "            'batch_train_loss': final_loss,\n",
    "            'batch_rot_loss': rot_loss,\n",
    "            'batch_trans_loss': trans_loss,\n",
    "            'batch_bb_atom_loss': bb_atom_loss,\n",
    "            'batch_dist_mat_loss': dist_mat_loss,\n",
    "            'total_loss': normalize_loss(final_loss),\n",
    "            'rot_loss': normalize_loss(rot_loss),\n",
    "            'trans_loss': normalize_loss(trans_loss),\n",
    "            'bb_atom_loss': normalize_loss(bb_atom_loss),\n",
    "            'dist_mat_loss': normalize_loss(dist_mat_loss),\n",
    "            'examples_per_step': torch.tensor(batch_size),\n",
    "            'res_length': torch.mean(torch.sum(bb_mask, dim=-1)),\n",
    "        }\n",
    "\n",
    "        return normalize_loss(final_loss),aux_data\n",
    "        \n",
    "        \n",
    "    def update_fn(self, data):\n",
    "        \"\"\"Updates the state using some data and returns metrics.\"\"\"\n",
    "        self._optimizer.zero_grad()\n",
    "        loss, aux_data = self.loss_fn(data)\n",
    "        loss.backward()\n",
    "        self._optimizer.step()\n",
    "        return loss, aux_data\n",
    "\n",
    "    def _calc_trans_0(self, trans_score, trans_t, t):\n",
    "        beta_t = self._diffuser._se3_diffuser._r3_diffuser.marginal_b_t(t)\n",
    "        beta_t = beta_t[..., None, None]\n",
    "        cond_var = 1 - torch.exp(-beta_t)\n",
    "        return (trans_score * cond_var + trans_t) / torch.exp(-1/2*beta_t)\n",
    "\n",
    "    def _set_t_feats(self, feats, t, t_placeholder):\n",
    "        feats['t'] = t * t_placeholder\n",
    "        rot_score_scaling, trans_score_scaling = self.diffuser.score_scaling(t)\n",
    "        feats['rot_score_scaling'] = rot_score_scaling * t_placeholder\n",
    "        feats['trans_score_scaling'] = trans_score_scaling * t_placeholder\n",
    "        return feats\n",
    "\n",
    "    def forward_traj(self, x_0, min_t, num_t):\n",
    "        forward_steps = np.linspace(min_t, 1.0, num_t)[:-1]\n",
    "        x_traj = [x_0]\n",
    "        for t in forward_steps:\n",
    "            x_t = self.diffuser.se3_diffuser._r3_diffuser.forward(\n",
    "                x_traj[-1], t, num_t)\n",
    "            x_traj.append(x_t)\n",
    "        x_traj = torch.stack(x_traj, axis=0)\n",
    "        return x_traj\n",
    "\n",
    "    def inference_fn(\n",
    "            self,\n",
    "            data_init,\n",
    "            num_t=None,\n",
    "            min_t=None,\n",
    "            center=True,\n",
    "            aux_traj=False,\n",
    "            self_condition=True,\n",
    "            noise_scale=1.0,\n",
    "        ):\n",
    "        \"\"\"Inference function.\n",
    "\n",
    "        Args:\n",
    "            data_init: Initial data values for sampling.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Run reverse process.\n",
    "        sample_feats = copy.deepcopy(data_init)\n",
    "        device = sample_feats['rigids_t'].device\n",
    "#         for k,v in sample_feats.items():\n",
    "#             sample_feats[k] = torch.repeat_interleave(sample_feats[k],torch.tensor([2]*8).to(device),dim=0)\n",
    "        device = sample_feats['rigids_t'].device\n",
    "        if sample_feats['rigids_t'].ndim == 2:\n",
    "            t_placeholder = torch.ones((1,)).to(device)#test remove\n",
    "#             t_placeholder = torch.ones(\n",
    "#                 (self.B,)).to(device)\n",
    "        else:\n",
    "            t_placeholder = torch.ones(\n",
    "                (sample_feats['rigids_t'].shape[0],)).to(device)\n",
    "        if num_t is None:\n",
    "            num_t = self._data_conf.num_t\n",
    "        if min_t is None:\n",
    "            min_t = self._data_conf.min_t\n",
    "        reverse_steps = np.linspace(min_t, 1.0, num_t)[::-1]\n",
    "        dt = 1/num_t\n",
    "        all_rigids = [du.move_to_np(copy.deepcopy(sample_feats['rigids_t']))]\n",
    "        all_bb_prots = []\n",
    "        all_trans_0_pred = []\n",
    "        all_bb_0_pred = []\n",
    "        with torch.no_grad():\n",
    "#             if self._model_conf.embed.embed_self_conditioning and self_condition:\n",
    "#                 sample_feats = self._set_t_feats(\n",
    "#                     sample_feats, reverse_steps[0], t_placeholder)\n",
    "#                 sample_feats = self._self_conditioning(sample_feats)\n",
    "            for t in reverse_steps:\n",
    "                if t > min_t:\n",
    "                    sample_feats = self._set_t_feats(sample_feats, t, t_placeholder)\n",
    "                    model_out = self._model(sample_feats)\n",
    "                    #model_out = self.model(sample_feats)\n",
    "                    rot_score = model_out['rot_score']\n",
    "                    trans_score = model_out['trans_score']\n",
    "                    rigid_pred = model_out['rigids']\n",
    "#                     if self._model_conf.embed.embed_self_conditioning:\n",
    "#                         sample_feats['sc_ca_t'] = rigid_pred[..., 4:]\n",
    "                    fixed_mask = sample_feats['fixed_mask'] * sample_feats['res_mask']\n",
    "                    diffuse_mask = (1 - sample_feats['fixed_mask']) * sample_feats['res_mask']\n",
    "                    rigids_t = self.diffuser.reverse(\n",
    "                        rigid_t=ru.Rigid.from_tensor_7(sample_feats['rigids_t']),\n",
    "                        rot_score=du.move_to_np(rot_score),\n",
    "                        trans_score=du.move_to_np(trans_score),\n",
    "                        diffuse_mask=du.move_to_np(diffuse_mask),\n",
    "                        t=t,\n",
    "                        dt=dt,\n",
    "                        center=center,\n",
    "                        noise_scale=noise_scale,\n",
    "                    )\n",
    "                else:\n",
    "                    model_out = self._model(sample_feats)\n",
    "                    #model_out = self.model(sample_feats)\n",
    "                    rigids_t = ru.Rigid.from_tensor_7(model_out['rigids'])\n",
    "                sample_feats['rigids_t'] = rigids_t.to_tensor_7().to(device)\n",
    "                if aux_traj:\n",
    "                    all_rigids.append(du.move_to_np(rigids_t.to_tensor_7()))\n",
    "\n",
    "                # Calculate x0 prediction derived from score predictions.\n",
    "                gt_trans_0 = sample_feats['rigids_t'][..., 4:]\n",
    "                pred_trans_0 = rigid_pred[..., 4:]\n",
    "                trans_pred_0 = diffuse_mask[..., None] * pred_trans_0 + fixed_mask[..., None] * gt_trans_0\n",
    "                psi_pred = model_out['psi']\n",
    "                if aux_traj:\n",
    "                    atom37_0 = all_atom.compute_backbone(\n",
    "                        ru.Rigid.from_tensor_7(rigid_pred),\n",
    "                        psi_pred\n",
    "                    )[0]\n",
    "                    all_bb_0_pred.append(du.move_to_np(atom37_0))\n",
    "                    all_trans_0_pred.append(du.move_to_np(trans_pred_0))\n",
    "                atom37_t = all_atom.compute_backbone(\n",
    "                    rigids_t, psi_pred)[0]\n",
    "                all_bb_prots.append(du.move_to_np(atom37_t))\n",
    "\n",
    "        # Flip trajectory so that it starts from t=0.\n",
    "        # This helps visualization.\n",
    "        flip = lambda x: np.flip(np.stack(x), (0,))\n",
    "        all_bb_prots = flip(all_bb_prots)\n",
    "        if aux_traj:\n",
    "            all_rigids = flip(all_rigids)\n",
    "            all_trans_0_pred = flip(all_trans_0_pred)\n",
    "            all_bb_0_pred = flip(all_bb_0_pred)\n",
    "\n",
    "        ret = {\n",
    "            'prot_traj': all_bb_prots,\n",
    "        }\n",
    "        if aux_traj:\n",
    "            ret['rigid_traj'] = all_rigids\n",
    "            ret['trans_traj'] = all_trans_0_pred\n",
    "            ret['psi_pred'] = psi_pred[None]\n",
    "            ret['rigid_0_traj'] = all_bb_0_pred\n",
    "        return ret\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33e16cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import se3_diffuse.utils as du"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a31f547",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_rigid_diffuser import so3_diffuser\n",
    "from data_rigid_diffuser import r3_diffuser\n",
    "from data_rigid_diffuser import oneHot_diffuser\n",
    "from scipy.spatial.transform import Rotation\n",
    "from data_rigid_diffuser import rigid_utils as ru\n",
    "import yaml\n",
    "from data_rigid_diffuser.diffuser import FrameDiffNoise\n",
    "from gudiff_model import Data_Graph\n",
    "from gudiff_model.Data_Graph import build_npose_from_coords, dump_coord_pdb, define_graph_edges, make_pe_encoding\n",
    "from gudiff_model.Data_Graph import Helix4_Dataset, Make_KNN_MP_Graphs\n",
    "from gudiff_model.Data_Graph_Null import  Make_nullKNN_MP_Graphs\n",
    "from data_rigid_diffuser import diffuser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf443f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from se3_transformer.model.basis import get_basis, update_basis_with_fused\n",
    "from se3_transformer.model.transformer import Sequential, SE3Transformer\n",
    "from se3_transformer.model.transformer_topk import SE3Transformer_topK\n",
    "from se3_transformer.model.FAPE_Loss import FAPE_loss, Qs2Rs, normQ\n",
    "from se3_transformer.model.layers.attentiontopK import AttentionBlockSE3\n",
    "from se3_transformer.model.layers.linear import LinearSE3\n",
    "from se3_transformer.model.layers.convolution import ConvSE3, ConvSE3FuseLevel\n",
    "from se3_transformer.model.layers.norm import NormSE3\n",
    "from se3_transformer.model.layers.pooling import GPooling, Latent_Unpool, Unpool_Layer\n",
    "from se3_transformer.runtime.utils import str2bool, to_cuda\n",
    "from se3_transformer.model.fiber import Fiber\n",
    "from se3_transformer.model.transformer import get_populated_edge_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48bf9476",
   "metadata": {},
   "outputs": [],
   "source": [
    "from se3_transformer.model.FAPE_Loss import FAPE_loss_null, FAPE_loss_real\n",
    "from se3_transformer.model.FAPE_Loss import get_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e213e885",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3eae2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B = 16\n",
    "# L=65\n",
    "# limit = 1028\n",
    "# h4_trainData = Helix4_Dataset(coords_tog[:limit])\n",
    "# h4_valData = Helix4_Dataset(coords_apa[:limit])\n",
    "# train_dL = DataLoader(h4_trainData, batch_size=B, shuffle=True, drop_last=True)\n",
    "# val_dL   = DataLoader(h4_valData, batch_size=B, shuffle=True, drop_last=True)\n",
    "# testiter = iter(train_dL)\n",
    "# bb_dict = next(testiter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbf8e13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87f67d73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "581b3006",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I think I adjusted inputs and outputs,\n",
    "#sigmoid for nodes features for real/null pred?\n",
    "#need loss function for real/null nodes\n",
    "#need to get function to just pull real nodes for viewing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16c6656",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b0ba79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02ba860f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pV_to_points(dict_in):\n",
    "    \n",
    "    CA_fp  = dict_in['bb_firstp']['CA'].reshape(B, L, 3).to(device)\n",
    "    NC_fp = CA_fp + dict_in['bb_firstp']['N_CA'].reshape(B, L, 3).to(device)\n",
    "    CC_fp = CA_fp + dict_in['bb_firstp']['C_CA'].reshape(B, L, 3).to(device)\n",
    "    fp =  torch.cat((NC_fp,CA_fp,CC_fp),dim=2).reshape(B,L,3,3)\n",
    "    \n",
    "    CA_lp  = dict_in['bb_firstp']['CA'].reshape(B, L, 3).to(device)\n",
    "    NC_lp = CA_fp + dict_in['bb_firstp']['N_CA'].reshape(B, L, 3).to(device)\n",
    "    CC_lp = CA_fp + dict_in['bb_firstp']['C_CA'].reshape(B, L, 3).to(device)\n",
    "    lp =  torch.cat((NC_lp,CA_lp,CC_lp),dim=2).reshape(B,L,3,3)\n",
    "    \n",
    "    return fp, lp\n",
    "def get_noise_pred_true_null(noised_dict, batched_t, graph_maker, graph_unet):\n",
    "    \n",
    "    CA_t  = noised_dict['bb_shifted']['CA'].reshape(B, L, 3).to('cuda')\n",
    "    NC_t = CA_t + noised_dict['bb_shifted']['N_CA'].reshape(B, L, 3).to('cuda')*N_CA_dist\n",
    "    CC_t = CA_t + noised_dict['bb_shifted']['C_CA'].reshape(B, L, 3).to('cuda')*C_CA_dist\n",
    "    true =  torch.cat((NC_t,CA_t,CC_t),dim=2).reshape(B,L,3,3)\n",
    "    \n",
    "    CA_n  = noised_dict['bb_noised']['CA'].reshape(B, L, 3).to('cuda')\n",
    "    NC_n = CA_n + noised_dict['bb_noised']['N_CA'].reshape(B, L, 3).to('cuda')*N_CA_dist\n",
    "    CC_n = CA_n + noised_dict['bb_noised']['C_CA'].reshape(B, L, 3).to('cuda')*C_CA_dist\n",
    "    noise_xyz =  torch.cat((NC_n,CA_n,CC_n),dim=2).reshape(B,L,3,3)\n",
    "    \n",
    "    feat_dict = graph_maker.prep_for_network(noised_dict)\n",
    "    out = graph_unet(feat_dict, batched_t)\n",
    "    CA_p = out['1'][:,0,:].reshape(B, L, 3)+CA_n #translation of Calpha\n",
    "    Qs = out['1'][:,1,:] # rotation\n",
    "    Qs = Qs.unsqueeze(1).repeat((1,2,1))\n",
    "    Qs = torch.cat((torch.ones((B*L,2,1),device=Qs.device),Qs),dim=-1).reshape(B,L,2,4)\n",
    "    Qs = normQ(Qs)\n",
    "    Rs = Qs2Rs(Qs)\n",
    "    N_C_to_Rot = torch.cat((noised_dict['bb_noised']['N_CA'].reshape(B, L, 3).to('cuda'),\n",
    "                            noised_dict['bb_noised']['C_CA'].reshape(B, L, 3).to('cuda')),dim=2).reshape(B,L,2,1,3)\n",
    "\n",
    "    \n",
    "    rot_vecs = einsum('bnkij,bnkhj->bnki',Rs, N_C_to_Rot)\n",
    "    NC_p = CA_p + rot_vecs[:,:,0,:].to('cuda')*N_CA_dist\n",
    "    CC_p = CA_p + rot_vecs[:,:,1,:].reshape(B, L, 3).to('cuda')*C_CA_dist\n",
    "\n",
    "    pred = torch.cat((NC_p,CA_p,CC_p),dim=2).reshape(B,L,3,3)\n",
    "    \n",
    "    nf_pred = out['0']\n",
    "    \n",
    "    nf_pred = out['0']\n",
    "    real_nodes_pred = torch.round(nf_pred ).clamp(0,1)\n",
    "    real_nodes_pred_mask = (real_nodes_pred.squeeze().sum(-1)>1.99).reshape(B,L)\n",
    "    \n",
    "    real_nodes_true_mask = noised_dict['real_nodes_mask']\n",
    "    #place roll here later\n",
    "#     true = true.to('cpu').numpy()*10\n",
    "#     noise_xyz = noise_xyz.to('cpu').numpy()*10\n",
    "#     pred = pred.detach().to('cpu').numpy()*10\n",
    "    \n",
    "    \n",
    "    return true, noise_xyz, pred , real_nodes_pred_mask, real_nodes_true_mask\n",
    "        \n",
    "def roll2_continous_true(real_mask_in):\n",
    "    \"\"\"Return roll amount to set zero on Nterminal residue for pdb file view\"\"\"\n",
    "\n",
    "    roll_con_out = []\n",
    "    for i,rmr in enumerate(real_mask_in):\n",
    "        ep_bool = (rmr^rmr.roll(-1) | rmr^rmr.roll(1)) & rmr\n",
    "        si = torch.arange(ep_bool.shape[0])[ep_bool]\n",
    "        #circular if start/end real nodes and we need to roll\n",
    "        if rmr[0] and rmr[-1]:\n",
    "            #roll last group across barrier\n",
    "            roll_con = -si[-1]\n",
    "        elif not rmr[0]: #move first group to front\n",
    "            roll_con = -si[0]\n",
    "        else:\n",
    "            roll_con=0\n",
    "\n",
    "        roll_con_out.append(roll_con)\n",
    "\n",
    "    return roll_con_out\n",
    "      \n",
    "def dump_tnp_null(true, noise, pred, t_val, e=0, numOut=1, real_mask=None, pred_mask=None, outdir='output/'):\n",
    "    \n",
    "    if numOut>true.shape[0]:\n",
    "        numOut = true.shape[0]\n",
    "    \n",
    "    tnk_dir = f'{outdir}/true_node_mask/'\n",
    "    pnk_dir = f'{outdir}/pred_node_mask/'\n",
    "    f_dir = f'{outdir}/full/'\n",
    "    \n",
    "    if not os.path.isdir(tnk_dir) and real_mask is not None:\n",
    "        os.makedirs(tnk_dir)\n",
    "    if not os.path.isdir(pnk_dir) and pred_mask is not None:\n",
    "        os.makedirs(pnk_dir)\n",
    "    if not os.path.isdir(f_dir) and real_mask is not None:\n",
    "        os.makedirs(f_dir)\n",
    "    \n",
    "    if real_mask is not None:\n",
    "        rc = roll2_continous_true(real_mask)\n",
    "        for x in range(numOut):\n",
    "            t_o = true[x].roll(int(rc[x]),dims=0).detach().to('cpu').numpy()*10\n",
    "            n_o = noise[x].roll(int(rc[x]),dims=0).detach().to('cpu').numpy()*10\n",
    "            p_o = pred[x].roll(int(rc[x]),dims=0).detach().to('cpu').numpy()*10\n",
    "            dump_coord_pdb(t_o, fileOut=f'{f_dir}/true_{t_val[x]*100:.0f}_e{e}_{x}.pdb')\n",
    "            dump_coord_pdb(n_o, fileOut=f'{f_dir}/noise_{t_val[x]*100:.0f}_e{e}_{x}.pdb')\n",
    "            dump_coord_pdb(p_o, fileOut=f'{f_dir}/pred_{t_val[x]*100:.0f}_e{e}_{x}.pdb')\n",
    "        \n",
    "    if pred_mask is not None:\n",
    "        rc = roll2_continous_true(pred_mask)\n",
    "        for x,c in enumerate(np.arange(numOut)):\n",
    "            t_o = true[x].roll(int(rc[x]),dims=0).detach().to('cpu').numpy()*10\n",
    "            n_o = noise[x].roll(int(rc[x]),dims=0).detach().to('cpu').numpy()*10\n",
    "            p_o = pred[x].roll(int(rc[x]),dims=0).detach().to('cpu').numpy()*10\n",
    "            pm = pred_mask[x].roll(int(rc[x]),dims=0)\n",
    "            dump_coord_pdb(t_o[pm], fileOut=f'{pnk_dir}/true_{t_val[x]*100:.0f}_e{e}_{x}.pdb')\n",
    "            dump_coord_pdb(n_o[pm], fileOut=f'{pnk_dir}/noise_{t_val[x]*100:.0f}_e{e}_{x}.pdb')\n",
    "            dump_coord_pdb(p_o[pm], fileOut=f'{pnk_dir}/pred_{t_val[x]*100:.0f}_e{e}_{x}.pdb')\n",
    "            \n",
    "    if real_mask is not None:\n",
    "        rc = roll2_continous_true(real_mask)\n",
    "        for x,c in enumerate(np.arange(numOut)):\n",
    "            t_o = true[x].roll(int(rc[x]),dims=0).detach().to('cpu').numpy()*10\n",
    "            n_o = noise[x].roll(int(rc[x]),dims=0).detach().to('cpu').numpy()*10\n",
    "            p_o = pred[x].roll(int(rc[x]),dims=0).detach().to('cpu').numpy()*10\n",
    "            rm = real_mask[x].roll(int(rc[x]),dims=0)\n",
    "            dump_coord_pdb(t_o[rm], fileOut=f'{pnk_dir}/true_{t_val[x]*100:.0f}_e{e}_{x}.pdb')\n",
    "            dump_coord_pdb(n_o[rm], fileOut=f'{pnk_dir}/noise_{t_val[x]*100:.0f}_e{e}_{x}.pdb')\n",
    "            dump_coord_pdb(p_o[rm], fileOut=f'{pnk_dir}/pred_{t_val[x]*100:.0f}_e{e}_{x}.pdb')\n",
    "            \n",
    "#     if real_mask is not None:\n",
    "#         rc = roll2_continous_true(real_mask)\n",
    "#         for x in range(numOut):\n",
    "#             dump_coord_pdb(true[x][real_mask[x]], fileOut=f'{tnk_dir}/true_{t_val[x]*100:.0f}_e{e}_{x}.pdb')\n",
    "#             dump_coord_pdb(noise[x][real_mask[x]], fileOut=f'{tnk_dir}/noise_{t_val[x]*100:.0f}_e{e}_{x}.pdb')\n",
    "#             dump_coord_pdb(pred[x][real_mask[x]], fileOut=f'{tnk_dir}/pred_{t_val[x]*100:.0f}_e{e}_{x}.pdb')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7e3e99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pV_to_points(dict_in):\n",
    "    \n",
    "    CA_fp  = dict_in['bb_firstp']['CA'].to(device)\n",
    "    NC_fp = CA_fp + dict_in['bb_firstp']['N_CA'].to(device)\n",
    "    CC_fp = CA_fp +dict_in['bb_firstp']['C_CA'].to(device)\n",
    "    fp =  torch.cat((NC_fp,CA_fp,CC_fp),dim=2).reshape(B,1,3,3)\n",
    "    \n",
    "    CA_lp  = dict_in['bb_lastp']['CA'].to(device)\n",
    "    NC_lp = CA_fp + dict_in['bb_lastp']['N_CA'].to(device)\n",
    "    CC_lp = CA_fp + dict_in['bb_lastp']['C_CA'].to(device)\n",
    "    lp =  torch.cat((NC_lp,CA_lp,CC_lp),dim=2).reshape(B,1,3,3)\n",
    "    \n",
    "    return fp, lp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7eeb65f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def model_step_null(noised_dict, batched_t, graph_maker, graph_unet, train=True):\n",
    "    #prep coordinates for output display from and comparison via FAPE\n",
    "    \n",
    "    CA_t  = noised_dict['bb_shifted']['CA'].reshape(B, L, 3).to(device)\n",
    "    NC_t = CA_t + noised_dict['bb_shifted']['N_CA'].reshape(B, L, 3).to(device)#not mult by bond distance, seems to help?\n",
    "    CC_t = CA_t + noised_dict['bb_shifted']['C_CA'].reshape(B, L, 3).to(device)#not mult \n",
    "    true =  torch.cat((NC_t,CA_t,CC_t),dim=2).reshape(B,L,3,3)\n",
    "\n",
    "    CA_n  = noised_dict['bb_noised']['CA'].reshape(B, L, 3).to(device)\n",
    "    NC_n = CA_n + noised_dict['bb_noised']['N_CA'].reshape(B, L, 3).to(device)\n",
    "    CC_n = CA_n + noised_dict['bb_noised']['C_CA'].reshape(B, L, 3).to(device)\n",
    "    noise_xyz =  torch.cat((NC_n,CA_n,CC_n),dim=2).reshape(B,L,3,3)\n",
    "\n",
    "    #prepare graphs\n",
    "    feat_dict = graph_maker.prep_for_network(noised_dict, cuda=True)\n",
    "    out =graph_unet(feat_dict,batched_t)\n",
    "    \n",
    "    \n",
    "    #FAPE Loss for the prediction\n",
    "    CA_p = out['1'][:,0,:].reshape(B, L, 3)+CA_n #translation of Calpha\n",
    "    Qs = out['1'][:,1,:] # rotation , convert from x,y,z (Quat) to rotate input vectors\n",
    "    Qs = Qs.unsqueeze(1).repeat((1,2,1))\n",
    "    Qs = torch.cat((torch.ones((B*L,2,1),device=Qs.device),Qs),dim=-1).reshape(B,L,2,4)\n",
    "    Qs = normQ(Qs)\n",
    "    Rs = Qs2Rs(Qs)\n",
    "    N_C_to_Rot = torch.cat((noised_dict['bb_noised']['N_CA'].reshape(B, L, 3).to(device),\n",
    "                            noised_dict['bb_noised']['C_CA'].reshape(B, L, 3).to(device)),dim=2).reshape(B,L,2,1,3)\n",
    "    rot_vecs = einsum('bnkij,bnkhj->bnki',Rs, N_C_to_Rot)\n",
    "    NC_p = CA_p + rot_vecs[:,:,0,:]*N_CA_dist #comparable but seems better not have it for true, but have it for pred\n",
    "    CC_p = CA_p + rot_vecs[:,:,1,:]*C_CA_dist #maybe this helep prevent \n",
    "\n",
    "    pred = torch.cat((NC_p,CA_p,CC_p),dim=2).reshape(B,L,3,3)\n",
    "    \n",
    "    #divide loss by real and null nodes\n",
    "    \n",
    "    fp, lp  = convert_pV_to_points(noised_dict)\n",
    "\n",
    "    real_mask = noised_dict['real_nodes_mask'].to('cuda')\n",
    "    score_scales = noised_dict['score_scales'].to('cuda')\n",
    "\n",
    "    lr, lr_d = FAPE_loss_real(pred, true, score_scales, real_mask,  d_clamp=10.0, d_clamp_inter=30.0,\n",
    "                   A=10.0, gamma=1.0, eps=1e-6)\n",
    "    \n",
    "    ln, ln_d = FAPE_loss_null(pred, fp, lp, real_mask, score_scales,  d_clamp=10.0,\n",
    "                       d_clamp_inter=30.0, A=10.0, gamma=1.0, eps=1e-6)\n",
    "    \n",
    "    structure_loss = lr*score_weights['3D_real']+ln*score_weights['3D_null']\n",
    "    \n",
    "    #score for node feats determining whether node is real or fake\n",
    "    nf_pred = out['0']\n",
    "\n",
    "    nf_feat_dim = noised_dict['real_nodes_noise'].shape[-1]\n",
    "    nf_true = torch.ones(noised_dict['real_nodes_mask'].shape+(nf_feat_dim,) + (1,),\n",
    "                         dtype=torch.float,device=device)\n",
    "\n",
    "    nf_real_mask_mult = real_mask.unsqueeze(-1).unsqueeze(-1).to(device)\n",
    "    nf_true = nf_true*nf_real_mask_mult\n",
    "\n",
    "    nf_pred = nf_pred.reshape(B,-1,nf_feat_dim)\n",
    "    pred_nf_loss = torch.sum(torch.abs(nf_true.squeeze()-nf_pred),dim=-1) #absolute value loss\n",
    "    pred_nf_loss = pred_nf_loss.to(device)\n",
    "    \n",
    "    ss_scales = to_cuda(noised_dict['score_scales'])[:,None,None]\n",
    "    pnfloss = (torch.sum((pred_nf_loss*ss_scales/L)))*score_weights['nf_real']\n",
    "    \n",
    "    final_loss = structure_loss + pnfloss\n",
    "    \n",
    "    \n",
    "    return final_loss, pnfloss.detach().cpu(), structure_loss.detach().cpu(), lr.detach().cpu(), ln.detach().cpu()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9e308d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ea96ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfa33c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4d19f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97322803",
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda'\n",
    "\n",
    "B = 8\n",
    "L= 128\n",
    "limit = 1028\n",
    "prot_trainData = Data_Graph.ProteinBB_Dataset(coords_tog[:limit], n_nodes=L,\n",
    "              n_atoms=5, coord_div=10, cast_type=torch.float32)\n",
    "train_dL = DataLoader(prot_trainData, batch_size=B, shuffle=True, drop_last=True)\n",
    "stride=4##########\n",
    "mkg = Make_nullKNN_MP_Graphs(KNN=30, mp_stride=stride, n_nodes=L)\n",
    "\n",
    "score_weights = {}\n",
    "score_weights['nf_real'] = torch.tensor(0.2,device=device)\n",
    "score_weights['3D_real'] = torch.tensor(1.0,device=device)\n",
    "score_weights['3D_null'] = torch.tensor(1.0,device=device)\n",
    "\n",
    "config_path='data_rigid_diffuser/base.yaml'\n",
    "fnd = FrameDiffNoise(config_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77cce375",
   "metadata": {},
   "outputs": [],
   "source": [
    "gunn= GraphUNet_Null(fiber_start = Fiber({0:17, 1:2}),\n",
    "                     fiber_out = Fiber({0:5,1:2}),\n",
    "                      k=4,\n",
    "                      batch_size = B,\n",
    "                      stride=stride,\n",
    "                       max_degree=3,\n",
    "                       channels=64,\n",
    "                      num_heads = 16,\n",
    "                      channels_div=8,\n",
    "                      num_layers = 1,\n",
    "                     num_layers_ca = 2,\n",
    "                     edge_feature_dim=1,\n",
    "                     latent_pool_type = 'max',\n",
    "                     t_size = 12,\n",
    "                     mult=2,\n",
    "                    zero_lin=True,\n",
    "                   use_tdeg1 = False,\n",
    "                 cuda=True).to('cuda')\n",
    "\n",
    "opti = torch.optim.Adam(gunn.parameters(), lr=0.0005, weight_decay=5e-6)\n",
    "#opti =torch.optim.Adadelta(gunn.parameters(), lr=0.0005, weight_decay=5e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7bf8bd0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.optim.adadelta.Adadelta"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.optim.Adadelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b7af006",
   "metadata": {},
   "outputs": [],
   "source": [
    "testiter = iter(train_dL)\n",
    "bb_dict = next(testiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1be1401e",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_cpu = np.ones((B,))*0.01\n",
    "noised_dict = fnd.forward(bb_dict,t_vec=t_cpu)\n",
    "batched_t = to_cuda(noised_dict['t_vec'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b615080",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nwoodall/miniconda3/envs/se33/lib/python3.9/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
      "/home/nwoodall/miniconda3/envs/se33/lib/python3.9/site-packages/dgl/backend/pytorch/tensor.py:449: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(3221.5481, device='cuda:0') tensor(2649.7085) tensor(443.5340) tensor(128.3047)\n",
      "1 tensor(721.7962, device='cuda:0') tensor(187.8804) tensor(426.0406) tensor(107.8751)\n",
      "2 tensor(524.7708, device='cuda:0') tensor(6.3072) tensor(419.9707) tensor(98.4929)\n",
      "3 tensor(509.0513, device='cuda:0') tensor(4.0618) tensor(413.1630) tensor(91.8263)\n",
      "4 tensor(499.3692, device='cuda:0') tensor(1.8367) tensor(408.9172) tensor(88.6152)\n",
      "5 tensor(495.5051, device='cuda:0') tensor(0.8494) tensor(407.3964) tensor(87.2594)\n",
      "6 tensor(496.9202, device='cuda:0') tensor(1.4569) tensor(408.6588) tensor(86.8044)\n",
      "7 tensor(493.8044, device='cuda:0') tensor(0.7936) tensor(407.4102) tensor(85.6007)\n",
      "8 tensor(491.7913, device='cuda:0') tensor(1.2068) tensor(406.0603) tensor(84.5241)\n",
      "9 tensor(488.5466, device='cuda:0') tensor(1.5562) tensor(403.9198) tensor(83.0707)\n",
      "10 tensor(484.4487, device='cuda:0') tensor(0.5390) tensor(403.2652) tensor(80.6445)\n",
      "11 tensor(481.1455, device='cuda:0') tensor(0.4078) tensor(401.9539) tensor(78.7837)\n",
      "12 tensor(478.2026, device='cuda:0') tensor(0.2079) tensor(400.9020) tensor(77.0928)\n",
      "13 tensor(474.8612, device='cuda:0') tensor(0.2196) tensor(400.6619) tensor(73.9797)\n",
      "14 tensor(473.4901, device='cuda:0') tensor(0.8739) tensor(399.3948) tensor(73.2215)\n",
      "15 tensor(468.6091, device='cuda:0') tensor(0.2750) tensor(397.2854) tensor(71.0488)\n",
      "16 tensor(467.0658, device='cuda:0') tensor(0.1887) tensor(397.2616) tensor(69.6156)\n",
      "17 tensor(464.7002, device='cuda:0') tensor(0.1762) tensor(396.1198) tensor(68.4043)\n",
      "18 tensor(462.8651, device='cuda:0') tensor(0.1286) tensor(396.0864) tensor(66.6500)\n",
      "19 tensor(459.7138, device='cuda:0') tensor(0.1261) tensor(394.3859) tensor(65.2017)\n",
      "20 tensor(460.9500, device='cuda:0') tensor(2.4028) tensor(394.8625) tensor(63.6847)\n",
      "21 tensor(455.2230, device='cuda:0') tensor(0.4518) tensor(390.8613) tensor(63.9100)\n",
      "22 tensor(450.2308, device='cuda:0') tensor(0.2255) tensor(388.4796) tensor(61.5258)\n",
      "23 tensor(450.7272, device='cuda:0') tensor(2.1631) tensor(386.9889) tensor(61.5751)\n",
      "24 tensor(441.6721, device='cuda:0') tensor(0.2580) tensor(381.2077) tensor(60.2064)\n",
      "25 tensor(438.3170, device='cuda:0') tensor(0.0759) tensor(378.9686) tensor(59.2725)\n",
      "26 tensor(537.4526, device='cuda:0') tensor(57.8400) tensor(399.1910) tensor(80.4215)\n",
      "27 tensor(486.5722, device='cuda:0') tensor(1.7096) tensor(402.2144) tensor(82.6480)\n",
      "28 tensor(475.6711, device='cuda:0') tensor(1.0660) tensor(399.8142) tensor(74.7909)\n",
      "29 tensor(469.0255, device='cuda:0') tensor(1.4766) tensor(396.2985) tensor(71.2503)\n",
      "30 tensor(464.1310, device='cuda:0') tensor(0.0390) tensor(395.4902) tensor(68.6020)\n",
      "31 tensor(461.4666, device='cuda:0') tensor(0.4446) tensor(395.1832) tensor(65.8387)\n",
      "32 tensor(457.6926, device='cuda:0') tensor(0.1059) tensor(393.2300) tensor(64.3567)\n",
      "33 tensor(458.2358, device='cuda:0') tensor(0.0622) tensor(392.7801) tensor(65.3935)\n",
      "34 tensor(453.8103, device='cuda:0') tensor(0.0083) tensor(391.5746) tensor(62.2274)\n",
      "35 tensor(453.1089, device='cuda:0') tensor(0.0334) tensor(390.2734) tensor(62.8024)\n",
      "36 tensor(449.2149, device='cuda:0') tensor(0.1153) tensor(388.7463) tensor(60.3533)\n",
      "37 tensor(448.2466, device='cuda:0') tensor(0.0767) tensor(387.2961) tensor(60.8739)\n",
      "38 tensor(445.4190, device='cuda:0') tensor(0.0535) tensor(385.8611) tensor(59.5045)\n",
      "39 tensor(440.4971, device='cuda:0') tensor(0.0025) tensor(382.0197) tensor(58.4749)\n",
      "40 tensor(435.1274, device='cuda:0') tensor(0.0033) tensor(377.9801) tensor(57.1440)\n",
      "41 tensor(430.5036, device='cuda:0') tensor(0.0040) tensor(374.7790) tensor(55.7205)\n",
      "42 tensor(427.2604, device='cuda:0') tensor(0.0032) tensor(371.5925) tensor(55.6646)\n",
      "43 tensor(423.3967, device='cuda:0') tensor(0.1755) tensor(369.4853) tensor(53.7358)\n",
      "44 tensor(419.2504, device='cuda:0') tensor(0.1910) tensor(366.3576) tensor(52.7019)\n",
      "45 tensor(416.0633, device='cuda:0') tensor(0.0026) tensor(364.3643) tensor(51.6964)\n",
      "46 tensor(410.9224, device='cuda:0') tensor(0.0026) tensor(360.4525) tensor(50.4673)\n",
      "47 tensor(434.8892, device='cuda:0') tensor(0.0041) tensor(374.6582) tensor(60.2270)\n",
      "48 tensor(439.4052, device='cuda:0') tensor(0.0061) tensor(378.2735) tensor(61.1257)\n",
      "49 tensor(429.1835, device='cuda:0') tensor(0.0038) tensor(372.5628) tensor(56.6169)\n",
      "50 tensor(425.2114, device='cuda:0') tensor(0.0041) tensor(369.6381) tensor(55.5692)\n",
      "51 tensor(419.3882, device='cuda:0') tensor(0.0030) tensor(365.3520) tensor(54.0332)\n",
      "52 tensor(416.2638, device='cuda:0') tensor(0.0031) tensor(363.6121) tensor(52.6486)\n",
      "53 tensor(413.5621, device='cuda:0') tensor(0.0028) tensor(362.1032) tensor(51.4561)\n",
      "54 tensor(410.8506, device='cuda:0') tensor(0.0018) tensor(359.9374) tensor(50.9116)\n",
      "55 tensor(409.0936, device='cuda:0') tensor(0.0018) tensor(357.7891) tensor(51.3026)\n",
      "56 tensor(404.7655, device='cuda:0') tensor(0.0017) tensor(355.8239) tensor(48.9401)\n",
      "57 tensor(403.5749, device='cuda:0') tensor(0.0143) tensor(354.6316) tensor(48.9290)\n",
      "58 tensor(403.5695, device='cuda:0') tensor(0.3414) tensor(355.1867) tensor(48.0413)\n",
      "59 tensor(402.5637, device='cuda:0') tensor(0.0143) tensor(354.0202) tensor(48.5291)\n",
      "60 tensor(401.1631, device='cuda:0') tensor(0.2449) tensor(353.6154) tensor(47.3030)\n",
      "61 tensor(399.3080, device='cuda:0') tensor(0.0009) tensor(352.7016) tensor(46.6053)\n",
      "62 tensor(398.5374, device='cuda:0') tensor(0.0012) tensor(350.9989) tensor(47.5372)\n",
      "63 tensor(396.4164, device='cuda:0') tensor(0.0002) tensor(349.6059) tensor(46.8103)\n",
      "64 tensor(395.2093, device='cuda:0') tensor(0.0003) tensor(347.7632) tensor(47.4457)\n",
      "65 tensor(393.5380, device='cuda:0') tensor(0.0003) tensor(347.6620) tensor(45.8757)\n",
      "66 tensor(393.0102, device='cuda:0') tensor(0.0003) tensor(347.1361) tensor(45.8738)\n",
      "67 tensor(390.4869, device='cuda:0') tensor(0.0008) tensor(345.3205) tensor(45.1658)\n",
      "68 tensor(390.7988, device='cuda:0') tensor(0.0003) tensor(345.2515) tensor(45.5470)\n",
      "69 tensor(389.1422, device='cuda:0') tensor(0.0002) tensor(344.3288) tensor(44.8130)\n",
      "70 tensor(386.8400, device='cuda:0') tensor(0.0002) tensor(342.0103) tensor(44.8295)\n",
      "71 tensor(385.0645, device='cuda:0') tensor(0.0007) tensor(339.5315) tensor(45.5323)\n",
      "72 tensor(379.3577, device='cuda:0') tensor(0.0004) tensor(335.0187) tensor(44.3386)\n",
      "73 tensor(375.7636, device='cuda:0') tensor(0.0007) tensor(330.6038) tensor(45.1592)\n",
      "74 tensor(376.6469, device='cuda:0') tensor(0.0002) tensor(331.6874) tensor(44.9592)\n",
      "75 tensor(373.7442, device='cuda:0') tensor(0.0001) tensor(329.0865) tensor(44.6577)\n",
      "76 tensor(373.9803, device='cuda:0') tensor(0.0001) tensor(329.8415) tensor(44.1387)\n",
      "77 tensor(370.4177, device='cuda:0') tensor(0.3239) tensor(326.2426) tensor(43.8513)\n",
      "78 tensor(365.5276, device='cuda:0') tensor(0.0001) tensor(322.7634) tensor(42.7641)\n",
      "79 tensor(364.3218, device='cuda:0') tensor(0.0001) tensor(321.3036) tensor(43.0182)\n",
      "80 tensor(363.8008, device='cuda:0') tensor(9.0403e-05) tensor(320.7256) tensor(43.0749)\n",
      "81 tensor(361.2311, device='cuda:0') tensor(8.6470e-05) tensor(317.9082) tensor(43.3227)\n",
      "82 tensor(366.3040, device='cuda:0') tensor(1.1817) tensor(320.8825) tensor(44.2399)\n",
      "83 tensor(364.5942, device='cuda:0') tensor(0.0051) tensor(320.7852) tensor(43.8039)\n",
      "84 tensor(360.9010, device='cuda:0') tensor(0.1629) tensor(318.2293) tensor(42.5089)\n",
      "85 tensor(360.3383, device='cuda:0') tensor(0.0007) tensor(317.5934) tensor(42.7442)\n",
      "86 tensor(358.7050, device='cuda:0') tensor(0.0005) tensor(316.7149) tensor(41.9896)\n",
      "87 tensor(357.5758, device='cuda:0') tensor(0.0004) tensor(315.3867) tensor(42.1886)\n",
      "88 tensor(355.6270, device='cuda:0') tensor(0.0003) tensor(313.3973) tensor(42.2292)\n",
      "89 tensor(355.4436, device='cuda:0') tensor(0.0003) tensor(312.8431) tensor(42.6002)\n",
      "90 tensor(353.3784, device='cuda:0') tensor(0.0002) tensor(311.9073) tensor(41.4709)\n",
      "91 tensor(352.8914, device='cuda:0') tensor(0.0004) tensor(310.2683) tensor(42.6228)\n",
      "92 tensor(350.3728, device='cuda:0') tensor(0.0002) tensor(309.6812) tensor(40.6916)\n",
      "93 tensor(348.7525, device='cuda:0') tensor(0.0004) tensor(307.4651) tensor(41.2870)\n",
      "94 tensor(348.5085, device='cuda:0') tensor(0.0003) tensor(307.4258) tensor(41.0823)\n",
      "95 tensor(348.5995, device='cuda:0') tensor(0.0001) tensor(308.1937) tensor(40.4057)\n",
      "96 tensor(348.4612, device='cuda:0') tensor(0.0002) tensor(306.4120) tensor(42.0491)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97 tensor(347.9015, device='cuda:0') tensor(0.0001) tensor(307.3578) tensor(40.5437)\n",
      "98 tensor(347.9319, device='cuda:0') tensor(0.0002) tensor(306.9611) tensor(40.9706)\n",
      "99 tensor(345.1673, device='cuda:0') tensor(0.0002) tensor(305.2675) tensor(39.8996)\n"
     ]
    }
   ],
   "source": [
    "t=0.02\n",
    "for e in range(100):\n",
    "    pnf_Score=0\n",
    "    e_score = 0\n",
    "    r_score = 0\n",
    "    l_score = 0\n",
    "    for i, bb_dict in enumerate(train_dL):\n",
    "        t_cpu = np.ones((B,))*t\n",
    "        \n",
    "        noised_dict = fnd.forward(bb_dict,t_vec=t_cpu)\n",
    "        batched_t = to_cuda(noised_dict['t_vec'])\n",
    "\n",
    "\n",
    "        #final_loss, pred_nf_loss, loss_3D = model_step_null(noised_dict,batched_t, mkg, gunn, train=True)\n",
    "        final_loss, pnfloss, structure_loss, real_loss, null_loss = model_step_null(noised_dict,batched_t, mkg, gunn, train=True)\n",
    "        \n",
    "        opti.zero_grad()\n",
    "        final_loss.backward()\n",
    "        opti.step()\n",
    "        fl = final_loss.detach()\n",
    "        e_score += fl\n",
    "        pnf_Score += pnfloss\n",
    "        r_score += real_loss\n",
    "        l_score += null_loss\n",
    "    #print(e,e_score,pred_nf_loss.sum(), loss_3D.sum())\n",
    "    if e%5==1:\n",
    "        true, noise_xyz, pred , real_nodes_pred_mask, real_nodes_true_mask = get_noise_pred_true_null(noised_dict, batched_t, mkg, gunn)\n",
    "        dump_tnp_null(true, noise_xyz, pred, t_cpu,e=e, numOut=1, real_mask=real_nodes_true_mask, pred_mask=None, outdir='output/')\n",
    "        \n",
    "    print(e,e_score,pnf_Score,r_score,l_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0873f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, bb_dict in enumerate(train_dL):\n",
    "    t_cpu = np.ones((B,))*0.05\n",
    "    batched_t = to_cuda(noised_dict['t_vec'])\n",
    "    noised_dict = fnd.forward(bb_dict,t_vec=t_cpu)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6291b623",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_dict = mkg.prep_for_network(noised_dict)\n",
    "out = gunn(feat_dict, batched_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333b5d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "noised_dict['real_nodes_noise'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b3613b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_loss, pred_nf_loss, loss_3D = model_step_null(noised_dict, batched_t, mkg, gunn, train=True)\n",
    "#final_loss, pred_nf_loss, loss_3D = model_step_null(noised_dict, batched_t, mkg, gunn, train=True)\n",
    "true, noise_xyz, pred , real_nodes_pred_mask, real_nodes_true_mask = get_noise_pred_true_null(noised_dict, batched_t, mkg, gunn)\n",
    "dump_tnp_null(true, noise_xyz, pred, t_cpu, numOut=1, real_mask=real_nodes_true_mask, pred_mask=None, outdir='output/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c515ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc1824c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c89d1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device='cuda'\n",
    "\n",
    "# B = 8\n",
    "# L= 128\n",
    "# limit = 1028\n",
    "# prot_trainData = Data_Graph.ProteinBB_Dataset(coords_tog[:limit], n_nodes=L,\n",
    "#               n_atoms=5, coord_div=10, cast_type=torch.float32)\n",
    "# train_dL = DataLoader(prot_trainData, batch_size=B, shuffle=True, drop_last=True)\n",
    "# stride=4##########\n",
    "# mkg = Make_nullKNN_MP_Graphs(KNN=30, mp_stride=stride, n_nodes=L)\n",
    "\n",
    "# score_weights = {}\n",
    "# score_weights['nf_real'] = torch.tensor(0.2,device=device)\n",
    "# score_weights['3D_real'] = torch.tensor(1.0,device=device)\n",
    "# score_weights['3D_null'] = torch.tensor(1.0,device=device)\n",
    "\n",
    "# config_path='data_rigid_diffuser/base.yaml'\n",
    "# fnd = FrameDiffNoise(config_path)\n",
    "\n",
    "# gunn= GraphUNet_Null(fiber_start = Fiber({0:17, 1:2}),\n",
    "#                      fiber_out = Fiber({0:5,1:2}),\n",
    "#                       k=4,\n",
    "#                       batch_size = B,\n",
    "#                       stride=stride,\n",
    "#                        max_degree=3,\n",
    "#                        channels=64,\n",
    "#                       num_heads = 16,\n",
    "#                       channels_div=8,\n",
    "#                       num_layers = 1,\n",
    "#                      num_layers_ca = 2,\n",
    "#                      edge_feature_dim=1,\n",
    "#                      latent_pool_type = 'max',\n",
    "#                      t_size = 12,\n",
    "#                      mult=2,\n",
    "#                     zero_lin=True,\n",
    "#                    use_tdeg1 = False,\n",
    "#                  cuda=True).to('cuda')\n",
    "\n",
    "# opti = torch.optim.Adam(gunn.parameters(), lr=0.0005, weight_decay=5e-6)\n",
    "# #opti =torch.optim.Adadelta(gunn.parameters(), lr=0.0005, weight_decay=5e-7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc000156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testiter = iter(train_dL)\n",
    "# bb_dict = next(testiter)\n",
    "\n",
    "# t_cpu = np.ones((B,))*0.05\n",
    "# noised_dict = fnd.forward(bb_dict,t_vec=t_cpu)\n",
    "# batched_t = to_cuda(noised_dict['t_vec'])\n",
    "# CA_t  = noised_dict['bb_shifted']['CA'].reshape(B, L, 3).to(device)\n",
    "# NC_t = CA_t + noised_dict['bb_shifted']['N_CA'].reshape(B, L, 3).to(device)#not mult by bond distance, seems to help?\n",
    "# CC_t = CA_t + noised_dict['bb_shifted']['C_CA'].reshape(B, L, 3).to(device)#not mult \n",
    "# true =  torch.cat((NC_t,CA_t,CC_t),dim=2).reshape(B,L,3,3)\n",
    "\n",
    "# CA_n  = noised_dict['bb_noised']['CA'].reshape(B, L, 3).to(device)\n",
    "# NC_n = CA_n + noised_dict['bb_noised']['N_CA'].reshape(B, L, 3).to(device)\n",
    "# CC_n = CA_n + noised_dict['bb_noised']['C_CA'].reshape(B, L, 3).to(device)\n",
    "# noise_xyz =  torch.cat((NC_n,CA_n,CC_n),dim=2).reshape(B,L,3,3)\n",
    "\n",
    "# #prepare graphs\n",
    "# feat_dict = mkg.prep_for_network(noised_dict, cuda=True)\n",
    "# out =gunn(feat_dict,batched_t)\n",
    "\n",
    "\n",
    "# #FAPE Loss for the prediction\n",
    "# CA_p = out['1'][:,0,:].reshape(B, L, 3)+CA_n #translation of Calpha\n",
    "# Qs = out['1'][:,1,:] # rotation , convert from x,y,z (Quat) to rotate input vectors\n",
    "# Qs = Qs.unsqueeze(1).repeat((1,2,1))\n",
    "# Qs = torch.cat((torch.ones((B*L,2,1),device=Qs.device),Qs),dim=-1).reshape(B,L,2,4)\n",
    "# Qs = normQ(Qs)\n",
    "# Rs = Qs2Rs(Qs)\n",
    "# N_C_to_Rot = torch.cat((noised_dict['bb_noised']['N_CA'].reshape(B, L, 3).to(device),\n",
    "#                         noised_dict['bb_noised']['C_CA'].reshape(B, L, 3).to(device)),dim=2).reshape(B,L,2,1,3)\n",
    "# rot_vecs = einsum('bnkij,bnkhj->bnki',Rs, N_C_to_Rot)\n",
    "# NC_p = CA_p + rot_vecs[:,:,0,:]*N_CA_dist #comparable but seems better not have it for true, but have it for pred\n",
    "# CC_p = CA_p + rot_vecs[:,:,1,:]*C_CA_dist #maybe this helep prevent \n",
    "\n",
    "# pred = torch.cat((NC_p,CA_p,CC_p),dim=2).reshape(B,L,3,3)\n",
    "\n",
    "# #divide loss by real and null nodes\n",
    "\n",
    "# fp, lp  = convert_pV_to_points(noised_dict)\n",
    "\n",
    "# real_mask = noised_dict['real_nodes_mask'].to('cuda')\n",
    "# score_scales = noised_dict['score_scales'].to('cuda')\n",
    "\n",
    "# lr, lr_d = FAPE_loss_real(pred, true, score_scales, real_mask,  d_clamp=10.0, d_clamp_inter=30.0,\n",
    "#                A=10.0, gamma=1.0, eps=1e-6)\n",
    "# ln, ln_d = FAPE_loss_null(pred, fp, lp, real_mask, true, score_scales,  d_clamp=10.0,\n",
    "#                    d_clamp_inter=30.0, A=10.0, gamma=0.1, eps=1e-6)\n",
    "# ld, ln_d = FAPE_loss(pred.unsqueeze(0), true,score_scales,  d_clamp=10.0,\n",
    "#                    d_clamp_inter=30.0, A=10.0, gamma=1.0, eps=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3586095",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "se33",
   "language": "python",
   "name": "se33"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
