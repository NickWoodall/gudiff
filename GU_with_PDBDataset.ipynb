{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3de2584b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"PDB dataset loader.\"\"\"\n",
    "# import math\n",
    "# from typing import Optional\n",
    "\n",
    "# import torch\n",
    "# import torch.distributed as dist\n",
    "\n",
    "# import tree\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import pandas as pd\n",
    "# import logging\n",
    "# import random\n",
    "# import functools as fn\n",
    "\n",
    "# from torch.utils import data\n",
    "# from data import utils as du\n",
    "# from openfold.data import data_transforms\n",
    "# from openfold.np import residue_constants\n",
    "# from openfold.utils import rigid_utils\n",
    "\n",
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "# from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d159d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28b5ad6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openfold.np import residue_constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c54836d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "711cb211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import tree\n",
    "#clear memory better\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "import numpy as np\n",
    "import util.npose_util as nu\n",
    "import os\n",
    "import pathlib\n",
    "import dgl\n",
    "from dgl import backend as F\n",
    "import torch_geometric\n",
    "from torch.utils.data import random_split, DataLoader, Dataset\n",
    "from typing import Dict\n",
    "from torch import Tensor\n",
    "from dgl import DGLGraph\n",
    "from torch import nn\n",
    "# from chemical import cos_ideal_NCAC #from RoseTTAFold2\n",
    "from torch import einsum\n",
    "import time\n",
    "import random\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09df63cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import se3_diffuse.utils as du\n",
    "from data_rigid_diffuser.diffuser import FrameDiffNoise\n",
    "from gudiff_model import Data_Graph\n",
    "from gudiff_model.Data_Graph import build_npose_from_coords, dump_coord_pdb, define_graph_edges, make_pe_encoding\n",
    "from gudiff_model.Data_Graph import Helix4_Dataset, Make_KNN_MP_Graphs\n",
    "from se3_transformer.model.basis import get_basis, update_basis_with_fused\n",
    "from se3_transformer.model.transformer import Sequential, SE3Transformer\n",
    "from se3_transformer.model.transformer_topk import SE3Transformer_topK\n",
    "from se3_transformer.model.FAPE_Loss import FAPE_loss, Qs2Rs, normQ\n",
    "from se3_transformer.model.layers.attentiontopK import AttentionBlockSE3\n",
    "from se3_transformer.model.layers.linear import LinearSE3\n",
    "from se3_transformer.model.layers.convolution import ConvSE3, ConvSE3FuseLevel\n",
    "from se3_transformer.model.layers.norm import NormSE3\n",
    "from se3_transformer.model.layers.pooling import GPooling, Latent_Unpool, Unpool_Layer\n",
    "from se3_transformer.runtime.utils import str2bool, to_cuda\n",
    "from se3_transformer.model.fiber import Fiber\n",
    "from se3_transformer.model.transformer import get_populated_edge_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dffe08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#indices for, unsure if needed\n",
    "CA = Data_Graph.CA\n",
    "N = Data_Graph.N\n",
    "C = Data_Graph.C\n",
    "\n",
    "#find better way to incorporate coord_scale\n",
    "\n",
    "#needed\n",
    "N_CA_dist = (Data_Graph.N_CA_dist/10.).to('cuda')\n",
    "C_CA_dist = (Data_Graph.C_CA_dist/10.).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8ed61fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#basically a I just need data loader to grab coordinates from datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9da631a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########CURRENTLY REWRITING THIS######\n",
    "\n",
    "def torch_normalize(v, eps=1e-6):\n",
    "    \"\"\"Normalize vector in last axis\"\"\"\n",
    "    norm = torch.linalg.vector_norm(v, dim=len(v.shape)-1)+eps\n",
    "    return v / norm[...,None]\n",
    "def gudiff_parse_chain_feats(chain_feats, scale_factor=10., cast_type=torch.float32):\n",
    "    ca_idx = residue_constants.atom_order['CA']\n",
    "    n_idx = residue_constants.atom_order['N']\n",
    "    c_idx = residue_constants.atom_order['C']\n",
    "    chain_feats['bb_mask'] = chain_feats['atom_mask'][:, ca_idx]\n",
    "    \n",
    "    bb_pos = chain_feats['atom_positions'][:, ca_idx]/scale_factor #scale factor mod\n",
    "    bb_center = np.sum(bb_pos, axis=0) / (np.sum(chain_feats['bb_mask']) + 1e-5)\n",
    "    centered_pos = chain_feats['atom_positions'] - bb_center[None, None, :]\n",
    "    \n",
    "    \n",
    "    coordinates = centered_pos/scale_factor\n",
    "    #unsqueeze to stack together later\n",
    "    N_CA_vec = torch.tensor(coordinates[:,n_idx] - coordinates[:,ca_idx], dtype=cast_type)/scale_factor\n",
    "    C_CA_vec = torch.tensor(coordinates[:,c_idx] - coordinates[:,ca_idx], dtype=cast_type)/scale_factor\n",
    "        \n",
    "    N_CA_vec = torch_normalize(N_CA_vec)#.unsqueeze(2) #do the unsqueeze later\n",
    "    C_CA_vec = torch_normalize(C_CA_vec)#.unsqueeze(2)\n",
    "\n",
    "    scaled_pos = centered_pos / scale_factor\n",
    "    chain_feats['atom_positions'] = scaled_pos * chain_feats['atom_mask'][..., None]\n",
    "    \n",
    "    chain_feats['CA'] = torch.tensor(coordinates[:,ca_idx],dtype=cast_type)\n",
    "    chain_feats['N_CA_vec'] = N_CA_vec\n",
    "    chain_feats['C_CA_vec'] = C_CA_vec\n",
    "    return chain_feats\n",
    "\n",
    "\n",
    "\n",
    "class smallPDBDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            diffuser,\n",
    "            meta_data_path = '/mnt/h/datasets/p200/metadata.csv',\n",
    "            filter_dict=True,\n",
    "            maxlen=None,\n",
    "            is_training=True,\n",
    "            input_t=None\n",
    "        ):\n",
    "        #self._log = logging.getLogger(__name__)\n",
    "        self._is_training = is_training\n",
    "        self.meta_data_path = meta_data_path\n",
    "        self._init_metadata(filter_dict=filter_dict,maxlen=maxlen) #includes create split that saves self.csv\n",
    "        self._diffuser = diffuser\n",
    "        self.input_t = input_t\n",
    "        \n",
    "    @property\n",
    "    def is_training(self):\n",
    "        return self._is_training\n",
    "\n",
    "    @property\n",
    "    def diffuser(self):\n",
    "        return self._diffuser\n",
    "\n",
    "    @property\n",
    "    def data_conf(self):\n",
    "        return self._data_conf\n",
    "\n",
    "    def _init_metadata(self, filter_dict=True, maxlen=None):\n",
    "        \"\"\"Initialize metadata.\"\"\"\n",
    "        \n",
    "        #meta_data_path = '/mnt/h/datasets/p200/metadata.csv'\n",
    "        pdb_csv = pd.read_csv(self.meta_data_path)\n",
    "        \n",
    "        if filter_dict:\n",
    "            filter_conf = {'allowed_oligomer': ['monomeric'],\n",
    "                           'max_loop_percent': 0.75}\n",
    "            pdb_csv = pdb_csv[pdb_csv.oligomeric_detail.isin(filter_conf['allowed_oligomer'])]\n",
    "            pdb_csv = pdb_csv[pdb_csv.coil_percent < filter_conf['max_loop_percent']]\n",
    "            pdb_csv = pdb_csv.sort_values('modeled_seq_len', ascending=False)\n",
    "            \n",
    "        if maxlen is not None:\n",
    "            pdb_csv = pdb_csv[:maxlen]\n",
    "        #self._create_split(pdb_csv)\n",
    "        self.csv = pdb_csv\n",
    "    def _create_split(self, pdb_csv):\n",
    "        # Training or validation specific logic.\n",
    "        #if self.is_training:\n",
    "        self.csv = pdb_csv\n",
    "        #self._log.info(\n",
    "        #    f'Training: {len(self.csv)} examples')\n",
    "#         else:\n",
    "#             all_lengths = np.sort(pdb_csv.modeled_seq_len.unique())\n",
    "#             length_indices = (len(all_lengths) - 1) * np.linspace(\n",
    "#                 0.0, 1.0, self._data_conf.num_eval_lengths)\n",
    "#             length_indices = length_indices.astype(int)\n",
    "            \n",
    "#             if self._simple:\n",
    "#                 eval_lengths = np.array([65]).astype(int)\n",
    "#             else:\n",
    "#                 eval_lengths = all_lengths[length_indices]\n",
    "                \n",
    "#             eval_csv = pdb_csv[pdb_csv.modeled_seq_len.isin(eval_lengths)]\n",
    "#             # Fix a random seed to get the same split each time.\n",
    "#             eval_csv = eval_csv.groupby('modeled_seq_len').sample(\n",
    "#                 self._data_conf.samples_per_eval_length, replace=True, random_state=123)\n",
    "#             eval_csv = eval_csv.sort_values('modeled_seq_len', ascending=False)\n",
    "#             self.csv = eval_csv\n",
    "#             self._log.info(\n",
    "#                 f'Validation: {len(self.csv)} examples with lengths {eval_lengths}')\n",
    "    # cache make the same sample in same batch \n",
    "    #@fn.lru_cache(maxsize=100)\n",
    "    def _process_csv_row(self, processed_file_path, index=0):\n",
    "        \n",
    "        processed_feats = du.read_pkl(processed_file_path)\n",
    "        chain_feats = gudiff_parse_chain_feats(processed_feats,scale_factor=10.)\n",
    "        \n",
    "        # Only take modeled residues.\n",
    "        modeled_idx = processed_feats['modeled_idx']\n",
    "        min_idx = np.min(modeled_idx)\n",
    "        max_idx = np.max(modeled_idx)\n",
    "        del processed_feats['modeled_idx']\n",
    "        processed_feats = tree.map_structure(\n",
    "            lambda x: x[min_idx:(max_idx+1)], processed_feats)\n",
    "        chain_feats = tree.map_structure(\n",
    "            lambda x: x[min_idx:(max_idx+1)], chain_feats)\n",
    "        \n",
    "\n",
    "        # Run through OpenFold data transforms.\n",
    "        # Re-number residue indices for each chain such that it starts from 1.\n",
    "        # Randomize chain indices.\n",
    "        chain_idx = processed_feats[\"chain_index\"]\n",
    "        res_idx = processed_feats['residue_index']\n",
    "        new_res_idx = np.zeros_like(res_idx)\n",
    "        new_chain_idx = np.zeros_like(res_idx)\n",
    "        all_chain_idx = np.unique(chain_idx).tolist()\n",
    "        shuffled_chain_idx = np.array(\n",
    "            random.sample(all_chain_idx, len(all_chain_idx))) - np.min(all_chain_idx) + 1\n",
    "        for i,chain_id in enumerate(all_chain_idx):\n",
    "            chain_mask = (chain_idx == chain_id).astype(int)\n",
    "            chain_min_idx = np.min(res_idx + (1 - chain_mask) * 1e3).astype(int)\n",
    "            new_res_idx = new_res_idx + (res_idx - chain_min_idx + 1) * chain_mask\n",
    "\n",
    "            # Shuffle chain_index\n",
    "            replacement_chain_id = shuffled_chain_idx[i]\n",
    "            new_chain_idx = new_chain_idx + replacement_chain_id * chain_mask\n",
    "\n",
    "        # To speed up processing, only take necessary features\n",
    "        final_feats = {\n",
    "            'chain_idx': new_chain_idx,\n",
    "            'residue_index': processed_feats['residue_index'],\n",
    "            'res_mask': processed_feats['bb_mask'],\n",
    "            'CA':   chain_feats['CA'],\n",
    "            'N_CA': chain_feats['N_CA_vec'], #when unsqueeze? later maybe take time to change this behavior\n",
    "            'C_CA': chain_feats['C_CA_vec'],\n",
    "            'file_path_index' : index\n",
    "        }\n",
    "        return final_feats\n",
    "    \n",
    "    def get_specific_t(self, idx, t):\n",
    "        example_idx = idx\n",
    "        csv_row = self.csv.iloc[example_idx]\n",
    "        if 'pdb_name' in csv_row:\n",
    "            pdb_name = csv_row['pdb_name']\n",
    "        elif 'chain_name' in csv_row:\n",
    "            pdb_name = csv_row['chain_name']\n",
    "        else:\n",
    "            raise ValueError('Need chain identifier.')\n",
    "            \n",
    "        processed_file_path = csv_row['processed_path']\n",
    "        chain_feats = self._process_csv_row(processed_file_path,index=idx)\n",
    "\n",
    "        bb_noised =  self._diffuser.forward(chain_feats, t=t)\n",
    "        chain_feats.update(bb_noised)\n",
    "\n",
    "        # Convert all features to tensors.\n",
    "        final_feats = tree.map_structure(\n",
    "                    lambda x: x if torch.is_tensor(x) else torch.tensor(x), chain_feats)\n",
    "        \n",
    "        return final_feats\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        example_idx = idx\n",
    "        csv_row = self.csv.iloc[example_idx]\n",
    "        if 'pdb_name' in csv_row:\n",
    "            pdb_name = csv_row['pdb_name']\n",
    "        elif 'chain_name' in csv_row:\n",
    "            pdb_name = csv_row['chain_name']\n",
    "        else:\n",
    "            raise ValueError('Need chain identifier.')\n",
    "        processed_file_path = csv_row['processed_path']\n",
    "        chain_feats = self._process_csv_row(processed_file_path, index=idx)\n",
    "\n",
    "        # Use a fixed seed for evaluation.\n",
    "#         if self.is_training:\n",
    "        rng = np.random.default_rng(None)\n",
    "#         else:\n",
    "#             rng = np.random.default_rng(idx)\n",
    "\n",
    "        # Sample t and diffuse.\n",
    "#         if self.is_training:\n",
    "        if self.input_t is None:\n",
    "            t = rng.uniform(1e-3, 1.0)\n",
    "        else:\n",
    "            t = self.input_t\n",
    "        bb_noised =  self._diffuser.forward(chain_feats, t=t)\n",
    "#         else:\n",
    "#             t = 1.0\n",
    "#             diff_feats_t = self.diffuser.sample_ref(\n",
    "#                 n_samples=gt_bb_rigid.shape[0],\n",
    "#                 impute=gt_bb_rigid,\n",
    "#                 diffuse_mask=None,\n",
    "#                 as_tensor_7=True,\n",
    "#             )\n",
    "        chain_feats.update(bb_noised)\n",
    "\n",
    "        # Convert all features to tensors.\n",
    "        final_feats = tree.map_structure(\n",
    "            lambda x: x if torch.is_tensor(x) else torch.tensor(x), chain_feats)\n",
    "        #final_feats = du.pad_feats(final_feats, csv_row['modeled_seq_len'])\n",
    "        #if self.is_training:\n",
    "#         else:\n",
    "#             return final_feats, pdb_name\n",
    "        \n",
    "        return final_feats\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "387ae865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils import data\n",
    "\n",
    "class TrainSampler(torch.utils.data.Sampler):\n",
    "\n",
    "    def __init__(self, batch_size, dataset,\n",
    "                 sample_mode='length_batch'):\n",
    "        \n",
    "        self.dataset = dataset\n",
    "        self._data_csv = dataset.csv\n",
    "        self._dataset_indices = list(range(len(self._data_csv)))\n",
    "        self._data_csv['index'] = self._dataset_indices\n",
    "        self._batch_size = batch_size\n",
    "        self.epoch = 0\n",
    "        self._sample_mode = sample_mode\n",
    "        self.sampler_len = len(self._dataset_indices) * self._batch_size\n",
    "        self.min_t = 1e-3\n",
    "        #self._log = logging.getLogger(__name__)\n",
    "        #self._data_conf = data_conf\n",
    "        #self._dataset = dataset\n",
    "        #self._data_csv = self._dataset.csv\n",
    "    def __iter__(self):\n",
    "        if self._sample_mode == 'length_batch':\n",
    "            # Each batch contains multiple proteins of the same length.\n",
    "            sampled_order = self._data_csv.groupby('modeled_seq_len').sample(\n",
    "                self._batch_size, replace=True, random_state=self.epoch) #one batch per length\n",
    "            return iter(sampled_order['index'].tolist())\n",
    "        elif self._sample_mode == 'single_length':\n",
    "            rand_index = self._data_csv['index'].to_numpy()\n",
    "            np.random.shuffle(rand_index)\n",
    "            num_batches = int(rand_index.shape[0]/self._batch_size)\n",
    "            rand_index = rand_index[:(num_batches*self._batch_size)] #drop last batch\n",
    "            return iter(rand_index)\n",
    "        else:\n",
    "            raise ValueError(f'Invalid sample mode: {self._sample_mode}')\n",
    "    \n",
    "#     def getbb(self, idx):\n",
    "#         csv_row = self._data_csv.iloc[idx]\n",
    "#         processed_file_path = csv_row['processed_path']\n",
    "#         chain_feats = self.dataset._process_csv_row(processed_file_path)\n",
    "#         return chain_feats  \n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.csv)\n",
    "\n",
    "    def set_epoch(self, epoch):\n",
    "        self.epoch = epoch\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.sampler_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ccb8820",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gudiff_model.Graph_UNet import GraphUNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72647f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_step(batch_feats, noised_dict, graph_maker, graph_unet, train=True):\n",
    "    L = batch_feats['CA'].shape[1]\n",
    "    B = batch_feats['CA'].shape[0]\n",
    "    CA_t  = batch_feats['CA']\n",
    "    NC_t = CA_t +  batch_feats['N_CA']\n",
    "    CC_t = CA_t +  batch_feats['C_CA']\n",
    "    true =  torch.cat((NC_t,CA_t,CC_t),dim=2).reshape(B,L,3,3)\n",
    "\n",
    "    CA_n  = batch_feats['CA_noised'].reshape(B, L, 3)#.to('cuda')\n",
    "    NC_n = CA_n + batch_feats['N_CA_noised'].reshape(B, L, 3)#.to('cuda')\n",
    "    CC_n = CA_n + batch_feats['C_CA_noised'].reshape(B, L, 3)#.to('cuda')\n",
    "    noise_xyz =  torch.cat((NC_n,CA_n,CC_n),dim=2).reshape(B,L,3,3)\n",
    "    \n",
    "    x = graph_maker.prep_for_network(noised_dict)\n",
    "    out = graph_unet(x, batch_feats['t'])\n",
    "    CA_p = out['1'][:,0,:].reshape(B, L, 3) + CA_n #translation of Calpha\n",
    "    Qs = out['1'][:,1,:] # rotation of frame\n",
    "    Qs = Qs.unsqueeze(1).repeat((1,2,1))\n",
    "    Qs = torch.cat((torch.ones((B*L,2,1),device=Qs.device),Qs),dim=-1).reshape(B,L,2,4)\n",
    "    Qs = normQ(Qs)\n",
    "    Rs = Qs2Rs(Qs)\n",
    "    N_C_to_Rot = torch.cat((noised_dict['N_CA'].reshape(B, L, 3),\n",
    "                            noised_dict['C_CA'].reshape(B, L, 3)),dim=2).reshape(B,L,2,1,3)\n",
    "    \n",
    "    rot_vecs = einsum('bnkij,bnkhj->bnki',Rs, N_C_to_Rot)\n",
    "    NC_p = CA_p + rot_vecs[:,:,0,:]*N_CA_dist #remove bc who cares? me maybe\n",
    "    CC_p = CA_p + rot_vecs[:,:,1,:]*C_CA_dist #remove maybe\n",
    "\n",
    "    pred = torch.cat((NC_p,CA_p,CC_p),dim=2).reshape(B,L,3,3)\n",
    "\n",
    "    tloss, loss = FAPE_loss(pred.unsqueeze(0), true, batch_feats['score_scale'])\n",
    "    \n",
    "    return pred, tloss\n",
    "\n",
    "def get_noise_pred_true(batch_feats, noised_dict, graph_maker, graph_unet, scale=10):\n",
    "    \n",
    "    L = batch_feats['CA'].shape[1]\n",
    "    B = batch_feats['CA'].shape[0]\n",
    "    CA_t  = batch_feats['CA']\n",
    "    NC_t = CA_t +  batch_feats['N_CA']*N_CA_dist\n",
    "    CC_t = CA_t +  batch_feats['C_CA']*C_CA_dist\n",
    "    true =  torch.cat((NC_t,CA_t,CC_t),dim=2).reshape(B,L,3,3)\n",
    "\n",
    "    CA_n  = batch_feats['CA_noised'].reshape(B, L, 3)#.to('cuda')\n",
    "    NC_n = CA_n + batch_feats['N_CA_noised'].reshape(B, L, 3)*N_CA_dist\n",
    "    CC_n = CA_n + batch_feats['C_CA_noised'].reshape(B, L, 3)*C_CA_dist\n",
    "    noise_xyz =  torch.cat((NC_n,CA_n,CC_n),dim=2).reshape(B,L,3,3)\n",
    "    \n",
    "    x = graph_maker.prep_for_network(noised_dict)\n",
    "    out = graph_unet(x, batch_feats['t'])\n",
    "    CA_p = out['1'][:,0,:].reshape(B, L, 3) + CA_n #translation of Calpha\n",
    "    Qs = out['1'][:,1,:] # rotation of frame\n",
    "    Qs = Qs.unsqueeze(1).repeat((1,2,1))\n",
    "    Qs = torch.cat((torch.ones((B*L,2,1),device=Qs.device),Qs),dim=-1).reshape(B,L,2,4)\n",
    "    Qs = normQ(Qs)\n",
    "    Rs = Qs2Rs(Qs)\n",
    "    N_C_to_Rot = torch.cat((noised_dict['N_CA'].reshape(B, L, 3),\n",
    "                            noised_dict['C_CA'].reshape(B, L, 3)),dim=2).reshape(B,L,2,1,3)\n",
    "    \n",
    "    rot_vecs = einsum('bnkij,bnkhj->bnki',Rs, N_C_to_Rot)\n",
    "    NC_p = CA_p + rot_vecs[:,:,0,:]*N_CA_dist #remove bc who cares? me maybe\n",
    "    CC_p = CA_p + rot_vecs[:,:,1,:]*C_CA_dist #remove maybe\n",
    "\n",
    "    pred = torch.cat((NC_p,CA_p,CC_p),dim=2).reshape(B,L,3,3)\n",
    "    \n",
    "    return true.to('cpu').numpy()*scale, noise_xyz.to('cpu').numpy()*scale, pred.detach().to('cpu').numpy()*scale\n",
    "\n",
    "def generate_tbatch(pdbdataset, index_in, input_t):\n",
    "    batch_list = []\n",
    "    for i,t in enumerate(input_t):\n",
    "        batch_list.append(pdbdataset.get_specific_t(index_in[i], input_t[i]))\n",
    "\n",
    "    batch_feats = {}\n",
    "    for k in batch_list[0].keys():\n",
    "        batch_feats[k] = torch.stack([batch_list[i][k] for i in range(len(batch_list))])\n",
    "    return batch_feats\n",
    "\n",
    "\n",
    "def dump_tnp(true, noise, pred, t_val, e=0, numOut=1,outdir='output/'):\n",
    "    \n",
    "    if numOut>true.shape[0]:\n",
    "        numOut = true.shape[0]\n",
    "    \n",
    "    for x in range(numOut):\n",
    "        dump_coord_pdb(true[x], fileOut=f'{outdir}/true_{t_val[x]*100:.0f}_e{e}_{x}.pdb')\n",
    "        dump_coord_pdb(noise[x], fileOut=f'{outdir}/noise_{t_val[x]*100:.0f}_e{e}_{x}.pdb')\n",
    "        dump_coord_pdb(pred[x], fileOut=f'{outdir}/pred_{t_val[x]*100:.0f}_e{e}_{x}.pdb')\n",
    "        \n",
    "def visualize_model(pdbdataset, view_t, epoch=1, numOut=1, outdir='output/'):\n",
    "    device='cuda'\n",
    "    index_in = np.random.choice(np.arange(len(pdbdataset)), size=len(view_t))\n",
    "    batch_feats = generate_tbatch(pdbdataset , index_in, view_t)\n",
    "    \n",
    "    batch_feats= tree.map_structure(\n",
    "                    lambda x: x.to(device), batch_feats)\n",
    "    noised_dict =   {'CA': batch_feats['CA_noised'] ,\n",
    "                     'N_CA': batch_feats['N_CA_noised'].unsqueeze(-2) ,\n",
    "                     'C_CA': batch_feats['C_CA_noised'].unsqueeze(-2)  }\n",
    "    \n",
    "    true, noise, pred = get_noise_pred_true(batch_feats, noised_dict, gm, gu, scale=10)\n",
    "    dump_tnp(true,noise,pred, view_t, e=epoch, numOut=numOut, outdir=f'{outdir}/models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0452154b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_save_folder(name=''):\n",
    "    base_folder = time.strftime(f'log/%y%b%d_%I%M%p_{name}/', time.localtime())\n",
    "    if not os.path.exists(base_folder):\n",
    "        os.makedirs(base_folder)\n",
    "    subfolders = ['models']\n",
    "    for subfolder in subfolders:\n",
    "        if not os.path.exists(base_folder + subfolder):\n",
    "            os.makedirs(base_folder + subfolder)\n",
    "            \n",
    "    return base_folder\n",
    "        \n",
    "def save_chkpt(model_path, model, optimizer, epoch, batch, val_losses, train_losses):\n",
    "    \"\"\"Save a training checkpoint\n",
    "    Args:\n",
    "        model_path (str): the path to save the model to\n",
    "        model (nn.Module): the model to save\n",
    "        optimizer (torch.optim.Optimizer): the optimizer to save\n",
    "        epoch (int): the current epoch\n",
    "        batch (int): the current batch in the epoch\n",
    "        loss_domain (list of int): a list of the shared domain for val and training \n",
    "            losses\n",
    "        val_losses (list of float): a list containing the validation losses\n",
    "        train_losses (list of float): a list containing the training losses\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "    state_dict = dict()\n",
    "    state_dict.update({'model':model.state_dict(),\n",
    "                       'optimizer':optimizer.state_dict(),\n",
    "                       'epoch':epoch,\n",
    "                       'batch':batch,\n",
    "                       'train_losses':train_losses,\n",
    "                       'val_losses':val_losses\n",
    "                       })\n",
    "    torch.save(state_dict, f'{model_path}model_e{epoch}')\n",
    "    \n",
    "def load_model(model_path, model_class):\n",
    "    \"\"\"Load a saved model\"\"\"\n",
    "    \n",
    "    device = 'cuda:0'\n",
    "    model = model_class()\n",
    "    model.load_state_dict(torch.load(model_path)['model'])\n",
    "    model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0174936",
   "metadata": {},
   "outputs": [],
   "source": [
    "#since length list is all the same is that the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bba3980d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdn = FrameDiffNoise()\n",
    "B = 16\n",
    "#single_t dataset\n",
    "# sd = smallPDBDataset(fdn , meta_data_path = '/mnt/h/datasets/bCov_4H/metadata.csv', \n",
    "#                      filter_dict=False, maxlen=1000, input_t=0.05)\n",
    "#single_t dataset\n",
    "sd = smallPDBDataset(fdn , meta_data_path = '/mnt/h/datasets/bCov_4H/metadata.csv', \n",
    "                     filter_dict=False, maxlen=5000)\n",
    "ts = TrainSampler(B,sd, sample_mode='single_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "770d5f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ts = TrainSampler(B,sd, sample_mode='length_batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91d5f736",
   "metadata": {},
   "outputs": [],
   "source": [
    "gu = GraphUNet(batch_size = B, num_layers_ca = 2).to('cuda')\n",
    "opti = torch.optim.Adam(gu.parameters(), lr=0.0005, weight_decay=5e-6) #prev lr=0.0005\n",
    "gm = Make_KNN_MP_Graphs() #consider precalculating graphs for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "920c0ed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f404ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dL = torch.utils.data.DataLoader(sd,\n",
    "        sampler=ts,\n",
    "        batch_size=B,\n",
    "        shuffle=False,\n",
    "        collate_fn=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18cb85fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch_feats in dL:\n",
    "#     device='cuda'\n",
    "#     batch_feats= tree.map_structure(\n",
    "#                     lambda x: x.to(device), batch_feats)\n",
    "#     noised_dict =   {'CA': batch_feats['CA_noised'] ,\n",
    "#                      'N_CA': batch_feats['N_CA_noised'].unsqueeze(-2) ,\n",
    "#                      'C_CA': batch_feats['C_CA_noised'].unsqueeze(-2)  }\n",
    "#     break\n",
    "# t,n,p = get_noise_pred_true(batch_feats, noised_dict, gm, gu, scale=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da479c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d44fd192",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nwoodall/miniconda3/envs/se33/lib/python3.9/site-packages/dgl/backend/pytorch/tensor.py:449: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), (\n"
     ]
    }
   ],
   "source": [
    "view_t = np.array([0.01,0.05,0.1,0.2,0.3,0.5,0.8,1.0])\n",
    "view_t = view_t[None,...].repeat(int(np.ceil(B/len(view_t))),axis=0).flatten()[:B]\n",
    "visualize_model(sd, view_t, epoch=1, numOut=1, outdir='output/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c421b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss Epoch 0: 0.1967860460281372;   Epoch time: 407\n",
      "Average Train Loss Epoch 1: 0.17869804799556732;   Epoch time: 387\n",
      "Average Train Loss Epoch 2: 0.17455224692821503;   Epoch time: 384\n",
      "Average Train Loss Epoch 3: 0.17577950656414032;   Epoch time: 384\n",
      "Average Train Loss Epoch 4: 0.1731521487236023;   Epoch time: 386\n",
      "Average Train Loss Epoch 5: 0.17853529751300812;   Epoch time: 385\n",
      "Average Train Loss Epoch 6: 0.17612414062023163;   Epoch time: 384\n",
      "Average Train Loss Epoch 7: 0.17582055926322937;   Epoch time: 384\n",
      "Average Train Loss Epoch 8: 0.1724880188703537;   Epoch time: 385\n",
      "Average Train Loss Epoch 9: 0.17326298356056213;   Epoch time: 378\n",
      "Average Train Loss Epoch 10: 0.17529898881912231;   Epoch time: 380\n",
      "Average Train Loss Epoch 11: 0.1757819652557373;   Epoch time: 386\n",
      "Average Train Loss Epoch 12: 0.17549477517604828;   Epoch time: 383\n",
      "Average Train Loss Epoch 13: 0.17425617575645447;   Epoch time: 382\n",
      "Average Train Loss Epoch 14: 0.17422497272491455;   Epoch time: 384\n",
      "Average Train Loss Epoch 15: 0.17610619962215424;   Epoch time: 386\n",
      "Average Train Loss Epoch 16: 0.17120501399040222;   Epoch time: 384\n",
      "Average Train Loss Epoch 17: 0.17387130856513977;   Epoch time: 379\n",
      "Average Train Loss Epoch 18: 0.17342659831047058;   Epoch time: 384\n",
      "Average Train Loss Epoch 19: 0.1722223162651062;   Epoch time: 385\n",
      "Average Train Loss Epoch 20: 0.17452822625637054;   Epoch time: 385\n",
      "Average Train Loss Epoch 21: 0.17550112307071686;   Epoch time: 384\n",
      "Average Train Loss Epoch 22: 0.17165902256965637;   Epoch time: 387\n",
      "Average Train Loss Epoch 23: 0.1721152514219284;   Epoch time: 379\n",
      "Average Train Loss Epoch 24: 0.17210827767848969;   Epoch time: 385\n",
      "Average Train Loss Epoch 25: 0.17335890233516693;   Epoch time: 384\n",
      "Average Train Loss Epoch 26: 0.1720774918794632;   Epoch time: 387\n",
      "Average Train Loss Epoch 27: 0.17294900119304657;   Epoch time: 384\n",
      "Average Train Loss Epoch 28: 0.17353782057762146;   Epoch time: 385\n",
      "Average Train Loss Epoch 29: 0.17455241084098816;   Epoch time: 387\n",
      "Average Train Loss Epoch 30: 0.17235982418060303;   Epoch time: 384\n",
      "Average Train Loss Epoch 31: 0.17149575054645538;   Epoch time: 384\n",
      "Average Train Loss Epoch 32: 0.17190347611904144;   Epoch time: 395\n",
      "Average Train Loss Epoch 33: 0.17225903272628784;   Epoch time: 384\n",
      "Average Train Loss Epoch 34: 0.17172330617904663;   Epoch time: 374\n",
      "Average Train Loss Epoch 35: 0.17319370806217194;   Epoch time: 378\n",
      "Average Train Loss Epoch 36: 0.1714888960123062;   Epoch time: 379\n",
      "Average Train Loss Epoch 37: 0.17326419055461884;   Epoch time: 380\n",
      "Average Train Loss Epoch 38: 0.17579948902130127;   Epoch time: 376\n",
      "Average Train Loss Epoch 39: 0.16948308050632477;   Epoch time: 383\n",
      "Average Train Loss Epoch 40: 0.16887953877449036;   Epoch time: 385\n",
      "Average Train Loss Epoch 41: 0.17057044804096222;   Epoch time: 386\n",
      "Average Train Loss Epoch 42: 0.16951653361320496;   Epoch time: 385\n",
      "Average Train Loss Epoch 43: 0.16635310649871826;   Epoch time: 384\n",
      "Average Train Loss Epoch 44: 0.16662509739398956;   Epoch time: 387\n",
      "Average Train Loss Epoch 45: 0.16663163900375366;   Epoch time: 386\n",
      "Average Train Loss Epoch 46: 0.16733485460281372;   Epoch time: 385\n",
      "Average Train Loss Epoch 47: 0.1674058437347412;   Epoch time: 382\n",
      "Average Train Loss Epoch 48: 0.16682106256484985;   Epoch time: 385\n",
      "Average Train Loss Epoch 49: 0.16452902555465698;   Epoch time: 386\n",
      "Average Train Loss Epoch 50: 0.16508977115154266;   Epoch time: 387\n",
      "Average Train Loss Epoch 51: 0.1650213897228241;   Epoch time: 387\n",
      "Average Train Loss Epoch 52: 0.16607436537742615;   Epoch time: 386\n",
      "Average Train Loss Epoch 53: 0.1644214242696762;   Epoch time: 385\n",
      "Average Train Loss Epoch 54: 0.16329455375671387;   Epoch time: 388\n",
      "Average Train Loss Epoch 55: 0.16306857764720917;   Epoch time: 383\n",
      "Average Train Loss Epoch 56: 0.16015282273292542;   Epoch time: 385\n",
      "Average Train Loss Epoch 57: 0.16127094626426697;   Epoch time: 386\n",
      "Average Train Loss Epoch 58: 0.16281633079051971;   Epoch time: 388\n",
      "Average Train Loss Epoch 59: 0.15964102745056152;   Epoch time: 387\n",
      "Average Train Loss Epoch 60: 0.15931396186351776;   Epoch time: 379\n",
      "Average Train Loss Epoch 61: 0.15867599844932556;   Epoch time: 386\n",
      "Average Train Loss Epoch 62: 0.15901382267475128;   Epoch time: 387\n",
      "Average Train Loss Epoch 63: 0.15848806500434875;   Epoch time: 386\n",
      "Average Train Loss Epoch 64: 0.15669037401676178;   Epoch time: 386\n",
      "Average Train Loss Epoch 65: 0.15794578194618225;   Epoch time: 381\n",
      "Average Train Loss Epoch 66: 0.15691223740577698;   Epoch time: 387\n",
      "Average Train Loss Epoch 67: 0.1576199233531952;   Epoch time: 391\n",
      "Average Train Loss Epoch 68: 0.1553240865468979;   Epoch time: 387\n",
      "Average Train Loss Epoch 69: 0.15542732179164886;   Epoch time: 388\n",
      "Average Train Loss Epoch 70: 0.15615782141685486;   Epoch time: 393\n",
      "Average Train Loss Epoch 71: 0.15400469303131104;   Epoch time: 388\n",
      "Average Train Loss Epoch 72: 0.1546173393726349;   Epoch time: 388\n",
      "Average Train Loss Epoch 73: 0.15251578390598297;   Epoch time: 386\n",
      "Average Train Loss Epoch 74: 0.15316782891750336;   Epoch time: 385\n",
      "Average Train Loss Epoch 75: 0.15220215916633606;   Epoch time: 385\n",
      "Average Train Loss Epoch 76: 0.15067197382450104;   Epoch time: 387\n",
      "Average Train Loss Epoch 77: 0.15116848051548004;   Epoch time: 386\n",
      "Average Train Loss Epoch 78: 0.15045256912708282;   Epoch time: 387\n",
      "Average Train Loss Epoch 79: 0.15201586484909058;   Epoch time: 389\n",
      "Average Train Loss Epoch 80: 0.15080131590366364;   Epoch time: 387\n",
      "Average Train Loss Epoch 81: 0.1508130431175232;   Epoch time: 387\n",
      "Average Train Loss Epoch 82: 0.1498810052871704;   Epoch time: 381\n",
      "Average Train Loss Epoch 83: 0.14881490170955658;   Epoch time: 388\n",
      "Average Train Loss Epoch 84: 0.148182675242424;   Epoch time: 389\n",
      "Average Train Loss Epoch 85: 0.14817754924297333;   Epoch time: 384\n",
      "Average Train Loss Epoch 86: 0.1462206244468689;   Epoch time: 390\n",
      "Average Train Loss Epoch 87: 0.1456836462020874;   Epoch time: 384\n",
      "Average Train Loss Epoch 88: 0.14539892971515656;   Epoch time: 383\n",
      "Average Train Loss Epoch 89: 0.14455997943878174;   Epoch time: 389\n",
      "Average Train Loss Epoch 90: 0.1439061313867569;   Epoch time: 389\n",
      "Average Train Loss Epoch 91: 0.1442083865404129;   Epoch time: 388\n",
      "Average Train Loss Epoch 92: 0.1445506364107132;   Epoch time: 388\n",
      "Average Train Loss Epoch 93: 0.14307363331317902;   Epoch time: 390\n",
      "Average Train Loss Epoch 94: 0.14323367178440094;   Epoch time: 388\n",
      "Average Train Loss Epoch 95: 0.14220288395881653;   Epoch time: 387\n",
      "Average Train Loss Epoch 96: 0.14090704917907715;   Epoch time: 388\n",
      "Average Train Loss Epoch 97: 0.14357712864875793;   Epoch time: 389\n",
      "Average Train Loss Epoch 98: 0.14216414093971252;   Epoch time: 382\n",
      "Average Train Loss Epoch 99: 0.14277032017707825;   Epoch time: 387\n",
      "Average Train Loss Epoch 100: 0.14178408682346344;   Epoch time: 385\n",
      "Average Train Loss Epoch 101: 0.1423562616109848;   Epoch time: 381\n",
      "Average Train Loss Epoch 102: 0.1423197090625763;   Epoch time: 383\n",
      "Average Train Loss Epoch 103: 0.14080160856246948;   Epoch time: 386\n",
      "Average Train Loss Epoch 104: 0.14016032218933105;   Epoch time: 387\n",
      "Average Train Loss Epoch 105: 0.14034533500671387;   Epoch time: 384\n",
      "Average Train Loss Epoch 106: 0.14097066223621368;   Epoch time: 385\n",
      "Average Train Loss Epoch 107: 0.13951826095581055;   Epoch time: 384\n",
      "Average Train Loss Epoch 108: 0.1393691748380661;   Epoch time: 382\n",
      "Average Train Loss Epoch 109: 0.13778313994407654;   Epoch time: 384\n",
      "Average Train Loss Epoch 110: 0.13872334361076355;   Epoch time: 383\n",
      "Average Train Loss Epoch 111: 0.13817478716373444;   Epoch time: 386\n",
      "Average Train Loss Epoch 112: 0.13944396376609802;   Epoch time: 386\n",
      "Average Train Loss Epoch 113: 0.13750767707824707;   Epoch time: 385\n",
      "Average Train Loss Epoch 114: 0.1368001401424408;   Epoch time: 380\n",
      "Average Train Loss Epoch 115: 0.13715842366218567;   Epoch time: 386\n",
      "Average Train Loss Epoch 116: 0.1388944536447525;   Epoch time: 385\n",
      "Average Train Loss Epoch 117: 0.13757187128067017;   Epoch time: 380\n",
      "Average Train Loss Epoch 118: 0.136331707239151;   Epoch time: 386\n",
      "Average Train Loss Epoch 119: 0.13632738590240479;   Epoch time: 385\n",
      "Average Train Loss Epoch 120: 0.13622312247753143;   Epoch time: 385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss Epoch 121: 0.1363457441329956;   Epoch time: 385\n",
      "Average Train Loss Epoch 122: 0.13643795251846313;   Epoch time: 384\n",
      "Average Train Loss Epoch 123: 0.13534756004810333;   Epoch time: 381\n",
      "Average Train Loss Epoch 124: 0.13565130531787872;   Epoch time: 379\n",
      "Average Train Loss Epoch 125: 0.1351713240146637;   Epoch time: 386\n",
      "Average Train Loss Epoch 126: 0.13479821383953094;   Epoch time: 385\n",
      "Average Train Loss Epoch 127: 0.13536608219146729;   Epoch time: 385\n",
      "Average Train Loss Epoch 128: 0.13483324646949768;   Epoch time: 385\n",
      "Average Train Loss Epoch 129: 0.13410915434360504;   Epoch time: 385\n",
      "Average Train Loss Epoch 130: 0.13392497599124908;   Epoch time: 385\n",
      "Average Train Loss Epoch 131: 0.1333518624305725;   Epoch time: 384\n",
      "Average Train Loss Epoch 132: 0.13162334263324738;   Epoch time: 387\n",
      "Average Train Loss Epoch 133: 0.1324034482240677;   Epoch time: 388\n",
      "Average Train Loss Epoch 134: 0.13062304258346558;   Epoch time: 386\n",
      "Average Train Loss Epoch 135: 0.13176696002483368;   Epoch time: 386\n",
      "Average Train Loss Epoch 136: 0.13034945726394653;   Epoch time: 387\n",
      "Average Train Loss Epoch 137: 0.13029929995536804;   Epoch time: 386\n",
      "Average Train Loss Epoch 138: 0.13117264211177826;   Epoch time: 382\n",
      "Average Train Loss Epoch 139: 0.13101617991924286;   Epoch time: 383\n",
      "Average Train Loss Epoch 140: 0.13178494572639465;   Epoch time: 386\n",
      "Average Train Loss Epoch 141: 0.12946712970733643;   Epoch time: 385\n",
      "Average Train Loss Epoch 142: 0.12933142483234406;   Epoch time: 385\n",
      "Average Train Loss Epoch 143: 0.1307154893875122;   Epoch time: 388\n",
      "Average Train Loss Epoch 144: 0.12997381389141083;   Epoch time: 386\n",
      "Average Train Loss Epoch 145: 0.12861107289791107;   Epoch time: 386\n",
      "Average Train Loss Epoch 146: 0.1287783533334732;   Epoch time: 386\n",
      "Average Train Loss Epoch 147: 0.12960591912269592;   Epoch time: 386\n",
      "Average Train Loss Epoch 148: 0.1289004683494568;   Epoch time: 387\n",
      "Average Train Loss Epoch 149: 0.12780825793743134;   Epoch time: 385\n",
      "Average Train Loss Epoch 150: 0.1295776665210724;   Epoch time: 386\n",
      "Average Train Loss Epoch 151: 0.12991845607757568;   Epoch time: 387\n",
      "Average Train Loss Epoch 152: 0.12910515069961548;   Epoch time: 386\n",
      "Average Train Loss Epoch 153: 0.13067664206027985;   Epoch time: 383\n",
      "Average Train Loss Epoch 154: 0.12779168784618378;   Epoch time: 380\n",
      "Average Train Loss Epoch 155: 0.12877380847930908;   Epoch time: 386\n",
      "Average Train Loss Epoch 156: 0.12784597277641296;   Epoch time: 386\n",
      "Average Train Loss Epoch 157: 0.1286504864692688;   Epoch time: 387\n",
      "Average Train Loss Epoch 158: 0.1281430572271347;   Epoch time: 388\n",
      "Average Train Loss Epoch 159: 0.12773090600967407;   Epoch time: 386\n",
      "Average Train Loss Epoch 160: 0.12826284766197205;   Epoch time: 404\n",
      "Average Train Loss Epoch 161: 0.12889273464679718;   Epoch time: 388\n",
      "Average Train Loss Epoch 162: 0.12756548821926117;   Epoch time: 385\n",
      "Average Train Loss Epoch 163: 0.12854516506195068;   Epoch time: 384\n",
      "Average Train Loss Epoch 164: 0.12731225788593292;   Epoch time: 386\n",
      "Average Train Loss Epoch 165: 0.12941691279411316;   Epoch time: 386\n",
      "Average Train Loss Epoch 166: 0.1282842755317688;   Epoch time: 386\n",
      "Average Train Loss Epoch 167: 0.12786786258220673;   Epoch time: 385\n",
      "Average Train Loss Epoch 168: 0.12733431160449982;   Epoch time: 390\n",
      "Average Train Loss Epoch 169: 0.127868190407753;   Epoch time: 384\n",
      "Average Train Loss Epoch 170: 0.12870366871356964;   Epoch time: 385\n",
      "Average Train Loss Epoch 171: 0.1283993124961853;   Epoch time: 386\n",
      "Average Train Loss Epoch 172: 0.12715359032154083;   Epoch time: 384\n",
      "Average Train Loss Epoch 173: 0.12633466720581055;   Epoch time: 382\n",
      "Average Train Loss Epoch 174: 0.12794633209705353;   Epoch time: 383\n",
      "Average Train Loss Epoch 175: 0.12746503949165344;   Epoch time: 387\n",
      "Average Train Loss Epoch 176: 0.12641651928424835;   Epoch time: 386\n",
      "Average Train Loss Epoch 177: 0.12798994779586792;   Epoch time: 401\n",
      "Average Train Loss Epoch 178: 0.1278354376554489;   Epoch time: 383\n",
      "Average Train Loss Epoch 179: 0.12741544842720032;   Epoch time: 378\n",
      "Average Train Loss Epoch 180: 0.1257675588130951;   Epoch time: 386\n",
      "Average Train Loss Epoch 181: 0.12638668715953827;   Epoch time: 383\n",
      "Average Train Loss Epoch 182: 0.12678572535514832;   Epoch time: 387\n",
      "Average Train Loss Epoch 183: 0.1273629516363144;   Epoch time: 387\n",
      "Average Train Loss Epoch 184: 0.1256154626607895;   Epoch time: 396\n",
      "Average Train Loss Epoch 185: 0.12750926613807678;   Epoch time: 398\n",
      "Average Train Loss Epoch 186: 0.12592478096485138;   Epoch time: 425\n",
      "Average Train Loss Epoch 187: 0.12606501579284668;   Epoch time: 411\n",
      "Average Train Loss Epoch 188: 0.12564195692539215;   Epoch time: 401\n",
      "Average Train Loss Epoch 189: 0.1255800575017929;   Epoch time: 383\n",
      "Average Train Loss Epoch 190: 0.12547749280929565;   Epoch time: 386\n",
      "Average Train Loss Epoch 191: 0.12593872845172882;   Epoch time: 386\n",
      "Average Train Loss Epoch 192: 0.12635906040668488;   Epoch time: 387\n",
      "Average Train Loss Epoch 193: 0.12500067055225372;   Epoch time: 387\n",
      "Average Train Loss Epoch 194: 0.12492267787456512;   Epoch time: 382\n",
      "Average Train Loss Epoch 195: 0.12614570558071136;   Epoch time: 403\n",
      "Average Train Loss Epoch 196: 0.12600024044513702;   Epoch time: 425\n",
      "Average Train Loss Epoch 197: 0.12498883903026581;   Epoch time: 421\n",
      "Average Train Loss Epoch 198: 0.12485504150390625;   Epoch time: 427\n",
      "Average Train Loss Epoch 199: 0.12494681775569916;   Epoch time: 438\n",
      "Average Train Loss Epoch 200: 0.12587611377239227;   Epoch time: 415\n",
      "Average Train Loss Epoch 201: 0.12488701194524765;   Epoch time: 454\n",
      "Average Train Loss Epoch 202: 0.1255086213350296;   Epoch time: 442\n",
      "Average Train Loss Epoch 203: 0.12568789720535278;   Epoch time: 414\n",
      "Average Train Loss Epoch 204: 0.12485107779502869;   Epoch time: 441\n",
      "Average Train Loss Epoch 205: 0.12574531137943268;   Epoch time: 405\n",
      "Average Train Loss Epoch 206: 0.12419579923152924;   Epoch time: 432\n",
      "Average Train Loss Epoch 207: 0.1250915676355362;   Epoch time: 430\n",
      "Average Train Loss Epoch 208: 0.1253196895122528;   Epoch time: 453\n",
      "Average Train Loss Epoch 209: 0.1246735230088234;   Epoch time: 444\n",
      "Average Train Loss Epoch 210: 0.12468116730451584;   Epoch time: 431\n",
      "Average Train Loss Epoch 211: 0.12437211722135544;   Epoch time: 427\n",
      "Average Train Loss Epoch 212: 0.12524043023586273;   Epoch time: 444\n",
      "Average Train Loss Epoch 213: 0.1263975352048874;   Epoch time: 466\n",
      "Average Train Loss Epoch 214: 0.12433889508247375;   Epoch time: 407\n",
      "Average Train Loss Epoch 215: 0.12416316568851471;   Epoch time: 392\n",
      "Average Train Loss Epoch 216: 0.12329740822315216;   Epoch time: 421\n",
      "Average Train Loss Epoch 217: 0.12480856478214264;   Epoch time: 390\n",
      "Average Train Loss Epoch 218: 0.12463182210922241;   Epoch time: 384\n",
      "Average Train Loss Epoch 219: 0.12393549084663391;   Epoch time: 384\n",
      "Average Train Loss Epoch 220: 0.12480396032333374;   Epoch time: 386\n",
      "Average Train Loss Epoch 221: 0.12314899265766144;   Epoch time: 385\n",
      "Average Train Loss Epoch 222: 0.12517531216144562;   Epoch time: 384\n",
      "Average Train Loss Epoch 223: 0.12494243681430817;   Epoch time: 385\n",
      "Average Train Loss Epoch 224: 0.12446357309818268;   Epoch time: 385\n",
      "Average Train Loss Epoch 225: 0.12350443005561829;   Epoch time: 383\n",
      "Average Train Loss Epoch 226: 0.12370899319648743;   Epoch time: 383\n",
      "Average Train Loss Epoch 227: 0.12314511835575104;   Epoch time: 384\n",
      "Average Train Loss Epoch 228: 0.12464473396539688;   Epoch time: 384\n",
      "Average Train Loss Epoch 229: 0.12359361350536346;   Epoch time: 383\n",
      "Average Train Loss Epoch 230: 0.12499679625034332;   Epoch time: 383\n",
      "Average Train Loss Epoch 231: 0.1248055249452591;   Epoch time: 384\n",
      "Average Train Loss Epoch 232: 0.1233123168349266;   Epoch time: 380\n",
      "Average Train Loss Epoch 233: 0.12340578436851501;   Epoch time: 383\n",
      "Average Train Loss Epoch 234: 0.12277980893850327;   Epoch time: 383\n",
      "Average Train Loss Epoch 235: 0.12452049553394318;   Epoch time: 384\n",
      "Average Train Loss Epoch 236: 0.12292902171611786;   Epoch time: 380\n",
      "Average Train Loss Epoch 237: 0.12412742525339127;   Epoch time: 383\n",
      "Average Train Loss Epoch 238: 0.12443714588880539;   Epoch time: 393\n",
      "Average Train Loss Epoch 239: 0.12359556555747986;   Epoch time: 384\n",
      "Average Train Loss Epoch 240: 0.1242419108748436;   Epoch time: 382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss Epoch 241: 0.12298718839883804;   Epoch time: 379\n",
      "Average Train Loss Epoch 242: 0.12298154830932617;   Epoch time: 378\n",
      "Average Train Loss Epoch 243: 0.1242702454328537;   Epoch time: 375\n",
      "Average Train Loss Epoch 244: 0.12410955876111984;   Epoch time: 375\n",
      "Average Train Loss Epoch 245: 0.12229809165000916;   Epoch time: 384\n",
      "Average Train Loss Epoch 246: 0.12395744770765305;   Epoch time: 384\n",
      "Average Train Loss Epoch 247: 0.12277005612850189;   Epoch time: 384\n",
      "Average Train Loss Epoch 248: 0.1227627769112587;   Epoch time: 376\n",
      "Average Train Loss Epoch 249: 0.12224075198173523;   Epoch time: 380\n",
      "Average Train Loss Epoch 250: 0.12251082807779312;   Epoch time: 381\n",
      "Average Train Loss Epoch 251: 0.12451999634504318;   Epoch time: 383\n",
      "Average Train Loss Epoch 252: 0.12378067523241043;   Epoch time: 383\n",
      "Average Train Loss Epoch 253: 0.12171947956085205;   Epoch time: 385\n",
      "Average Train Loss Epoch 254: 0.12362827360630035;   Epoch time: 384\n",
      "Average Train Loss Epoch 255: 0.1229877695441246;   Epoch time: 381\n",
      "Average Train Loss Epoch 256: 0.12384112924337387;   Epoch time: 384\n",
      "Average Train Loss Epoch 257: 0.12364277243614197;   Epoch time: 382\n",
      "Average Train Loss Epoch 258: 0.12321335077285767;   Epoch time: 380\n",
      "Average Train Loss Epoch 259: 0.12365320324897766;   Epoch time: 383\n",
      "Average Train Loss Epoch 260: 0.1219562515616417;   Epoch time: 385\n",
      "Average Train Loss Epoch 261: 0.12382712960243225;   Epoch time: 383\n",
      "Average Train Loss Epoch 262: 0.12310213595628738;   Epoch time: 383\n",
      "Average Train Loss Epoch 263: 0.12311021983623505;   Epoch time: 379\n",
      "Average Train Loss Epoch 264: 0.12207887321710587;   Epoch time: 385\n",
      "Average Train Loss Epoch 265: 0.1219949945807457;   Epoch time: 384\n",
      "Average Train Loss Epoch 266: 0.12306670844554901;   Epoch time: 383\n",
      "Average Train Loss Epoch 267: 0.12323515117168427;   Epoch time: 383\n",
      "Average Train Loss Epoch 268: 0.1224016547203064;   Epoch time: 385\n",
      "Average Train Loss Epoch 269: 0.12216293811798096;   Epoch time: 384\n",
      "Average Train Loss Epoch 270: 0.12154346704483032;   Epoch time: 384\n",
      "Average Train Loss Epoch 271: 0.12324628978967667;   Epoch time: 385\n",
      "Average Train Loss Epoch 272: 0.12166576832532883;   Epoch time: 381\n",
      "Average Train Loss Epoch 273: 0.12280601263046265;   Epoch time: 384\n",
      "Average Train Loss Epoch 274: 0.12156268954277039;   Epoch time: 383\n",
      "Average Train Loss Epoch 275: 0.12111421674489975;   Epoch time: 377\n",
      "Average Train Loss Epoch 276: 0.12339027225971222;   Epoch time: 381\n",
      "Average Train Loss Epoch 277: 0.12181045114994049;   Epoch time: 383\n",
      "Average Train Loss Epoch 278: 0.12273185700178146;   Epoch time: 391\n",
      "Average Train Loss Epoch 279: 0.12167458981275558;   Epoch time: 386\n",
      "Average Train Loss Epoch 280: 0.12262392044067383;   Epoch time: 376\n",
      "Average Train Loss Epoch 281: 0.12296460568904877;   Epoch time: 384\n",
      "Average Train Loss Epoch 282: 0.12273311614990234;   Epoch time: 384\n",
      "Average Train Loss Epoch 283: 0.12172433733940125;   Epoch time: 382\n",
      "Average Train Loss Epoch 284: 0.12123637646436691;   Epoch time: 385\n",
      "Average Train Loss Epoch 285: 0.12107021361589432;   Epoch time: 384\n",
      "Average Train Loss Epoch 286: 0.12267263233661652;   Epoch time: 381\n",
      "Average Train Loss Epoch 287: 0.12157661467790604;   Epoch time: 401\n",
      "Average Train Loss Epoch 288: 0.12263664603233337;   Epoch time: 386\n",
      "Average Train Loss Epoch 289: 0.12227792292833328;   Epoch time: 386\n",
      "Average Train Loss Epoch 290: 0.12122732400894165;   Epoch time: 383\n",
      "Average Train Loss Epoch 291: 0.12193229794502258;   Epoch time: 380\n",
      "Average Train Loss Epoch 292: 0.12284359335899353;   Epoch time: 385\n",
      "Average Train Loss Epoch 293: 0.12192957103252411;   Epoch time: 386\n",
      "Average Train Loss Epoch 294: 0.12234951555728912;   Epoch time: 383\n",
      "Average Train Loss Epoch 295: 0.12232725322246552;   Epoch time: 377\n",
      "Average Train Loss Epoch 296: 0.12220349907875061;   Epoch time: 378\n",
      "Average Train Loss Epoch 297: 0.12151092290878296;   Epoch time: 377\n",
      "Average Train Loss Epoch 298: 0.12178479880094528;   Epoch time: 380\n",
      "Average Train Loss Epoch 299: 0.12236954271793365;   Epoch time: 384\n"
     ]
    }
   ],
   "source": [
    "model_path = make_save_folder(name=f'4h_full_t_load_check')\n",
    "num_epochs = 300\n",
    "e_start= 0\n",
    "save_per=10\n",
    "avg_vloss=0\n",
    "\n",
    "#visualize_T\n",
    "#view_t = np.array([0.05]*8)\n",
    "\n",
    "view_t = np.array([0.01,0.05,0.1,0.2,0.3,0.5,0.8,1.0])\n",
    "view_t = view_t[None,...].repeat(int(np.ceil(B/len(view_t))),axis=0).flatten()[:B]\n",
    "\n",
    "\n",
    "for e in range(e_start, e_start+num_epochs):\n",
    "    \n",
    "    running_tloss = 0 \n",
    "    start = time.time()\n",
    "    for i,batch_feats in enumerate(dL):\n",
    "\n",
    "        device='cuda'\n",
    "        batch_feats= tree.map_structure(\n",
    "                        lambda x: x.to(device), batch_feats)\n",
    "        noised_dict =   {'CA': batch_feats['CA_noised'] ,\n",
    "                         'N_CA': batch_feats['N_CA_noised'].unsqueeze(-2) ,\n",
    "                         'C_CA': batch_feats['C_CA_noised'].unsqueeze(-2)  }\n",
    "\n",
    "        pred, train_loss = model_step(batch_feats, noised_dict, gm, gu, train=True)\n",
    "        opti.zero_grad()\n",
    "        train_loss.backward()\n",
    "        opti.step()\n",
    "\n",
    "        running_tloss += train_loss.detach().cpu()\n",
    "        \n",
    "    \n",
    "\n",
    "    #n,p,t = get_noise_pred_true(batch_feats, noised_dict, gm, gu, scale=10)\n",
    "    end = time.time()\n",
    "    avg_tloss = running_tloss/(i+1)\n",
    "    print(f'Average Train Loss Epoch {e}: {avg_tloss};   Epoch time: {end-start:.0f}')\n",
    "    if e %save_per==save_per-1:\n",
    "        visualize_model(sd, view_t, epoch=e, numOut=8, outdir=f'{model_path}')\n",
    "        save_chkpt(f'{model_path}', gu, opti, e, B, avg_vloss, avg_tloss)\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "27010f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch_feats in dL:\n",
    "#     device='cuda'\n",
    "#     batch_feats= tree.map_structure(\n",
    "#                     lambda x: x.to(device), batch_feats)\n",
    "#     noised_dict =   {'CA': batch_feats['CA_noised'] ,\n",
    "#                      'N_CA': batch_feats['N_CA_noised'].unsqueeze(-2) ,\n",
    "#                      'C_CA': batch_feats['C_CA_noised'].unsqueeze(-2)  }\n",
    "#     break\n",
    "# t,n,p = get_noise_pred_true(batch_feats, noised_dict, gm, gu, scale=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9cec9c7a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dump_tnp(t,\u001b[43mn\u001b[49m,p,batch_feats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m'\u001b[39m], e\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, numOut\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,outdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'n' is not defined"
     ]
    }
   ],
   "source": [
    "dump_tnp(t,n,p,batch_feats['t'], e=0, numOut=1,outdir='output/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02152d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the conversions don't seem correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d800e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_model(bb_dict, noised_bb, batched_t, epoch, numOut=1, outdir='output/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb8b9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "t=0.05\n",
    "t_vec = np.ones((B,))*t\n",
    "print(t_vec)\n",
    "model_path = make_save_folder(name=f'fdiff_modelStep_doublecheck')\n",
    "num_epochs = 50\n",
    "e_start= 0\n",
    "save_per=10\n",
    "avg_vloss=0\n",
    "\n",
    "for e in range(e_start, e_start+num_epochs):\n",
    "    \n",
    "    running_tloss = 0 \n",
    "    start = time.time()\n",
    "    for i, bb_dict in enumerate(train_dL):\n",
    "        noised_bb, tv, ss = fdn.forward_fixed_nodes(bb_dict,t_vec=t_vec, useR3=useR3)\n",
    "        tv = tv.to('cuda')\n",
    "        ss = ss.to('cuda')\n",
    "        train_loss = model_step(bb_dict, noised_bb, tv, ss, gm, gu)\n",
    "        opti.zero_grad()\n",
    "        train_loss.backward()\n",
    "        opti.step()\n",
    "\n",
    "        running_tloss += train_loss.detach().cpu()\n",
    "    \n",
    "    end = time.time()\n",
    "    avg_tloss = running_tloss/(i+1)\n",
    "    print(f'Average Train Loss Epoch {e}: {avg_tloss};   Epoch time: {end-start:.0f}')\n",
    "\n",
    "    if e %save_per==save_per-1:\n",
    "        with torch.no_grad():\n",
    "            running_vloss = 0\n",
    "            for i, bb_dictv in enumerate(val_dL):\n",
    "                noised_bb, tv, ss = fdn.forward_fixed_nodes(bb_dictv,t_vec=None, useR3=useR3) #this does the opposite of traditional, upweighting lower\n",
    "                tv = tv.to('cuda')\n",
    "                ss = ss.to('cuda')\n",
    "                valid_loss = model_step(bb_dictv, noised_bb, tv, ss, gm, gu)\n",
    "                running_vloss += valid_loss\n",
    "                \n",
    "        avg_vloss = running_vloss/(i+1)\n",
    "        print(f'Average Valid Loss Epoch {e}: {avg_vloss}')\n",
    "                \n",
    "        noised_bb, tv, ss = fdn.forward_fixed_nodes(bb_dict, t_vec=vis_t)\n",
    "        tv = tv.to('cuda')\n",
    "        visualize_model(bb_dict, noised_bb, tv, e, numOut=8,outdir=f'{model_path}')\n",
    "        save_chkpt(f'{model_path}', gu, opti, e, B, avg_vloss, avg_tloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d886a502",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c9db18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "se33",
   "language": "python",
   "name": "se33"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
