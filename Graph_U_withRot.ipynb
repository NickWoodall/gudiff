{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a02a1c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import util.npose_util as nu\n",
    "import os\n",
    "import pathlib\n",
    "import dgl\n",
    "from dgl import backend as F\n",
    "#import torch_geometric\n",
    "from torch.utils.data import random_split, DataLoader, Dataset\n",
    "from typing import Dict\n",
    "from torch import Tensor\n",
    "from dgl import DGLGraph\n",
    "from torch import nn\n",
    "from torch import einsum\n",
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf443f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_rigid_diffuser.diffuser import FrameDiffNoise\n",
    "from se3_transformer.model.basis import get_basis, update_basis_with_fused\n",
    "from se3_transformer.model.transformer import Sequential, SE3Transformer\n",
    "from se3_transformer.model.transformer_topk import SE3Transformer_topK\n",
    "from se3_transformer.model.FAPE_Loss import FAPE_loss, Qs2Rs, normQ\n",
    "from se3_transformer.model.layers.attentiontopK import AttentionBlockSE3\n",
    "from se3_transformer.model.layers.linear import LinearSE3\n",
    "from se3_transformer.model.layers.convolution import ConvSE3, ConvSE3FuseLevel\n",
    "from se3_transformer.model.layers.norm import NormSE3\n",
    "from se3_transformer.model.layers.pooling import GPooling, Latent_Unpool, Unpool_Layer\n",
    "from se3_transformer.runtime.utils import str2bool, to_cuda\n",
    "from se3_transformer.model.fiber import Fiber\n",
    "from se3_transformer.model.transformer import get_populated_edge_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2511634d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#npose indexing\n",
    "# Useful numbers\n",
    "# N [-1.45837285,  0 , 0]\n",
    "# CA [0., 0., 0.]\n",
    "# C [0.55221403, 1.41890368, 0.        ]\n",
    "# CB [ 0.52892494, -0.77445692, -1.19923854]\n",
    "\n",
    "N_CA_dist = torch.tensor(1.458/10.0).to('cuda')\n",
    "C_CA_dist = torch.tensor(1.523/10.0).to('cuda')\n",
    "\n",
    "if ( hasattr(os, 'ATOM_NAMES') ):\n",
    "    assert( hasattr(os, 'PDB_ORDER') )\n",
    "\n",
    "    ATOM_NAMES = os.ATOM_NAMES\n",
    "    PDB_ORDER = os.PDB_ORDER\n",
    "else:\n",
    "    ATOM_NAMES=['N', 'CA', 'CB', 'C', 'O']\n",
    "    PDB_ORDER = ['N', 'CA', 'C', 'O', 'CB']\n",
    "\n",
    "_byte_atom_names = []\n",
    "_atom_names = []\n",
    "for i, atom_name in enumerate(ATOM_NAMES):\n",
    "    long_name = \" \" + atom_name + \"       \"\n",
    "    _atom_names.append(long_name[:4])\n",
    "    _byte_atom_names.append(atom_name.encode())\n",
    "\n",
    "    globals()[atom_name] = i\n",
    "\n",
    "R = len(ATOM_NAMES)\n",
    "\n",
    "if ( \"N\" not in globals() ):\n",
    "    N = -1\n",
    "if ( \"C\" not in globals() ):\n",
    "    C = -1\n",
    "if ( \"CB\" not in globals() ):\n",
    "    CB = -1\n",
    "\n",
    "\n",
    "_pdb_order = []\n",
    "for name in PDB_ORDER:\n",
    "    _pdb_order.append( ATOM_NAMES.index(name) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e213e885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path_str  = 'data/h4_ca_coords.npz'\n",
    "# test_limit = 1028\n",
    "# rr = np.load(data_path_str)\n",
    "# ca_coords = [rr[f] for f in rr.files][0][:test_limit,:,:3]\n",
    "# ca_coords.shape\n",
    "\n",
    "# getting N-Ca, Ca-C vectors to add as typeI features\n",
    "#apa = apart helices for val/train split\n",
    "#tog = together helices for val/train split\n",
    "apa_path_str  = 'data_npose/h4_apa_coords.npz'\n",
    "tog_path_str  = 'data_npose/h4_tog_coords.npz'\n",
    "\n",
    "#grab the first 3 atoms which are N,CA,C\n",
    "test_limit = 1028*5\n",
    "rr = np.load(apa_path_str)\n",
    "coords_apa = [rr[f] for f in rr.files][0][:test_limit,:]\n",
    "\n",
    "rr = np.load(tog_path_str)\n",
    "coords_tog = [rr[f] for f in rr.files][0][:test_limit,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4c2ed33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_npose_from_coords(coords_in):\n",
    "    \"\"\"Use N, CA, C coordinates to generate O an CB atoms\"\"\"\n",
    "    rot_mat_cat = np.ones(sum((coords_in.shape[:-1], (1,)), ()))\n",
    "    \n",
    "    coords = np.concatenate((coords_in,rot_mat_cat),axis=-1)\n",
    "    \n",
    "    npose = np.ones((coords_in.shape[0]*5,4)) #5 is atoms per res\n",
    "\n",
    "    by_res = npose.reshape(-1, 5, 4)\n",
    "    \n",
    "    if ( \"N\" in ATOM_NAMES ):\n",
    "        by_res[:,N,:3] = coords_in[:,0,:3]\n",
    "    if ( \"CA\" in ATOM_NAMES ):\n",
    "        by_res[:,CA,:3] = coords_in[:,1,:3]\n",
    "    if ( \"C\" in ATOM_NAMES ):\n",
    "        by_res[:,C,:3] = coords_in[:,2,:3]\n",
    "    if ( \"O\" in ATOM_NAMES ):\n",
    "        by_res[:,O,:3] = nu.build_O(npose)\n",
    "    if ( \"CB\" in ATOM_NAMES ):\n",
    "        tpose = nu.tpose_from_npose(npose)\n",
    "        by_res[:,CB,:] = nu.build_CB(tpose)\n",
    "\n",
    "    return npose\n",
    "\n",
    "def dump_coord_pdb(coords_in, fileOut='fileOut.pdb'):\n",
    "    \n",
    "    npose =  build_npose_from_coords(coords_in)\n",
    "    nu.dump_npdb(npose,fileOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "508327fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.])\n",
      "tensor([0.6000, 0.8000, 0.4000, 0.9000, 0.3000, 1.0000, 0.2000, 1.0000, 0.1000,\n",
      "        1.0000, 0.1000, 1.0000])\n",
      "tensor([1.0000, 0.2000, 0.8000, 0.6000, 0.6000, 0.8000, 0.4000, 0.9000, 0.3000,\n",
      "        1.0000, 0.2000, 1.0000])\n",
      "tensor([ 0.9000, -0.5000,  1.0000,  0.2000,  0.8000,  0.6000,  0.6000,  0.8000,\n",
      "         0.4000,  0.9000,  0.3000,  1.0000])\n",
      "tensor([ 0.4000, -0.9000,  1.0000, -0.3000,  1.0000,  0.3000,  0.8000,  0.7000,\n",
      "         0.6000,  0.8000,  0.4000,  0.9000])\n"
     ]
    }
   ],
   "source": [
    "#goal define edges of\n",
    "#connected backbone 1, \n",
    "#unconnected atoms 0,\n",
    "\n",
    "\n",
    "def define_graph_edges(n_nodes):\n",
    "    #connected backbone\n",
    "\n",
    "    con_v1 = np.arange(n_nodes-1) #vertex 1 of edges in chronological order\n",
    "    con_v2 = np.arange(1,n_nodes) #vertex 2 of edges in chronological order\n",
    "\n",
    "    ind = con_v1*(n_nodes-1)+con_v2-1 #account for removed self connections (-1)\n",
    "\n",
    "\n",
    "    #unconnected backbone\n",
    "\n",
    "    nodes = np.arange(n_nodes)\n",
    "    v1 = np.repeat(nodes,n_nodes-1) #starting vertices, same number repeated for each edge\n",
    "\n",
    "    start_v2 = np.repeat(np.arange(n_nodes)[None,:],n_nodes,axis=0)\n",
    "    diag_ind = np.diag_indices(n_nodes)\n",
    "    start_v2[diag_ind] = -1 #diagonal of matrix is self connections which we remove (self connections are managed by SE3 Conv channels)\n",
    "    v2 = start_v2[start_v2>-0.5] #remove diagonal and flatten\n",
    "\n",
    "    edge_data = torch.zeros(len(v2))\n",
    "    edge_data[ind] = 1\n",
    "    \n",
    "    return v1,v2,edge_data, ind\n",
    "\n",
    "\n",
    "\n",
    "def make_pe_encoding(n_nodes=65, embed_dim = 12, scale = 1000, cast_type=torch.float32, print_out=False):\n",
    "    #positional encoding of node\n",
    "    i_array = np.arange(1,(embed_dim/2)+1)\n",
    "    wk = (1/(scale**(i_array*2/embed_dim)))\n",
    "    t_array = np.arange(n_nodes)\n",
    "    si = torch.tensor(np.sin(wk*t_array.reshape((-1,1))))\n",
    "    ci = torch.tensor(np.cos(wk*t_array.reshape((-1,1))))\n",
    "    pe = torch.stack((si,ci),axis=2).reshape(t_array.shape[0],embed_dim).type(cast_type)\n",
    "    \n",
    "    if print_out == True:\n",
    "        for x in range(int(n_nodes/12)):\n",
    "            print(np.round(pe[x],1))\n",
    "    \n",
    "    return pe\n",
    "    \n",
    "    \n",
    "#v1,v2,edge_data, ind = define_graph_edges(n_nodes)\n",
    "#norm_p = normalize_points(ca_coords,print_dist=True)\n",
    "pe = make_pe_encoding(n_nodes=65, embed_dim = 12, scale = 10, print_out=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cc4e2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_relative_pos(graph_in: dgl.DGLGraph) -> torch.Tensor:\n",
    "    x = graph_in.ndata['pos']\n",
    "    src, dst = graph_in.edges()\n",
    "    rel_pos = x[dst] - x[src]\n",
    "    return rel_pos\n",
    "\n",
    "def torch_normalize(v, eps=1e-6):\n",
    "    \"\"\"Normalize vector in last axis\"\"\"\n",
    "    norm = torch.linalg.vector_norm(v, dim=len(v.shape)-1)+eps\n",
    "    return v / norm[...,None]\n",
    "\n",
    "def normalize(v):\n",
    "    \"\"\"Normalize vector in last axis\"\"\"\n",
    "    norm = np.linalg.norm(v,axis=len(v.shape)-1)\n",
    "    norm[norm == 0] = 1\n",
    "    return v / norm[...,None]\n",
    "\n",
    "class Helix4_Dataset(Dataset):\n",
    "    def __init__(self, coordinates: np.array, cast_type=torch.float32):\n",
    "        #prots,#length_prot in aa, #residues/aa, #xyz per atom\n",
    "           \n",
    "        #alphaFold reduce by 10\n",
    "        coord_div = 10\n",
    "        \n",
    "        coordinates = coordinates/coord_div\n",
    "        self.ca_coords = torch.tensor(coordinates[:,:,CA,:], dtype=cast_type)\n",
    "        #unsqueeze to stack together later\n",
    "        self.N_CA_vec = torch.tensor(coordinates[:,:,N,:] - coordinates[:,:,CA,:], dtype=cast_type)\n",
    "        self.C_CA_vec = torch.tensor(coordinates[:,:,C,:] - coordinates[:,:,CA,:], dtype=cast_type)\n",
    "        \n",
    "        self.N_CA_vec = torch_normalize(self.N_CA_vec).unsqueeze(2)\n",
    "        self.C_CA_vec = torch_normalize(self.C_CA_vec).unsqueeze(2)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ca_coords)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {'CA':self.ca_coords[idx], 'N_CA':self.N_CA_vec[idx], 'C_CA':self.C_CA_vec[idx]}\n",
    "\n",
    "    \n",
    "class Make_KNN_MP_Graphs():\n",
    "    \n",
    "    #8 long positional encoding\n",
    "    NODE_FEATURE_DIM_0 = 12\n",
    "    EDGE_FEATURE_DIM = 1 # 0 or 1 primary seq connection or not\n",
    "    NODE_FEATURE_DIM_1 = 2\n",
    "    \n",
    "    def __init__(self, mp_stride=4, n_nodes=65, radius=15, coord_div=10, cast_type=torch.float32, channels_start=33,\n",
    "                       ndf1=6, ndf0=32,cuda=True):\n",
    "        \n",
    "        self.KNN = 30\n",
    "        self.n_nodes = n_nodes\n",
    "        self.pe = make_pe_encoding(n_nodes=n_nodes)\n",
    "        self.mp_stride = mp_stride\n",
    "        self.cast_type = cast_type\n",
    "        self.channels_start = channels_start\n",
    "        \n",
    "        self.cuda = cuda\n",
    "        self.ndf1 = ndf1 #awkard adding of nodes features to mpGraph\n",
    "        self.ndf0 = ndf0\n",
    "        \n",
    "        #record to get size of tensor for t_value insertion\n",
    "        self.n_camp_nodes=self.n_nodes\n",
    "        for x in range(0,self.n_nodes, self.mp_stride):\n",
    "            self.n_camp_nodes += 1\n",
    "        \n",
    "    def create_and_batch(self, bb_dict):\n",
    "        \n",
    "        graphList = []\n",
    "        mpGraphList = []\n",
    "        mpRevGraphList = []\n",
    "        mpSelfGraphList = []\n",
    "        \n",
    "        for j, caXYZ in enumerate(bb_dict['CA']):\n",
    "            graph = dgl.knn_graph(caXYZ, self.KNN)\n",
    "            graph.ndata['pe'] = pe\n",
    "            graph.ndata['pos'] = caXYZ\n",
    "            graph.ndata['bb_ori'] = torch.cat((bb_dict['N_CA'][j],  bb_dict['C_CA'][j]),axis=1)\n",
    "            \n",
    "            #define covalent connections\n",
    "            esrc, edst = graph.edges()\n",
    "            graph.edata['con'] = (torch.abs(esrc-edst)==1).type(self.cast_type).reshape((-1,1))\n",
    "            \n",
    "            mp_list = torch.zeros((len(list(range(0,self.n_nodes, self.mp_stride))),caXYZ.shape[1]))\n",
    "            \n",
    "            new_src = torch.tensor([],dtype=torch.int)\n",
    "            new_dst = torch.tensor([],dtype=torch.int)\n",
    "            \n",
    "            new_src_rev = torch.tensor([], dtype=torch.int)\n",
    "            new_dst_rev = torch.tensor([], dtype=torch.int)\n",
    "           \n",
    "            i=0#mp list counter\n",
    "            for x in range(0,self.n_nodes, self.mp_stride):\n",
    "                src, dst = graph.in_edges(x) #dst repeats x\n",
    "                n_tot = torch.cat((torch.tensor(x).unsqueeze(0),src)) #add x to node list\n",
    "                mp_list[i] = caXYZ[n_tot].sum(axis=0)/n_tot.shape[0]\n",
    "                mp_node = i + graph.num_nodes() #add midpoints nodes at end of graph\n",
    "                #define edges between midpoint nodes and nodes defining midpoint for midpointGraph\n",
    "                \n",
    "                new_src = torch.cat((new_src,n_tot))\n",
    "                new_dst = torch.cat((new_dst,\n",
    "                                     (torch.tensor(mp_node).unsqueeze(0).repeat(n_tot.shape[0]))))\n",
    "                #and reverse graph for coming off\n",
    "                new_src_rev = torch.cat((new_src_rev,\n",
    "                                         (torch.tensor(mp_node).unsqueeze(0).repeat(n_tot.shape[0]))))\n",
    "                new_dst_rev = torch.cat((new_dst_rev,n_tot))\n",
    "                \n",
    "                i+=1\n",
    "                \n",
    "            mpGraph = dgl.graph((new_src,new_dst))\n",
    "            mpGraph.ndata['pos'] = torch.cat((caXYZ,mp_list),axis=0).type(self.cast_type)\n",
    "            mp_node_indx = torch.arange(0,self.n_nodes, self.mp_stride).type(torch.int)\n",
    "            #match output shape of first transformer\n",
    "            pe_mp = torch.cat((pe,torch.zeros((pe.shape[0],self.channels_start-pe.shape[1]))),axis=1)\n",
    "            mpGraph.ndata['pe'] = torch.cat((pe_mp,pe_mp[mp_node_indx]))\n",
    "            mpGraph.edata['con'] = torch.zeros((mpGraph.num_edges(),1))\n",
    "            \n",
    "            mpGraph_rev = dgl.graph((new_src_rev,new_dst_rev))\n",
    "            mpGraph_rev.ndata['pos'] = torch.cat((caXYZ,mp_list),axis=0).type(self.cast_type)\n",
    "            mpGraph_rev.ndata['pe'] = torch.cat((pe_mp,pe_mp[mp_node_indx]))\n",
    "            mpGraph_rev.edata['con'] = torch.zeros((mpGraph_rev.num_edges(),1))\n",
    "            \n",
    "            #make graph for self interaction of midpoints\n",
    "            v1,v2,edge_data, ind = define_graph_edges(len(mp_list))\n",
    "            mpSelfGraph = dgl.graph((v1,v2))\n",
    "            mpSelfGraph.edata['con'] = edge_data.reshape((-1,1))\n",
    "            mpSelfGraph.ndata['pe'] = pe[mp_node_indx] #not really needed\n",
    "            mpSelfGraph.ndata['pos'] = mp_list.type(self.cast_type)\n",
    "            \n",
    "            \n",
    "            mpSelfGraphList.append(mpSelfGraph) \n",
    "            mpGraphList.append(mpGraph)\n",
    "            mpRevGraphList.append(mpGraph_rev)\n",
    "            graphList.append(graph)\n",
    "        \n",
    "        return dgl.batch(graphList), dgl.batch(mpGraphList), dgl.batch(mpSelfGraphList), dgl.batch(mpRevGraphList)\n",
    "    \n",
    "#     def prep_for_network(self, bb_dict, cuda=True):\n",
    "    \n",
    "#         batched_graph, batched_mpgraph, batched_mpself_graph, batched_mpRevgraph =  self.create_and_batch(bb_dict)\n",
    "        \n",
    "#         edge_feats        =    {'0':   batched_graph.edata['con'][:, :self.EDGE_FEATURE_DIM, None]}\n",
    "#         edge_feats_mp     = {'0': batched_mpgraph.edata['con'][:, :self.EDGE_FEATURE_DIM, None]} #def all zero now\n",
    "#         edge_feats_mpself = {'0': batched_mpself_graph.edata['con'][:, :self.EDGE_FEATURE_DIM, None]}\n",
    "# #         edge_feats_mp     = {'0': batched_mpRevgraph.edata['con'][:, :self.EDGE_FEATURE_DIM, None]}\n",
    "#         batched_graph.edata['rel_pos']   = _get_relative_pos(batched_graph)\n",
    "#         batched_mpgraph.edata['rel_pos'] = _get_relative_pos(batched_mpgraph)\n",
    "#         batched_mpself_graph.edata['rel_pos'] = _get_relative_pos(batched_mpself_graph)\n",
    "#         batched_mpRevgraph.edata['rel_pos'] = _get_relative_pos(batched_mpRevgraph)\n",
    "#         # get node features\n",
    "#         node_feats =         {'0': batched_graph.ndata['pe'][:, :self.NODE_FEATURE_DIM_0, None],\n",
    "#                               '1': batched_graph.ndata['bb_ori'][:,:self.NODE_FEATURE_DIM_1, :3]}\n",
    "#         node_feats_mp =      {'0': batched_mpgraph.ndata['pe'][:, :self.ndf0, None],\n",
    "#                               '1': torch.ones((batched_mpgraph.num_nodes(),self.ndf1,3))}\n",
    "#         #unused\n",
    "#         node_feats_mpself =  {'0': batched_mpself_graph.ndata['pe'][:, :self.NODE_FEATURE_DIM_0, None]}\n",
    "        \n",
    "#         if cuda:\n",
    "#             bg,nf,ef = to_cuda(batched_graph), to_cuda(node_feats), to_cuda(edge_feats)\n",
    "#             bg_mp, nf_mp, ef_mp = to_cuda(batched_mpgraph), to_cuda(node_feats_mp), to_cuda(edge_feats_mp)\n",
    "#             bg_mps, nf_mps, ef_mps = to_cuda(batched_mpself_graph), to_cuda(node_feats_mpself), to_cuda(edge_feats_mpself)\n",
    "#             bg_mpRev = to_cuda(batched_mpRevgraph)\n",
    "            \n",
    "#             return bg,nf,ef, bg_mp, nf_mp, ef_mp, bg_mps, nf_mps, ef_mps, bg_mpRev\n",
    "        \n",
    "#         else:\n",
    "#             bg,nf,ef = batched_graph, node_feats, edge_feats\n",
    "#             bg_mp, nf_mp, ef_mp = batched_mpgraph, node_feats_mp, edge_feats_mp\n",
    "#             bg_mps, nf_mps, ef_mps = batched_mpself_graph, node_feats_mpself, edge_feats_mpself\n",
    "#             bg_mpRev = batched_mpRevgraph\n",
    "            \n",
    "#             return bg,nf,ef, bg_mp, nf_mp, ef_mp, bg_mps, nf_mps, ef_mps, bg_mpRev \n",
    "    def prep_for_network(self, bb_dict, t_vec, cuda=True):\n",
    "    \n",
    "        batched_graph, batched_mpgraph, batched_mpself_graph, batched_mpRevgraph =  self.create_and_batch(bb_dict)\n",
    "        \n",
    "        edge_feats        =    {'0':   batched_graph.edata['con'][:, :self.EDGE_FEATURE_DIM, None]}\n",
    "        edge_feats_mp     = {'0': batched_mpgraph.edata['con'][:, :self.EDGE_FEATURE_DIM, None]} #def all zero now\n",
    "        edge_feats_mpself = {'0': batched_mpself_graph.edata['con'][:, :self.EDGE_FEATURE_DIM, None]}\n",
    "        #edge_feats_mp     = {'0': batched_mpRevgraph.edata['con'][:, :self.EDGE_FEATURE_DIM, None]}\n",
    "        batched_graph.edata['rel_pos']   = _get_relative_pos(batched_graph)\n",
    "        batched_mpgraph.edata['rel_pos'] = _get_relative_pos(batched_mpgraph)\n",
    "        batched_mpself_graph.edata['rel_pos'] = _get_relative_pos(batched_mpself_graph)\n",
    "        batched_mpRevgraph.edata['rel_pos'] = _get_relative_pos(batched_mpRevgraph)\n",
    "        \n",
    "        # get node features, concatenate T\n",
    "        t_ca = t_vec[:,None].repeat_interleave(gm.n_nodes,dim=1).flatten() \n",
    "        t_camp = t_vec[:,None].repeat_interleave(gm.n_camp_nodes,dim=1).flatten() \n",
    "        \n",
    "        nf_cat_t = torch.cat((batched_graph.ndata['pe'], t_ca.unsqueeze(1)),dim=1)\n",
    "        node_feats =         {'0': nf_cat_t[:, :self.NODE_FEATURE_DIM_0+1, None],\n",
    "                              '1': batched_graph.ndata['bb_ori'][:,:self.NODE_FEATURE_DIM_1, :3]}\n",
    "        \n",
    "        nfmp_cat_t = torch.cat((batched_mpgraph.ndata['pe'], t_camp.unsqueeze(1)),dim=1)\n",
    "        node_feats_mp =      {'0': nfmp_cat_t[:, :self.ndf0+1, None],\n",
    "                              '1': torch.ones((batched_mpgraph.num_nodes(),self.ndf1,3))}\n",
    "        #unused\n",
    "        node_feats_mpself =  {'0': batched_mpself_graph.ndata['pe'][:, :self.NODE_FEATURE_DIM_0, None]}\n",
    "        \n",
    "        if cuda:\n",
    "            bg,nf,ef = to_cuda(batched_graph), to_cuda(node_feats), to_cuda(edge_feats)\n",
    "            bg_mp, nf_mp, ef_mp = to_cuda(batched_mpgraph), to_cuda(node_feats_mp), to_cuda(edge_feats_mp)\n",
    "            bg_mps, nf_mps, ef_mps = to_cuda(batched_mpself_graph), to_cuda(node_feats_mpself), to_cuda(edge_feats_mpself)\n",
    "            bg_mpRev = to_cuda(batched_mpRevgraph)\n",
    "            \n",
    "            return bg,nf,ef, bg_mp, nf_mp, ef_mp, bg_mps, nf_mps, ef_mps, bg_mpRev\n",
    "        \n",
    "        else:\n",
    "            bg,nf,ef = batched_graph, node_feats, edge_feats\n",
    "            bg_mp, nf_mp, ef_mp = batched_mpgraph, node_feats_mp, edge_feats_mp\n",
    "            bg_mps, nf_mps, ef_mps = batched_mpself_graph, node_feats_mpself, edge_feats_mpself\n",
    "            bg_mpRev = batched_mpRevgraph\n",
    "            \n",
    "            return bg,nf,ef, bg_mp, nf_mp, ef_mp, bg_mps, nf_mps, ef_mps, bg_mpRev\n",
    "        \n",
    "            \n",
    "\n",
    "def get_edge_features(graph,edge_feature_dim=1):\n",
    "    return {'0': graph.edata['con'][:, :edge_feature_dim, None]}\n",
    "\n",
    "def define_poolGraph(n_nodes, batch_size, cast_type=torch.float32, cuda_out=True ):\n",
    "    \n",
    "    v1,v2,edge_data, ind = define_graph_edges(n_nodes)\n",
    "    #pe = make_pe_encoding(n_nodes=n_nodes)#pe e\n",
    "    \n",
    "    graphList = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        \n",
    "        g = dgl.graph((v1,v2))\n",
    "        g.edata['con'] = edge_data.type(cast_type).reshape((-1,1))\n",
    "        g.ndata['pos'] = torch.zeros((n_nodes,3),dtype=torch.float32)\n",
    "\n",
    "        graphList.append(g)\n",
    "        \n",
    "    batched_graph = dgl.batch(graphList)\n",
    "    \n",
    "    if cuda_out:\n",
    "        return to_cuda(batched_graph)\n",
    "    else:\n",
    "        return batched_graph            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2f09455",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_edge_features(graph, edge_feat_dim=1):\n",
    "    return {'0': graph.edata['con'][:, :edge_feat_dim, None]}\n",
    "\n",
    "def prep_for_gcn(graph, xyz_pos, edge_feats_input, idx, max_degree=3, comp_grad=True):\n",
    "    \n",
    "    src, dst = graph.edges()\n",
    "    \n",
    "    new_pos = F.gather_row(xyz_pos, idx)\n",
    "    rel_pos = F.gather_row(new_pos,dst) - F.gather_row(new_pos,src) \n",
    "    \n",
    "    basis_out = get_basis(rel_pos, max_degree=max_degree,\n",
    "                                   compute_gradients=comp_grad,\n",
    "                                   use_pad_trick=False)\n",
    "    basis_out = update_basis_with_fused(basis_out, max_degree, use_pad_trick=False,\n",
    "                                            fully_fused=False)\n",
    "    edge_feats_out = get_populated_edge_features(rel_pos, edge_feats_input)\n",
    "    return edge_feats_out, basis_out, new_pos\n",
    "\n",
    "def gs(dict_in):\n",
    "    str_out = ''\n",
    "    for x in dict_in.keys():\n",
    "        sh = dict_in[x].shape\n",
    "        str_out = f'{str_out}{x}: {sh}   '\n",
    "    print(str_out)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "401432e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphUNet(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 fiber_start = Fiber({0:13, 1:2}),\n",
    "                 fiber_out = Fiber({1:2}),\n",
    "                 k=4,\n",
    "                 batch_size = 8,\n",
    "                 stride=4,\n",
    "                 max_degree=3,\n",
    "                 channels=32,\n",
    "                 num_heads = 8,\n",
    "                 channels_div=4,\n",
    "                 num_layers = 1,\n",
    "                 num_layers_ca =2,\n",
    "                 edge_feature_dim=1,\n",
    "                 latent_pool_type = 'avg',\n",
    "                 cuda=True):\n",
    "        super(GraphUNet, self).__init__()\n",
    "        \n",
    "        ##to do pass precalculated basis\n",
    "        \n",
    "        self.comp_basis_grad = True\n",
    "        self.cuda = cuda\n",
    "        \n",
    "        if cuda:\n",
    "            self.device='cuda:0'\n",
    "        else:\n",
    "            self.device='cpu'\n",
    "        \n",
    "        self.max_degree=max_degree\n",
    "        self.B = batch_size\n",
    "        self.k = k\n",
    "        \n",
    "        self.num_layers = 1\n",
    "        self.num_layers_ca = num_layers_ca\n",
    "        self.channels = 32\n",
    "        self.feat0 = 33 #add one for t\n",
    "        self.feat1 = 6\n",
    "        self.channels_div = 4\n",
    "        self.num_heads = 8\n",
    "        self.mult = int(stride/2)\n",
    "        self.fiber_edge=Fiber({0:edge_feature_dim})\n",
    "        self.edge_feat_dim = edge_feature_dim\n",
    "        \n",
    "        self.pool_type = latent_pool_type\n",
    "        \n",
    "        self.channels_down_ca = channels\n",
    "        #down c_alpha interactions by radius\n",
    "        self.fiber_start =  fiber_start\n",
    "        self.fiber_hidden_down_ca = Fiber.create(self.max_degree, self.channels_down_ca)\n",
    "        self.fiber_out_down_ca =Fiber({0: self.feat0, 1: self.feat1})\n",
    "        \n",
    "        self.down_ca = SE3Transformer(num_layers = self.num_layers_ca,\n",
    "                        fiber_in=self.fiber_start,\n",
    "                        fiber_hidden= self.fiber_hidden_down_ca, \n",
    "                        fiber_out=self.fiber_out_down_ca,\n",
    "                        num_heads = self.num_heads,\n",
    "                        channels_div = self.channels_div,\n",
    "                        fiber_edge=self.fiber_edge,\n",
    "                        low_memory=True,\n",
    "                        tensor_cores=False)\n",
    "        \n",
    "        self.channels_down_ca2mp = self.channels_down_ca*self.mult\n",
    "        \n",
    "        #pool from c_alpha onto midpoints\n",
    "        self.fiber_in_down_ca2mp     = self.fiber_out_down_ca\n",
    "        self.fiber_hidden_down_ca2mp = Fiber.create(max_degree, self.channels_down_ca2mp)\n",
    "        self.fiber_out_down_ca2mp    = Fiber({0: self.feat0*self.mult, 1: self.feat1*self.mult})\n",
    "\n",
    "        self.down_ca2mp = SE3Transformer(num_layers = self.num_layers,\n",
    "                            fiber_in     = self.fiber_in_down_ca2mp,\n",
    "                            fiber_hidden = self.fiber_hidden_down_ca2mp, \n",
    "                            fiber_out    = self.fiber_out_down_ca2mp,\n",
    "                            num_heads =    self.num_heads,\n",
    "                            channels_div = self.channels_div,\n",
    "                            fiber_edge=self. fiber_edge,\n",
    "                            low_memory=True,\n",
    "                            tensor_cores=False)\n",
    "        \n",
    "        self.fiber_in_mptopk =  self.fiber_out_down_ca2mp\n",
    "        self.fiber_hidden_down_mp  =self.fiber_hidden_down_ca2mp\n",
    "        self.fiber_out_down_mp_out =self.fiber_out_down_ca2mp\n",
    "        self.fiber_out_topkpool=Fiber({0: self.feat0*self.mult*self.mult})\n",
    "\n",
    "        self.mp_topk = SE3Transformer_topK(num_layers      = self.num_layers,\n",
    "                                        fiber_in      = self.fiber_in_mptopk,\n",
    "                                        fiber_hidden  = self.fiber_hidden_down_mp, \n",
    "                                        fiber_out     = self.fiber_out_down_mp_out ,\n",
    "                                        fiber_out_topk= self.fiber_out_topkpool,\n",
    "                                        k             = self.k,\n",
    "                                        num_heads     = self.num_heads,\n",
    "                                        channels_div  = self.channels_div,\n",
    "                                        fiber_edge    =  self.fiber_edge,\n",
    "                                        low_memory=True,\n",
    "                                        tensor_cores=False)\n",
    "        \n",
    "        self.gsmall = define_poolGraph(self.k, self.B, cast_type=torch.float32, cuda_out=self.cuda)\n",
    "        self.ef_small = pull_edge_features(self.gsmall, edge_feat_dim=self.edge_feat_dim)\n",
    "        \n",
    "        #change to doing convolutions instead of points\n",
    "        self.fiber_in_down_gcn   =  self.fiber_out_topkpool\n",
    "        self.fiber_out_down_gcn  = Fiber({0: self.feat0*self.mult*self.mult, 1: self.feat1*self.mult})\n",
    "\n",
    "        self.down_gcn = ConvSE3(fiber_in  = self.fiber_in_down_gcn,\n",
    "                           fiber_out = self.fiber_out_down_gcn,\n",
    "                           fiber_edge= self.fiber_edge,\n",
    "                             self_interaction=True,\n",
    "                             use_layer_norm=True,\n",
    "                             max_degree=self.max_degree,\n",
    "                             fuse_level= ConvSE3FuseLevel.NONE,\n",
    "                             low_memory= True)\n",
    "        \n",
    "        \n",
    "        self.fiber_in_down_gcnp = self.fiber_out_down_gcn\n",
    "        #probably rename latent\n",
    "        self.latent_size = self.feat0*self.mult*self.mult\n",
    "        self.fiber_latent = Fiber({0: self.latent_size})\n",
    "\n",
    "        self.down_gcn2pool = ConvSE3(fiber_in=self.fiber_in_down_gcnp,\n",
    "                                     fiber_out=self.fiber_latent,\n",
    "                                     fiber_edge=self.fiber_edge,\n",
    "                                     self_interaction=True,\n",
    "                                     use_layer_norm=True,\n",
    "                                     max_degree=self.max_degree,\n",
    "                                     fuse_level= ConvSE3FuseLevel.NONE,\n",
    "                                     low_memory= True)\n",
    "        \n",
    "        self.global_pool = GPooling(pool=self.pool_type, feat_type=0)\n",
    "\n",
    "        self.latent_unpool_layer = Latent_Unpool(fiber_in = self.fiber_latent, fiber_add = self.fiber_out_down_gcn, \n",
    "                                            knodes = self.k)\n",
    "\n",
    "        self.up_gcn = ConvSE3(fiber_in=self.fiber_out_down_gcn,\n",
    "                             fiber_out=self.fiber_out_down_gcn,\n",
    "                             fiber_edge=self.fiber_edge,\n",
    "                             self_interaction=True,\n",
    "                             use_layer_norm=True,\n",
    "                             max_degree=self.max_degree,\n",
    "                             fuse_level= ConvSE3FuseLevel.NONE,\n",
    "                             low_memory= True)\n",
    "        \n",
    "        self.unpool_layer = Unpool_Layer(fiber_in=self.fiber_out_down_gcn, fiber_add=self.fiber_out_down_ca)\n",
    "        \n",
    "        self.fiber_in_up_gcn_mp = self.unpool_layer.fiber_out\n",
    "        self.fiber_hidden_up_mp= self.fiber_hidden_down_ca2mp\n",
    "        self.fiber_out_up_gcn_mp = self.fiber_out_down_mp_out\n",
    "\n",
    "        self.up_gcn_mp = SE3Transformer(num_layers = num_layers,\n",
    "                        fiber_in=self.fiber_in_up_gcn_mp,\n",
    "                        fiber_hidden= self.fiber_hidden_up_mp, \n",
    "                        fiber_out=self.fiber_out_up_gcn_mp,\n",
    "                        num_heads = self.num_heads,\n",
    "                        channels_div = self.channels_div,\n",
    "                        fiber_edge=self.fiber_edge,\n",
    "                        low_memory=True,\n",
    "                        tensor_cores=False)\n",
    "        \n",
    "        self.unpool_layer_off_mp = Unpool_Layer(fiber_in=self.fiber_out_down_mp_out, fiber_add=self.fiber_out_down_mp_out)\n",
    "\n",
    "        self.fiber_in_up_off_mp = self.fiber_out_up_gcn_mp\n",
    "        self.fiber_hidden_up_off_mp = self.fiber_hidden_up_mp\n",
    "        self.fiber_out_up_off_mp = self.fiber_out_down_ca \n",
    "        \n",
    "        #uses reverse graph to move mp off \n",
    "        \n",
    "        self.up_off_mp = SE3Transformer(num_layers = self.num_layers,\n",
    "                        fiber_in=self.fiber_in_up_off_mp,\n",
    "                        fiber_hidden= self.fiber_hidden_up_off_mp, \n",
    "                        fiber_out=self.fiber_out_up_off_mp,\n",
    "                        num_heads = self.num_heads,\n",
    "                        channels_div = self.channels_div,\n",
    "                        fiber_edge=self.fiber_edge,\n",
    "                        low_memory=True,\n",
    "                        tensor_cores=False)\n",
    "        \n",
    "        self.fiber_out = fiber_out\n",
    "        \n",
    "        self.up_ca = SE3Transformer(num_layers =self.num_layers_ca,\n",
    "                                    fiber_in=self.fiber_out_down_ca,\n",
    "                                    fiber_hidden= self.fiber_hidden_down_ca, \n",
    "                                    fiber_out=self.fiber_out,\n",
    "                                    num_heads = self.num_heads,\n",
    "                                    channels_div = self.channels_div,\n",
    "                                    fiber_edge= self.fiber_edge,\n",
    "                                    low_memory=True,\n",
    "                                    tensor_cores=False)\n",
    "        \n",
    "        self.linear = LinearSE3(fiber_in=fiber_out,\n",
    "                                fiber_out=fiber_out)\n",
    "        \n",
    "        self.zero_linear()\n",
    "        \n",
    "    def zero_linear(self):\n",
    "        nn.init.zeros_(self.linear.weights['1'])\n",
    "        \n",
    "    def concat_mp_feats(self, ca_feats_in, mp_feats):\n",
    "\n",
    "        nf0_c = ca_feats_in['0'].shape[-2]\n",
    "        nf1_c = ca_feats_in['1'].shape[-2]\n",
    "\n",
    "        out0_cat_shape = (B,self.ca_nodes,-1,1)\n",
    "        mp0_cat_shape  = (B,self.mp_nodes,-1,1)\n",
    "        out1_cat_shape = (B,self.ca_nodes,-1,3)\n",
    "        mp1_cat_shape  = (B,self.mp_nodes,-1,3)\n",
    "\n",
    "        nf_c = {} #nf_cat\n",
    "        nf_c['0'] = torch.cat((ca_feats_in['0'].reshape(out0_cat_shape), \n",
    "                                 mp_feats['0'].reshape(mp0_cat_shape)[:,-(self.mp_nodes-self.ca_nodes):,:,:]),\n",
    "                              axis=1).reshape((-1,nf0_c,1))\n",
    "\n",
    "        nf_c['1'] = torch.cat((ca_feats_in['1'].reshape(out1_cat_shape), \n",
    "                                 mp_feats['1'].reshape(mp1_cat_shape)[:,-(self.mp_nodes-self.ca_nodes):,:,:]),\n",
    "                              axis=1).reshape((-1,nf1_c,3))\n",
    "\n",
    "        return nf_c\n",
    "        \n",
    "    def pull_out_mp_feats(self, ca_mp_feats):\n",
    "\n",
    "        nf0_c = ca_mp_feats['0'].shape[1]\n",
    "        nf1_c = ca_mp_feats['1'].shape[1]\n",
    "\n",
    "        nf_mp_ = {}\n",
    "        #select just mp nodes to move on, the other nodes don't connect but mainting self connections\n",
    "        nf_mp_['0'] = ca_mp_feats['0'].reshape(B,self.mp_nodes,\n",
    "                                               nf0_c,1)[:,-(self.mp_nodes-self.ca_nodes):,...].reshape((-1,nf0_c,1))\n",
    "        nf_mp_['1'] = ca_mp_feats['1'].reshape(B,self.mp_nodes,\n",
    "                                               nf1_c,3)[:,-(self.mp_nodes-self.ca_nodes):,...].reshape((-1,nf1_c,3))\n",
    "\n",
    "        return nf_mp_\n",
    "    \n",
    "    def print_parameters(self):\n",
    "        \n",
    "        print('fiber_start:'        , self.fiber_start)\n",
    "        print('fiber_hidden_down_ca', self.fiber_hidden_down_ca)\n",
    "        print('fiber_out_down_ca'   , self.fiber_out_down_ca)\n",
    "        \n",
    "        print('down_ca')\n",
    "        \n",
    "        print('fiber_in_down_ca2mp'   , self.fiber_in_down_ca2mp)\n",
    "        print('fiber_hidden_down_ca2mp', self.fiber_hidden_down_ca2mp)\n",
    "        print('fiber_out_down_ca2mp'   , self.fiber_out_down_ca2mp )\n",
    "        \n",
    "        print('down_ca2mp')\n",
    "        \n",
    "        \n",
    "        print('fiber_in_mptopk'   , self.fiber_in_mptopk)\n",
    "        print('fiber_hidden_down_mp', self.fiber_hidden_down_mp)\n",
    "        print('fiber_out_down_mp_out'   , self.fiber_out_down_mp_out)\n",
    "        print('fiber_out_topkpool'   , self.fiber_out_topkpool)\n",
    "        \n",
    "        print('mp_topk')\n",
    "        \n",
    "        print(f'k is {self.k}, make gsmall')\n",
    "        \n",
    "        print('fiber_in_down_gcn'   , self.fiber_in_down_gcn)\n",
    "        print('fiber_out_down_gcn', self.fiber_out_down_gcn)\n",
    "\n",
    "        print('down_gcn')\n",
    "        \n",
    "        \n",
    "        print('fiber_in_down_gcnp'   ,  self.fiber_in_down_gcnp)\n",
    "        print('fiber_latent', self.fiber_latent)\n",
    "\n",
    "        print('down_gcn2pool')\n",
    "        print('global_pool')\n",
    "        print('latent_unpool_layer')\n",
    "        \n",
    "        print('fiber_in_up_gcn',self.fiber_out_down_gcn )\n",
    "        print('fiber_out_up_gcn',self.fiber_out_down_gcn )\n",
    "        print('up_gcn')\n",
    "\n",
    "        print('unpool_layer_in',self.fiber_out_down_gcn)\n",
    "        print('unpool_layer_add',self.fiber_out_down_gcn)\n",
    "        print('unpool_layer_out',self.unpool_layer.fiber_out)\n",
    "        print('unpool')\n",
    "        \n",
    "        \n",
    "        print('fiber_in_up_gcn_mp'   , self.fiber_in_up_gcn_mp)\n",
    "        print('fiber_hidden_up_mp'   , self.fiber_hidden_up_mp)\n",
    "        print('fiber_out_up_gcn_mp'   , self.fiber_out_up_gcn_mp )\n",
    "        \n",
    "        print('up_gcn_mp')\n",
    "        \n",
    "        \n",
    "\n",
    "        print('unpool_layer_off_mp_in', self.fiber_out_down_mp_out)\n",
    "        print('unpool_layer_off_mp_add',self.fiber_out_down_mp_out)\n",
    "        \n",
    "        self.unpool_layer_off_mp = Unpool_Layer(fiber_in=self.fiber_out_down_mp_out, fiber_add=self.fiber_out_down_mp_out)\n",
    "\n",
    "        self.fiber_in_up_off_mp = self.fiber_out_up_gcn_mp\n",
    "        self.fiber_hidden_up_off_mp = self.fiber_hidden_up_mp\n",
    "        self.fiber_out_up_off_mp = self.fiber_out_down_ca \n",
    "        \n",
    "        print('fiber_in_up_off_mp'   , self.fiber_in_up_off_mp)\n",
    "        print('fiber_hidden_up_off_mp'   , self.fiber_hidden_up_off_mp)\n",
    "        print('fiber_out_up_off_mp'   , self.fiber_out_up_off_mp )\n",
    "        \n",
    "        print('up_off_mp')\n",
    "        \n",
    "        \n",
    "        \n",
    "        print('fiber_in_up_ca'   , self.fiber_out_down_ca)\n",
    "        print('fiber_hidden_up_ca'   , self.fiber_hidden_down_ca)\n",
    "        print('fiber_out'   , self.fiber_out )\n",
    "        \n",
    "        print('linear final')\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, input_tuple):\n",
    "\n",
    "        b_graph, nf, ef, b_graph_mp, nf_mp, ef_mp, b_graph_mps, nf_mps, ef_mps, b_graph_mpRev = input_tuple\n",
    "        #assumes equal node numbers in g raphs\n",
    "        self.ca_nodes = int(b_graph.batch_num_nodes()[0])\n",
    "        self.mp_nodes = int(b_graph_mp.batch_num_nodes()[0]) #ca+mp nodes number\n",
    "\n",
    "        #SE3 Attention Transformer, c_alpha \n",
    "        nf_ca_down_out = self.down_ca(b_graph, nf, ef)\n",
    "\n",
    "        #concatenate on midpoints feats\n",
    "        nf_down_cat_mp = self.concat_mp_feats(nf_ca_down_out, nf_mp)\n",
    "\n",
    "        #pool from ca onto selected midpoints via SE3 Attention transformer\n",
    "        #edges from ca to mp only (ca nodes zero after this)\n",
    "        nf_down_ca2mp_out = self.down_ca2mp(b_graph_mp, nf_down_cat_mp, ef_mp)\n",
    "\n",
    "        #remove ca node feats from tensor \n",
    "        nf_mp_out = self.pull_out_mp_feats(nf_down_ca2mp_out)\n",
    "\n",
    "        node_feats_tk, topk_feats, topk_indx = self.mp_topk(b_graph_mps, nf_mp_out, ef_mps)\n",
    "\n",
    "        #make new basis for small graph of k selected midpoints\n",
    "        edge_feats_out, basis_out, new_pos = prep_for_gcn(self.gsmall, b_graph_mps.ndata['pos'], self.ef_small,\n",
    "                                                          topk_indx,\n",
    "                                                          max_degree=self.max_degree, comp_grad=True)\n",
    "\n",
    "        down_gcn_out = self.down_gcn(topk_feats, edge_feats_out, self.gsmall,  basis_out)\n",
    "\n",
    "        down_gcnpool_out = self.down_gcn2pool(down_gcn_out, edge_feats_out, self.gsmall,  basis_out)\n",
    "\n",
    "        pooled_tensor = self.global_pool(down_gcnpool_out,self.gsmall)\n",
    "        pooled = {'0':pooled_tensor}\n",
    "        #----------------------------------------- end of down section\n",
    "        lat_unp = self.latent_unpool_layer(pooled,down_gcn_out)\n",
    "\n",
    "        up_gcn_out = self.up_gcn(lat_unp, edge_feats_out, self.gsmall,  basis_out)\n",
    "\n",
    "        k_to_mp  = self.unpool_layer(up_gcn_out,node_feats_tk,topk_indx)\n",
    "\n",
    "        up_mp_gcn_out = self.up_gcn_mp(b_graph_mps, k_to_mp, ef_mps)\n",
    "        \n",
    "        off_mp_add = {}\n",
    "        for k,v in up_mp_gcn_out.items():\n",
    "            off_mp_add[k] = torch.add(up_mp_gcn_out[k],nf_mp_out[k])\n",
    "\n",
    "\n",
    "        #####triple check from here\n",
    "        #midpoints node indices for unpool layer\n",
    "        mp_node_indx = torch.arange(self.ca_nodes,self.mp_nodes, device=self.device)\n",
    "        mp_idx = mp_node_indx[None,...].repeat_interleave(self.B,0)\n",
    "        mp_idx =((torch.arange(self.B,device=self.device)*(self.mp_nodes)).reshape((-1,1))+mp_idx).reshape((-1))\n",
    "        \n",
    "        #during unpool, keep mp=values and ca=zeros\n",
    "        zeros_mp_ca = {}\n",
    "        for k,v in nf_down_cat_mp.items():\n",
    "            zeros_mp_ca[k] = torch.zeros_like(v, device=self.device)\n",
    "\n",
    "\n",
    "        unpoff_out = self.unpool_layer_off_mp(off_mp_add, zeros_mp_ca, mp_idx)\n",
    "        \n",
    "        out_up_off_mp = self.up_off_mp(b_graph_mpRev, unpoff_out, ef_mp)\n",
    "        \n",
    "        #select just ca nodes, mp = zeros from last convolution\n",
    "        inv_mp_idx= torch.arange(0,self.ca_nodes, device=self.device)\n",
    "        inv_mp_idx = inv_mp_idx[None,...].repeat_interleave(self.B,0)\n",
    "        inv_mp_idx =((torch.arange(self.B,device=self.device)*(self.mp_nodes)).reshape((-1,1))\n",
    "                     +inv_mp_idx).reshape((-1))\n",
    "\n",
    "        node_final_ca = {}\n",
    "        for key in out_up_off_mp.keys():\n",
    "            node_final_ca[key] = torch.add(out_up_off_mp[key][inv_mp_idx,...],nf_ca_down_out[key])\n",
    "\n",
    "        #return updates \n",
    "        return self.linear(self.up_ca(b_graph, node_final_ca, ef))\n",
    "                \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5a815da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_dL_output(backbone_dict, noised_dict, t_vec, graph_maker):\n",
    "    \"Convert from dataloader to graph and noise input for diffusion model.\"\n",
    "    \n",
    "    CA_t  = backbone_dict['CA'].reshape(B, L, 3).to('cuda')\n",
    "    NC_t = CA_t + backbone_dict['N_CA'].reshape(B, L, 3).to('cuda')*N_CA_dist\n",
    "    CC_t = CA_t + backbone_dict['C_CA'].reshape(B, L, 3).to('cuda')*C_CA_dist\n",
    "    true =  torch.cat((NC_t,CA_t,CC_t),dim=2).reshape(B,L,3,3)\n",
    "    \n",
    "    CA_n  = noised_dict['CA'].reshape(B, L, 3).to('cuda')\n",
    "    NC_n = CA_n + noised_dict['N_CA'].reshape(B, L, 3).to('cuda')*N_CA_dist\n",
    "    CC_n = CA_n + noised_dict['C_CA'].reshape(B, L, 3).to('cuda')*C_CA_dist\n",
    "    noise_xyz =  torch.cat((NC_n,CA_n,CC_n),dim=2).reshape(B,L,3,3)\n",
    "    \n",
    "    to_model = graph_maker.prep_for_network(noised_dict, t_vec)\n",
    "    return to_model, true, noise_xyz\n",
    "    \n",
    "def model_step(backbone_dict, noised_dict, t_vec, score_scales, graph_maker, graph_unet, train=True):\n",
    "    \n",
    "    to_model, true, noise_xyz = prep_dL_output(backbone_dict, noised_dict, t_vec, graph_maker)\n",
    "\n",
    "    out = graph_unet(to_model)\n",
    "    CA_p = out['1'][:,0,:].reshape(B, L, 3)+ noise_xyz[:,:,1,:]#CA_n #translation of Calpha\n",
    "    Qs = out['1'][:,1,:] # rotation\n",
    "    Qs = Qs.unsqueeze(1).repeat((1,2,1))\n",
    "    Qs = torch.cat((torch.ones((B*L,2,1),device=Qs.device),Qs),dim=-1).reshape(B,L,2,4)\n",
    "    Qs = normQ(Qs)\n",
    "    Rs = Qs2Rs(Qs)\n",
    "    N_C_to_Rot = torch.cat((noised_dict['N_CA'].reshape(B, L, 3).to('cuda'),\n",
    "                            noised_dict['C_CA'].reshape(B, L, 3).to('cuda')),dim=2).reshape(B,L,2,1,3)\n",
    "\n",
    "    rot_vecs = einsum('bnkij,bnkhj->bnki',Rs, N_C_to_Rot)\n",
    "    NC_p = CA_p + rot_vecs[:,:,0,:].to('cuda')*N_CA_dist\n",
    "    CC_p = CA_p + rot_vecs[:,:,1,:].reshape(B, L, 3).to('cuda')*C_CA_dist\n",
    "\n",
    "    pred = torch.cat((NC_p,CA_p,CC_p),dim=2).reshape(B,L,3,3)\n",
    "    \n",
    "    tloss, loss = FAPE_loss(pred.unsqueeze(0), true, score_scales)\n",
    "    \n",
    "    return tloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02f23891",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noise_pred_true(backbone_dict, noised_dict, t_vec, graph_maker, graph_unet, B=64):\n",
    "    \n",
    "    num_r = int(B/len(t_vec))\n",
    "\n",
    "    CA_t  = backbone_dict['CA'][0].reshape(1, L, 3).to('cuda')\n",
    "    NC_t = CA_t + backbone_dict['N_CA'][0].reshape(1, L, 3).to('cuda')*N_CA_dist\n",
    "    CC_t = CA_t + backbone_dict['C_CA'][0].reshape(1, L, 3).to('cuda')*C_CA_dist\n",
    "    true =  torch.cat((NC_t,CA_t,CC_t),dim=2).reshape(1,L,3,3).repeat(B,1,1,1)\n",
    "\n",
    "    CA_n  = noised_dict['CA'].repeat(num_r,1,1,1).reshape(B, L, 3).to('cuda')\n",
    "    NC_n = CA_n + noised_dict['N_CA'].repeat(num_r ,1,1,1).reshape(B, L, 3).to('cuda')*N_CA_dist\n",
    "    CC_n = CA_n + noised_dict['C_CA'].repeat(num_r ,1,1,1).reshape(B, L, 3).to('cuda')*C_CA_dist\n",
    "    noise_xyz =  torch.cat((NC_n,CA_n,CC_n),dim=2).reshape(B,L,3,3).repeat(num_r,1,1,1)\n",
    "\n",
    "    b_nd = {}\n",
    "    b_nd['CA'] = CA_n.cpu()\n",
    "    b_nd['N_CA'] = NC_n.reshape(B, L, 1, 3).cpu()\n",
    "    b_nd['C_CA'] = CC_n.reshape(B, L, 1, 3).cpu()\n",
    "\n",
    "    t_vecr = t_vec.repeat(num_r).cpu()\n",
    "    \n",
    "    x = graph_maker.prep_for_network(b_nd, t_vecr)\n",
    "    out = graph_unet(x)\n",
    "    CA_p = out['1'][:,0,:].reshape(B, L, 3)+CA_n #translation of Calpha\n",
    "    Qs = out['1'][:,1,:] # rotation\n",
    "    Qs = Qs.unsqueeze(1).repeat((1,2,1))\n",
    "    Qs = torch.cat((torch.ones((B*L,2,1),device=Qs.device),Qs),dim=-1).reshape(B,L,2,4)\n",
    "    Qs = normQ(Qs)\n",
    "    Rs = Qs2Rs(Qs)\n",
    "    N_C_to_Rot = torch.cat((b_nd['N_CA'].reshape(B, L, 3).to('cuda'),\n",
    "                            b_nd['C_CA'].reshape(B, L, 3).to('cuda')),dim=2).reshape(B,L,2,1,3)\n",
    "\n",
    "\n",
    "    rot_vecs = einsum('bnkij,bnkhj->bnki',Rs, N_C_to_Rot)\n",
    "    NC_p = CA_p + rot_vecs[:,:,0,:].to('cuda')*N_CA_dist\n",
    "    CC_p = CA_p + rot_vecs[:,:,1,:].reshape(B, L, 3).to('cuda')*C_CA_dist\n",
    "\n",
    "    pred = torch.cat((NC_p,CA_p,CC_p),dim=2).reshape(B,L,3,3)\n",
    "    \n",
    "    return true.to('cpu').numpy()*10, noise_xyz.to('cpu').numpy()*10, pred.detach().to('cpu').numpy()*10\n",
    "\n",
    "\n",
    "\n",
    "def dump_tnp(true, noise, pred, t_vec, e=0, outdir='output/'):\n",
    "    \n",
    "    outdir = f'{outdir}epoch{e}_models'\n",
    "    if not os.path.exists(outdir):\n",
    "        os.makedirs(outdir)\n",
    "    \n",
    "    for i,c in enumerate(t_vec):\n",
    "        dump_coord_pdb(true[0], fileOut=f'{outdir}/true_{e}_{0}.pdb')\n",
    "        dump_coord_pdb(noise[i], fileOut=f'{outdir}/noise_{e}_{c*100:.0f}.pdb')\n",
    "        dump_coord_pdb(pred[i], fileOut=f'{outdir}/pred_{e}_{c*100:.0f}.pdb')\n",
    "        \n",
    "def visualize_model(bb_dict, noised_bb, epoch, t_vec, g_maker,graph_unet, batch=64, outdir='output/'):\n",
    "    true, noise, pred = get_noise_pred_true(bb_dict, noised_bb, t_vec, g_maker, graph_unet,B=batch)\n",
    "    dump_tnp(true,noise,pred,t_vec, e=epoch, outdir=f'{outdir}/models/')\n",
    "    \n",
    "\n",
    "    \n",
    "def make_save_folder(name=''):\n",
    "    base_folder = time.strftime(f'log/%y%b%d_%I%M%p_{name}/', time.localtime())\n",
    "    if not os.path.exists(base_folder):\n",
    "        os.makedirs(base_folder)\n",
    "    subfolders = ['models','runs']\n",
    "    for subfolder in subfolders:\n",
    "        if not os.path.exists(base_folder + subfolder):\n",
    "            os.makedirs(base_folder + subfolder)\n",
    "            \n",
    "    writer = SummaryWriter(f'{base_folder}runs/{name}_')\n",
    "            \n",
    "    return base_folder, writer\n",
    "        \n",
    "def save_chkpt(model_path, model, optimizer, epoch, batch, val_losses, train_losses):\n",
    "    \"\"\"Save a training checkpoint\n",
    "    Args:\n",
    "        model_path (str): the path to save the model to\n",
    "        model (nn.Module): the model to save\n",
    "        optimizer (torch.optim.Optimizer): the optimizer to save\n",
    "        epoch (int): the current epoch\n",
    "        batch (int): the current batch in the epoch\n",
    "        loss_domain (list of int): a list of the shared domain for val and training \n",
    "            losses\n",
    "        val_losses (list of float): a list containing the validation losses\n",
    "        train_losses (list of float): a list containing the training losses\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "    state_dict = dict()\n",
    "    state_dict.update({'model':model.state_dict(),\n",
    "                       'optimizer':optimizer.state_dict(),\n",
    "                       'epoch':epoch,\n",
    "                       'batch':batch,\n",
    "                       'train_losses':train_losses,\n",
    "                       'val_losses':val_losses\n",
    "                       })\n",
    "    torch.save(state_dict, f'{model_path}model_e{epoch}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "149c44dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 64\n",
    "L=65\n",
    "limit = 1028*5\n",
    "h4_trainData = Helix4_Dataset(coords_tog[:limit])\n",
    "h4_valData = Helix4_Dataset(coords_apa[:limit])\n",
    "train_dL = DataLoader(h4_trainData, batch_size=B, shuffle=True, drop_last=True)\n",
    "val_dL   = DataLoader(h4_valData, batch_size=B, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58ac8828",
   "metadata": {},
   "outputs": [],
   "source": [
    "gu = GraphUNet(batch_size = B, fiber_start = Fiber({0:13, 1:2}), num_layers_ca=2).to('cuda')\n",
    "opti = torch.optim.Adam(gu.parameters(), lr=0.005, weight_decay=5e-6)\n",
    "#gm = Make_KNN_MP_Graphs(mp_stride=4)\n",
    "gm = Make_KNN_MP_Graphs(mp_stride=10)\n",
    "dataiter = iter(train_dL)\n",
    "bb_dict = next(dataiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e03d575d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdn= FrameDiffNoise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f39afb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_path, writer = make_save_folder(name=f'testing_fullT')\n",
    "num_epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b15462",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nwoodall/miniconda3/envs/se33/lib/python3.9/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
      "/home/nwoodall/miniconda3/envs/se33/lib/python3.9/site-packages/dgl/backend/pytorch/tensor.py:449: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss Epoch 0: 0.0014951324556022882\n",
      "Average Valid Loss Epoch 0: 0.0013334271498024464\n",
      "Average Train Loss Epoch 1: 0.0012956506107002497\n",
      "Average Train Loss Epoch 2: 0.0010133107425644994\n",
      "Average Train Loss Epoch 3: 0.0009247238049283624\n",
      "Average Train Loss Epoch 4: 0.0008931725169532001\n",
      "Average Train Loss Epoch 5: 0.0008410437148995697\n",
      "Average Train Loss Epoch 6: 0.0008023898117244244\n",
      "Average Train Loss Epoch 7: 0.0007988594588823617\n",
      "Average Train Loss Epoch 8: 0.0007823741179890931\n",
      "Average Train Loss Epoch 9: 0.0007513013551943004\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    \n",
    "    running_tloss = 0 \n",
    "    for i, bb_dict in enumerate(train_dL):\n",
    "        noised_bb, t_vec, score_scales = fdn(bb_dict)\n",
    "        train_loss = model_step(bb_dict, noised_bb, t_vec ,score_scales, gm, gu)\n",
    "        opti.zero_grad()\n",
    "        train_loss.backward()\n",
    "        opti.step()\n",
    "\n",
    "        running_tloss += train_loss.detach().cpu()\n",
    "    \n",
    "    avg_tloss = running_tloss/(i+1)\n",
    "    print(f'Average Train Loss Epoch {e}: {avg_tloss}')\n",
    "    writer.add_scalar('training loss',\n",
    "                        avg_tloss,\n",
    "                        e * len(train_dL) + i)\n",
    "\n",
    "    if e%10==0:\n",
    "        with torch.no_grad():\n",
    "            running_vloss = 0\n",
    "            for j, bb_dict in enumerate(val_dL):\n",
    "                noised_bb, t_vec, score_scales = fdn(bb_dict)\n",
    "                valid_loss = model_step(bb_dict, noised_bb, t_vec ,score_scales, gm, gu).cpu()\n",
    "                running_vloss += valid_loss\n",
    "                \n",
    "        avg_vloss = running_vloss/(i+1)\n",
    "        print(f'Average Valid Loss Epoch {e}: {avg_vloss}')\n",
    "\n",
    "        writer.add_scalar('valid loss',\n",
    "                        avg_vloss,\n",
    "                        e * len(val_dL) + i)\n",
    "        \n",
    "        \n",
    "        nd, tv = fdn.noise_visualize(bb_dict)\n",
    "        visualize_model(bb_dict, nd, e, tv, gm, gu, batch=B, outdir=model_path)\n",
    "        save_chkpt(model_path, gu, opti, e, B, avg_vloss, avg_tloss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f25579",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensorboard --log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e49b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_test(backbone_dict, noised_dict):\n",
    "    CA_t  = bb_dict['CA'].reshape(B, L, 3).to('cuda')\n",
    "    NC_t = CA_t + bb_dict['N_CA'].reshape(B, L, 3).to('cuda')*N_CA_dist\n",
    "    CC_t = CA_t + bb_dict['C_CA'].reshape(B, L, 3).to('cuda')*C_CA_dist\n",
    "    true =  torch.cat((NC_t,CA_t,CC_t),dim=2).reshape(B,L,3,3)\n",
    "    \n",
    "    CA_n  = noised_dict['CA'].reshape(B, L, 3).to('cuda')\n",
    "    NC_n = CA_n + noised_dict['N_CA'].reshape(B, L, 3).to('cuda')*N_CA_dist\n",
    "    CC_n = CA_n + noised_dict['C_CA'].reshape(B, L, 3).to('cuda')*C_CA_dist\n",
    "    noise_xyz =  torch.cat((NC_n,CA_n,CC_n),dim=2).reshape(B,L,3,3)\n",
    "    return true.to('cpu').numpy()*10, noise_xyz.to('cpu').numpy()*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d45bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.npose_util import makePointPDB\n",
    "\n",
    "\n",
    "def view_mp_graph(train_dL: DataLoader, B=64):\n",
    "    dataiter = iter(train_dL)\n",
    "    bb_dict = next(dataiter)\n",
    "    to_model, true, noise_xyz = prep_dL_output(bb_dict, bb_dict , gm)\n",
    "    bg,nf,ef, bg_mp, nf_mp, ef_mp, bg_mps, nf_mps, ef_mps, bg_mpRev = to_model\n",
    "\n",
    "    graph = bg\n",
    "    graph_mps = bg_mps\n",
    "\n",
    "    pos = graph.ndata['pos'].reshape(B,-1,3)*10\n",
    "    pos_mps = graph_mps.ndata['pos'].reshape(B,-1,3)*10\n",
    "\n",
    "    makePointPDB(pos[0],'coords.pdb',outDirec='output')\n",
    "    makePointPDB(pos_mps[0],'mp.pdb',outDirec='output')\n",
    "    dump_coord_pdb(true[0].cpu().numpy()*10,fileOut='output/true.pdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e402249b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_midpoint(ep_in):\n",
    "    \"\"\"Get midpoint, of each batched set of points\"\"\"\n",
    "    \n",
    "    #calculate midpoint\n",
    "    midpoint = ep_in.sum(axis=1)/np.repeat(ep_in.shape[1], ep_in.shape[2])\n",
    "    \n",
    "    return midpoint\n",
    "\n",
    "def normalize_points(input_xyz, print_dist=False):\n",
    "    \n",
    "    #broadcast to distance matrix [Batch, M, R3] to [Batch,M,1, R3] to [Batch,1,M, R3] to [Batch, M,M, R3] \n",
    "    vec_diff = input_xyz[...,None,:]-input_xyz[...,None,:,:]\n",
    "    dist = np.sqrt(np.sum(np.square(vec_diff),axis=len(input_xyz.shape)))\n",
    "    furthest_dist = np.max(dist)\n",
    "    centroid  = get_midpoint(input_xyz)\n",
    "    if print_dist:\n",
    "        print(f'largest distance {furthest_dist:0.1f}')\n",
    "    \n",
    "    xyz_mean_zero = input_xyz - centroid[:,None,:]\n",
    "    return xyz_mean_zero/furthest_dist\n",
    "def count_trainable_params(model_in):\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model_in.parameters())\n",
    "    params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5a5083ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path, model_class, B = 64):\n",
    "    \"\"\"Load a saved model\"\"\"\n",
    "    device='cuda'\n",
    "    model = model_class(batch_size = B, fiber_start = Fiber({0:13, 1:2})).to(device)\n",
    "    model.load_state_dict(torch.load(model_path)['model'])\n",
    "    model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "25a3f95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gu2 = load_model(f'{model_path}model_e199', GraphUNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78db7c47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1a8be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noise_pred_true_st(backbone_dict, noised_dict, t, graph_maker, graph_unet):\n",
    "    #might need fix, might sub \n",
    "    CA_t  = backbone_dict['CA'].reshape(B, L, 3).to('cuda')\n",
    "    NC_t = CA_t + backbone_dict['N_CA'].reshape(B, L, 3).to('cuda')*N_CA_dist\n",
    "    CC_t = CA_t + backbone_dict['C_CA'].reshape(B, L, 3).to('cuda')*C_CA_dist\n",
    "    true =  torch.cat((NC_t,CA_t,CC_t),dim=2).reshape(B,L,3,3)\n",
    "    \n",
    "    CA_n  = noised_dict['CA'].reshape(B, L, 3).to('cuda')\n",
    "    NC_n = CA_n + noised_dict['N_CA'].reshape(B, L, 3).to('cuda')*N_CA_dist\n",
    "    CC_n = CA_n + noised_dict['C_CA'].reshape(B, L, 3).to('cuda')*C_CA_dist\n",
    "    noise_xyz =  torch.cat((NC_n,CA_n,CC_n),dim=2).reshape(B,L,3,3)\n",
    "    \n",
    "    x = graph_maker.prep_for_network(noised_dict, t)\n",
    "    out = graph_unet(x)\n",
    "    CA_p = out['1'][:,0,:].reshape(B, L, 3)+CA_n #translation of Calpha\n",
    "    Qs = out['1'][:,1,:] # rotation\n",
    "    Qs = Qs.unsqueeze(1).repeat((1,2,1))\n",
    "    Qs = torch.cat((torch.ones((B*L,2,1),device=Qs.device),Qs),dim=-1).reshape(B,L,2,4)\n",
    "    Qs = normQ(Qs)\n",
    "    Rs = Qs2Rs(Qs)\n",
    "    N_C_to_Rot = torch.cat((noised_dict['N_CA'].reshape(B, L, 3).to('cuda'),\n",
    "                            noised_dict['C_CA'].reshape(B, L, 3).to('cuda')),dim=2).reshape(B,L,2,1,3)\n",
    "    \n",
    "    \n",
    "    rot_vecs = einsum('bnkij,bnkhj->bnki',Rs, N_C_to_Rot)\n",
    "    NC_p = CA_p + rot_vecs[:,:,0,:].to('cuda')*N_CA_dist\n",
    "    CC_p = CA_p + rot_vecs[:,:,1,:].reshape(B, L, 3).to('cuda')*C_CA_dist\n",
    "\n",
    "    pred = torch.cat((NC_p,CA_p,CC_p),dim=2).reshape(B,L,3,3)\n",
    "    \n",
    "    return true.to('cpu').numpy()*10, noise_xyz.to('cpu').numpy()*10, pred.detach().to('cpu').numpy()*10\n",
    "\n",
    "def visualize_model_single_t(bb_dict, noised_bb, epoch,g_maker,graph_unet, numOut=1, outdir='output/'):\n",
    "    true, noise, pred = get_noise_pred_true(bb_dict, noised_bb, t, g_maker, graph_unet)\n",
    "    dump_tnp(true,noise,pred, e=epoch, numOut=numOut, outdir=f'{outdir}/models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f249030c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_scaling(self, t):\n",
    "    rot_score_scaling = self._so3_diffuser.score_scaling(t)\n",
    "    trans_score_scaling = self._r3_diffuser.score_scaling(t)\n",
    "    return rot_score_scaling, trans_score_scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7112b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_scaling(self, t: float):\n",
    "    return 1 / np.sqrt(self.conditional_var(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5124f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def conditional_var(self, t, use_torch=False):\n",
    "        \"\"\"Conditional variance of p(xt|x0).\n",
    "\n",
    "        Var[x_t|x_0] = conditional_var(t)*I\n",
    "\n",
    "        \"\"\"\n",
    "        if use_torch:\n",
    "            return 1 - torch.exp(-self.marginal_b_t(t))\n",
    "        return 1 - np.exp(-self.marginal_b_t(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "274fa369",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (64) must match the size of tensor b (65) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m w_loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflip(w_loss, (\u001b[38;5;241m0\u001b[39m,))\n\u001b[1;32m      4\u001b[0m w_loss \u001b[38;5;241m=\u001b[39m w_loss \u001b[38;5;241m/\u001b[39m w_loss\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m----> 6\u001b[0m tot_loss \u001b[38;5;241m=\u001b[39m (\u001b[43mw_loss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m)\u001b[38;5;241m.\u001b[39msum()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (64) must match the size of tensor b (65) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "# weighting loss\n",
    "w_loss = torch.pow(torch.full((I,), gamma, device=pred.device), torch.arange(I, device=pred.device))\n",
    "w_loss = torch.flip(w_loss, (0,))\n",
    "w_loss = w_loss / w_loss.sum()\n",
    "\n",
    "tot_loss = (w_loss * loss).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbbe57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def marginal_b_t(self, t):\n",
    "    return t*self.min_b + (1/2)*(t**2)*(self.max_b-self.min_b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "se33",
   "language": "python",
   "name": "se33"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
