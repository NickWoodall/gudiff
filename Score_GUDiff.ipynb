{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dba4fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import util.npose_util as nu\n",
    "import os\n",
    "import pathlib\n",
    "import dgl\n",
    "from dgl import backend as F\n",
    "import torch_geometric\n",
    "from torch.utils.data import random_split, DataLoader, Dataset\n",
    "from typing import Dict\n",
    "from torch import Tensor\n",
    "from dgl import DGLGraph\n",
    "from torch import nn\n",
    "# from chemical import cos_ideal_NCAC #from RoseTTAFold2\n",
    "from torch import einsum\n",
    "import time\n",
    "import torch\n",
    "import pickle\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c10146e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from se3_transformer.model.basis import get_basis, update_basis_with_fused\n",
    "from se3_transformer.model.transformer import Sequential, SE3Transformer\n",
    "from se3_transformer.model.transformer_topk import SE3Transformer_topK\n",
    "from se3_transformer.model.FAPE_Loss import FAPE_loss, Qs2Rs, normQ\n",
    "from se3_transformer.model.layers.attentiontopK import AttentionBlockSE3\n",
    "from se3_transformer.model.layers.linear import LinearSE3\n",
    "from se3_transformer.model.layers.convolution import ConvSE3, ConvSE3FuseLevel\n",
    "from se3_transformer.model.layers.norm import NormSE3\n",
    "from se3_transformer.model.layers.pooling import GPooling, Latent_Unpool, Unpool_Layer\n",
    "from se3_transformer.runtime.utils import str2bool, to_cuda\n",
    "from se3_transformer.model.fiber import Fiber\n",
    "from se3_transformer.model.transformer import get_populated_edge_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84eb4b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from se3_diffuse import rigid_utils as ru\n",
    "from se3_diffuse import utils as du\n",
    "from se3_diffuse import se3_diffuser\n",
    "from gudiff_model import Data_Graph\n",
    "from gudiff_model.Data_Graph import Helix4_Dataset_Score, Make_KNN_MP_Graphs\n",
    "from gudiff_model.Graph_UNet import GraphUNet\n",
    "from gudiff_model.Data_Graph import build_npose_from_coords, dump_coord_pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "343e5c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#indices for, unsure if needed\n",
    "CA = Data_Graph.CA\n",
    "N = Data_Graph.N\n",
    "C = Data_Graph.C\n",
    "\n",
    "#find better way to incorporate coord_scale\n",
    "\n",
    "#needed\n",
    "N_CA_dist = (Data_Graph.N_CA_dist/10.).to('cuda')\n",
    "C_CA_dist = (Data_Graph.C_CA_dist/10.).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9434530",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_score_step(backbone_dict, noised_dict, graph_maker, graph_unet,\n",
    "                     rot_loss_t_threshold = 0.2, trans_weight=1.0, rot_weight=0.5,\n",
    "                     device='cuda'):\n",
    "    \n",
    "    \n",
    "    \n",
    "    batched_t = noised_dict['batched_t'].to(device)\n",
    "    \n",
    "    CA_t  = backbone_dict['CA'].reshape(B, L, 3).to(device)\n",
    "    NC_t = CA_t + backbone_dict['N_CA'].reshape(B, L, 3).to(device)\n",
    "    CC_t = CA_t + backbone_dict['C_CA'].reshape(B, L, 3).to(device)\n",
    "    true =  torch.cat((NC_t,CA_t,CC_t),dim=2).reshape(B,L,3,3)\n",
    "\n",
    "    CA_n  = noised_dict['CA'].reshape(B, L, 3).to(device)\n",
    "    NC_n = CA_n + noised_dict['N_CA'].reshape(B, L, 3).to(device)\n",
    "    CC_n = CA_n + noised_dict['C_CA'].reshape(B, L, 3).to(device)\n",
    "    noise_xyz =  torch.cat((NC_n,CA_n,CC_n),dim=2).reshape(B,L,3,3)\n",
    "\n",
    "    x = graph_maker.prep_for_network(noised_dict)\n",
    "    out = graph_unet(x, batched_t)\n",
    "\n",
    "\n",
    "    pred_trans_score = out['1'][:,0,:]\n",
    "    pred_rots_score = out['1'][:,1,:]\n",
    "\n",
    "    true_trans_score = noised_dict['trans_score'].reshape((-1,3)).to(device)\n",
    "    tss = noisy['trans_score_scaling'][:,None,None].to(device)\n",
    "\n",
    "    true_rots_score = noised_dict['rot_score'].reshape((-1,3)).to(device)\n",
    "    rss = noisy['rot_score_scaling'][:,None,None].to(device)\n",
    "\n",
    "\n",
    "    loss_trans_mse = torch.square(pred_trans_score-true_trans_score)/(tss**2)\n",
    "    loss_rot_mse = torch.square(pred_rots_score-true_rots_score)/(rss**2)\n",
    "\n",
    "    rot_loss =   torch.sum(loss_rot_mse,dim=(-1, -2))/ (B*L) #\n",
    "    trans_loss = torch.sum(loss_trans_mse,dim=(-1, -2))/ (B*L) #\n",
    "    \n",
    "    rot_loss *= batched_t > rot_loss_t_threshold\n",
    "\n",
    "    loss_out = rot_loss*torch.tensor(rot_weight).to(device)+trans_loss*torch.tensor(trans_weight).to(device)\n",
    "    \n",
    "    return loss_out\n",
    "\n",
    "def model_score_step_trans(backbone_dict, noised_dict, graph_maker, graph_unet,\n",
    "                     rot_loss_t_threshold = 0.2, trans_weight=1.0, rot_weight=0.5,\n",
    "                     device='cuda'):\n",
    "    \n",
    "    \n",
    "    \n",
    "    batched_t = noised_dict['batched_t'].to(device)\n",
    "    \n",
    "    CA_t  = backbone_dict['CA'].reshape(B, L, 3).to(device)\n",
    "    NC_t = CA_t + backbone_dict['N_CA'].reshape(B, L, 3).to(device)\n",
    "    CC_t = CA_t + backbone_dict['C_CA'].reshape(B, L, 3).to(device)\n",
    "    true =  torch.cat((NC_t,CA_t,CC_t),dim=2).reshape(B,L,3,3)\n",
    "\n",
    "    CA_n  = noised_dict['CA'].reshape(B, L, 3).to(device)\n",
    "    NC_n = CA_n + noised_dict['N_CA'].reshape(B, L, 3).to(device)\n",
    "    CC_n = CA_n + noised_dict['C_CA'].reshape(B, L, 3).to(device)\n",
    "    noise_xyz =  torch.cat((NC_n,CA_n,CC_n),dim=2).reshape(B,L,3,3)\n",
    "\n",
    "    x = graph_maker.prep_for_network(noised_dict)\n",
    "    out = graph_unet(x, batched_t)\n",
    "\n",
    "\n",
    "    pred_trans_score = out['1'][:,0,:]\n",
    "    #pred_rots_score = out['1'][:,1,:]\n",
    "\n",
    "    true_trans_score = noised_dict['trans_score'].reshape((-1,3)).to(device)\n",
    "    tss = noisy['trans_score_scaling'][:,None,None].to(device)\n",
    "\n",
    "#     true_rots_score = noised_dict['rot_score'].reshape((-1,3)).to(device)\n",
    "#     rss = noisy['rot_score_scaling'][:,None,None].to(device)\n",
    "\n",
    "\n",
    "    loss_trans_mse = torch.square(pred_trans_score-true_trans_score)#/(tss**2)\n",
    "#     loss_rot_mse = torch.square(pred_rots_score-true_rots_score)/(rss**2)\n",
    "\n",
    "    #rot_loss =   torch.sum(loss_rot_mse,dim=(-1, -2))/ (B*L) #\n",
    "    trans_loss = torch.sum(loss_trans_mse,dim=(-1, -2))/ (B*L) #\n",
    "    trans_loss = torch.clamp(trans_loss, min=None, max=100)\n",
    "    \n",
    "    #rot_loss *= batched_t > rot_loss_t_threshold\n",
    "\n",
    "    loss_out = trans_loss*torch.tensor(trans_weight).to(device)\n",
    "    \n",
    "    return loss_out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f669eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add threshold and relative weights\n",
    "\n",
    "#   # Loss weights.\n",
    "#   trans_loss_weight: 1.0\n",
    "#   rot_loss_weight: 0.5\n",
    "#   rot_loss_t_threshold: 0.2\n",
    "#   separate_rot_loss: True\n",
    "#   trans_x0_threshold: 1.0\n",
    "#   coordinate_scaling: ${diffuser.r3.coordinate_scaling}\n",
    "#   bb_atom_loss_weight: 1.0\n",
    "#   bb_atom_loss_t_filter: 0.25\n",
    "#   dist_mat_loss_weight: 1.0\n",
    "#   dist_mat_loss_t_filter: 0.25\n",
    "#   aux_loss_weight: 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26cf95ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rot_mse = (gt_rot_score - pred_rot_score)**2 * loss_mask[..., None]\n",
    "# rot_loss = torch.sum(\n",
    "#     rot_mse / rot_score_scaling[:, None, None]**2,\n",
    "#     dim=(-1, -2)\n",
    "# ) / (loss_mask.sum(dim=-1) + 1e-10)\n",
    "# rot_loss *= self._exp_conf.rot_loss_weight\n",
    "# rot_loss *= batch['t'] > self._exp_conf.rot_loss_t_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a0b8e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMA(nn.Module):\n",
    "    def __init__(self, mu):\n",
    "        super(EMA, self).__init__()\n",
    "        self.mu = mu\n",
    "        self.shadow = {}\n",
    "\n",
    "    def register(self, name, val):\n",
    "        self.shadow[name] = val.clone()\n",
    "\n",
    "    def forward(self, name, x):\n",
    "        assert name in self.shadow\n",
    "        new_average = (1.0 - self.mu) * x + self.mu * self.shadow[name]\n",
    "        self.shadow[name] = new_average.clone()\n",
    "        return new_average\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3f24384",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# in batch training loop\n",
    "# for batch in batches:\n",
    "\n",
    "# optimizer.step()\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         param.data = ema(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cc4d8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_save_folder(name=''):\n",
    "    base_folder = time.strftime(f'log/%y%b%d_%I%M%p_{name}/', time.localtime())\n",
    "    if not os.path.exists(base_folder):\n",
    "        os.makedirs(base_folder)\n",
    "    subfolders = ['models']\n",
    "    for subfolder in subfolders:\n",
    "        if not os.path.exists(base_folder + subfolder):\n",
    "            os.makedirs(base_folder + subfolder)\n",
    "            \n",
    "    return base_folder\n",
    "        \n",
    "def save_chkpt(model_path, model, optimizer, epoch, batch, val_losses, train_losses):\n",
    "    \"\"\"Save a training checkpoint\n",
    "    Args:\n",
    "        model_path (str): the path to save the model to\n",
    "        model (nn.Module): the model to save\n",
    "        optimizer (torch.optim.Optimizer): the optimizer to save\n",
    "        epoch (int): the current epoch\n",
    "        batch (int): the current batch in the epoch\n",
    "        loss_domain (list of int): a list of the shared domain for val and training \n",
    "            losses\n",
    "        val_losses (list of float): a list containing the validation losses\n",
    "        train_losses (list of float): a list containing the training losses\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "    state_dict = dict()\n",
    "    state_dict.update({'model':model.state_dict(),\n",
    "                       'optimizer':optimizer.state_dict(),\n",
    "                       'epoch':epoch,\n",
    "                       'batch':batch,\n",
    "                       'train_losses':train_losses,\n",
    "                       'val_losses':val_losses\n",
    "                       })\n",
    "    torch.save(state_dict, f'{model_path}model_e{epoch}')\n",
    "\n",
    "    \n",
    "def save_chkpt_ema(model_path, model, optimizer, epoch, batch, ema, train_losses):\n",
    "    \"\"\"Save a training checkpoint\n",
    "    Args:\n",
    "        model_path (str): the path to save the model to\n",
    "        model (nn.Module): the model to save\n",
    "        optimizer (torch.optim.Optimizer): the optimizer to save\n",
    "        epoch (int): the current epoch\n",
    "        batch (int): the current batch in the epoch\n",
    "        loss_domain (list of int): a list of the shared domain for val and training \n",
    "            losses\n",
    "        val_losses (list of float): a list containing the validation losses\n",
    "        train_losses (list of float): a list containing the training losses\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "    state_dict = dict()\n",
    "    state_dict.update({'model':model.state_dict(),\n",
    "                       'optimizer':optimizer.state_dict(),\n",
    "                       'epoch':epoch,\n",
    "                       'batch':batch,\n",
    "                       'train_losses':train_losses\n",
    "                       })\n",
    "    torch.save(state_dict, f'{model_path}model_e{epoch}')\n",
    "\n",
    "    # Open a file and use dump()\n",
    "    with open(f'{model_path}/ema_save_{epoch}.pkl', 'wb') as file:\n",
    "        # A new file will be created\n",
    "        pickle.dump(ema, file)\n",
    "    \n",
    "def load_model(model_path, model_class):\n",
    "    \"\"\"Load a saved model\"\"\"\n",
    "    \n",
    "    device = 'cuda:0'\n",
    "    model = model_class()\n",
    "    model.load_state_dict(torch.load(model_path)['model'])\n",
    "    model.to(device)\n",
    "    \n",
    "    return model \n",
    "    \n",
    "def load_model_ema(model_path, model_class, e):\n",
    "    \"\"\"Load a saved model\"\"\"\n",
    "    \n",
    "    model_path_m = f'{model_path}/model_e{e}'\n",
    "    \n",
    "    device = 'cuda:0'\n",
    "    model = model_class(batch_size = B, num_layers_ca = 2, zero_lin=False)\n",
    "    model.load_state_dict(torch.load(model_path_m)['model'])\n",
    "    model.to(device)\n",
    "    opti = torch.optim.Adam(model.parameters(), lr=0.0005, weight_decay=5e-5)\n",
    "    opti.load_state_dict(torch.load(f'{model_path_m}')['optimizer'])\n",
    "    \n",
    "    with open(f'{model_path}/ema_save_{e}.pkl', 'rb') as file:\n",
    "        # Call load method to deserialze\n",
    "        ema_out = pickle.load(file)\n",
    "\n",
    "    return model, ema_out, opti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be8e18e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path_str  = 'data/h4_ca_coords.npz'\n",
    "# test_limit = 1028\n",
    "# rr = np.load(data_path_str)\n",
    "# ca_coords = [rr[f] for f in rr.files][0][:test_limit,:,:3]\n",
    "# ca_coords.shape\n",
    "\n",
    "# getting N-Ca, Ca-C vectors to add as typeI features\n",
    "#apa = apart helices for val/train split\n",
    "#tog = together helices for val/train split\n",
    "apa_path_str  = 'data_npose/h4_apa_coords.npz'\n",
    "tog_path_str  = 'data_npose/h4_tog_coords.npz'\n",
    "\n",
    "#grab the first 3 atoms which are N,CA,C\n",
    "test_limit = 5048\n",
    "rr = np.load(apa_path_str)\n",
    "coords_apa = [rr[f] for f in rr.files][0][:test_limit,:]\n",
    "\n",
    "rr = np.load(tog_path_str)\n",
    "coords_tog = [rr[f] for f in rr.files][0][:test_limit,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "628f1f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 32\n",
    "L=65\n",
    "limit = 2028\n",
    "h4_trainData = Helix4_Dataset_Score(coords_tog[:limit])\n",
    "train_dL = DataLoader(h4_trainData, batch_size=B, shuffle=True, drop_last=True)\n",
    "test_iter = iter(train_dL)\n",
    "test_batch = next(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95602440",
   "metadata": {},
   "outputs": [],
   "source": [
    "se3d = se3_diffuser.SE3Diffuser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2bc3e44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gu = GraphUNet(batch_size = B, num_layers_ca = 2, zero_lin=False).to('cuda')\n",
    "opti = torch.optim.Adam(gu.parameters(), lr=0.0005, weight_decay=5e-5)\n",
    "ema = EMA(0.980)\n",
    "for name, param in gu.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        ema.register(name, param.data)\n",
    "gm = Make_KNN_MP_Graphs() #consider precalculating graphs for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "69a64949",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82c989c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = 'log/23Sep11_0841AM_full_diff_score_trans_only'\n",
    "# e_start=190\n",
    "# gu, ema, opti = load_model_ema(model_path, GraphUNet, 189)\n",
    "#ema.mu = 0.95\n",
    "# num_epochs = 200\n",
    "# e_start= 190\n",
    "# #model_path = make_save_folder(name=f'full_diff_score_trans_only')\n",
    "# save_per=10\n",
    "# avg_vloss=0\n",
    "# avg_tloss=0\n",
    "\n",
    "# for e in range(e_start, e_start+num_epochs):\n",
    "    \n",
    "#     running_tloss = 0 \n",
    "#     start = time.time()\n",
    "#     for i, bb_dict in enumerate(train_dL):\n",
    "#         noisy = se3d.forward_marginal_trans(bb_dict,t_vec=None)\n",
    "#         train_loss = torch.sum(model_score_step_trans(bb_dict, noisy, gm, gu))\n",
    "\n",
    "#         opti.zero_grad()\n",
    "#         train_loss.backward()\n",
    "#         opti.step()\n",
    "        \n",
    "#         for name, param in gu.named_parameters():\n",
    "#             if param.requires_grad:\n",
    "#                 param.data = ema(name, param.data)\n",
    "\n",
    "#         running_tloss += train_loss.detach().cpu()\n",
    "    \n",
    "#     end = time.time()\n",
    "#     avg_tloss = running_tloss/(i+1)\n",
    "#     print(f'Average Train Loss Epoch {e}: {avg_tloss};   Epoch time: {end-start:.0f}')\n",
    "    \n",
    "    \n",
    "#     if e %save_per==save_per-1:\n",
    "#         save_chkpt_ema(f'{model_path}', gu, opti, e, B, ema, avg_tloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea247b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# se3d._diffuse_rot = False\n",
    "# test_iter = iter(train_dL)\n",
    "# test_batch = next(test_iter)\n",
    "# dt = 0.01\n",
    "# t_start = 0.01\n",
    "# t_end = 0\n",
    "# tr = np.arange(t_start,0,-dt)\n",
    "# tr\n",
    "# t_vec = np.ones((B,))*t_start\n",
    "# noisy = se3d.forward_marginal_trans(test_batch,t_vec=t_vec)\n",
    "# batched_t = noisy['batched_t'].to('cuda')\n",
    "# x = gm.prep_for_network(noisy)\n",
    "# out = gu(x, batched_t)\n",
    "# ts = out['1'][:,0,:].reshape((32,65,3))\n",
    "# trans_t_1 = se3d._r3_diffuser.reverse(\n",
    "#                 x_t=noisy['CA'].numpy(),\n",
    "#                 score_t=ts.detach().cpu().numpy(),\n",
    "#                 t=t_start,\n",
    "#                 dt=t_start,\n",
    "#                 center=\"True\",\n",
    "#                 noise_scale=0\n",
    "#                 )\n",
    "# device='cpu'\n",
    "# CA_n  = trans_t_1.reshape(B, L, 3)\n",
    "# NC_n = CA_n + noisy['N_CA'].reshape(B, L, 3).numpy()\n",
    "# CC_n = CA_n + noisy['C_CA'].reshape(B, L, 3).numpy()\n",
    "# noise_xyz =  np.concatenate((NC_n,CA_n,CC_n),axis=2).reshape(B,L,3,3)\n",
    "# dump_coord_pdb(noise_xyz[0]*10,fileOut='output/test.pdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610c818d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nwoodall/miniconda3/envs/se33/lib/python3.9/site-packages/dgl/backend/pytorch/tensor.py:449: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), (\n"
     ]
    }
   ],
   "source": [
    "#fresh run\n",
    "num_epochs = 200\n",
    "e_start= 0\n",
    "model_path = make_save_folder(name=f'full_diff_score_trans_only_notss')\n",
    "save_per=10\n",
    "avg_vloss=0\n",
    "avg_tloss=0\n",
    "\n",
    "for e in range(e_start, e_start+num_epochs):\n",
    "    \n",
    "    running_tloss = 0 \n",
    "    start = time.time()\n",
    "    for i, bb_dict in enumerate(train_dL):\n",
    "        noisy = se3d.forward_marginal_trans(bb_dict,t_vec=None)\n",
    "        train_loss = torch.sum(model_score_step_trans(bb_dict, noisy, gm, gu))\n",
    "\n",
    "        opti.zero_grad()\n",
    "        train_loss.backward()\n",
    "        opti.step()\n",
    "        \n",
    "        for name, param in gu.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                param.data = ema(name, param.data)\n",
    "\n",
    "        running_tloss += train_loss.detach().cpu()\n",
    "    \n",
    "    end = time.time()\n",
    "    avg_tloss = running_tloss/(i+1)\n",
    "    print(f'Average Train Loss Epoch {e}: {avg_tloss};   Epoch time: {end-start:.0f}')\n",
    "    \n",
    "    \n",
    "    if e %save_per==save_per-1:\n",
    "        save_chkpt_ema(f'{model_path}', gu, opti, e, B, ema, avg_tloss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01663cc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0510968",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da837885",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c15753",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241f687b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "805efb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nwoodall/miniconda3/envs/se33/lib/python3.9/site-packages/dgl/backend/pytorch/tensor.py:449: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss Epoch 0: 3084.808837890625;   Epoch time: 245\n",
      "Average Train Loss Epoch 1: 3649.027587890625;   Epoch time: 249\n",
      "Average Train Loss Epoch 2: 2278.03173828125;   Epoch time: 245\n",
      "Average Train Loss Epoch 3: 4651.07763671875;   Epoch time: 244\n",
      "Average Train Loss Epoch 4: 3042.232666015625;   Epoch time: 242\n",
      "Average Train Loss Epoch 5: 3258.308349609375;   Epoch time: 242\n",
      "Average Train Loss Epoch 6: 10177.689453125;   Epoch time: 241\n",
      "Average Train Loss Epoch 7: 2328.85546875;   Epoch time: 241\n",
      "Average Train Loss Epoch 8: 4874.81982421875;   Epoch time: 241\n",
      "Average Train Loss Epoch 9: 2034.1339111328125;   Epoch time: 241\n",
      "Average Train Loss Epoch 10: 5845.41015625;   Epoch time: 241\n",
      "Average Train Loss Epoch 11: 4438.75927734375;   Epoch time: 241\n",
      "Average Train Loss Epoch 12: 4779.22314453125;   Epoch time: 242\n",
      "Average Train Loss Epoch 13: 2697.35595703125;   Epoch time: 242\n",
      "Average Train Loss Epoch 14: 3766.337646484375;   Epoch time: 242\n",
      "Average Train Loss Epoch 15: 6226.7822265625;   Epoch time: 242\n",
      "Average Train Loss Epoch 16: 2120.460693359375;   Epoch time: 242\n",
      "Average Train Loss Epoch 17: 14452.369140625;   Epoch time: 242\n",
      "Average Train Loss Epoch 18: 4172.78369140625;   Epoch time: 242\n",
      "Average Train Loss Epoch 19: 3562.13623046875;   Epoch time: 242\n",
      "Average Train Loss Epoch 20: 3875.414794921875;   Epoch time: 242\n",
      "Average Train Loss Epoch 21: 2552.465087890625;   Epoch time: 242\n",
      "Average Train Loss Epoch 22: 3450.140625;   Epoch time: 242\n",
      "Average Train Loss Epoch 23: 6952.56982421875;   Epoch time: 242\n",
      "Average Train Loss Epoch 24: 2105.71484375;   Epoch time: 242\n",
      "Average Train Loss Epoch 25: 3061.574951171875;   Epoch time: 242\n",
      "Average Train Loss Epoch 26: 8413.2255859375;   Epoch time: 242\n",
      "Average Train Loss Epoch 27: 3347.289306640625;   Epoch time: 242\n",
      "Average Train Loss Epoch 28: 7078.30908203125;   Epoch time: 242\n",
      "Average Train Loss Epoch 29: 3906.210205078125;   Epoch time: 242\n",
      "Average Train Loss Epoch 30: 4273.45751953125;   Epoch time: 242\n",
      "Average Train Loss Epoch 31: 1864.3140869140625;   Epoch time: 246\n",
      "Average Train Loss Epoch 32: 4623.62890625;   Epoch time: 242\n",
      "Average Train Loss Epoch 33: 3392.435791015625;   Epoch time: 241\n",
      "Average Train Loss Epoch 34: 3680.818115234375;   Epoch time: 241\n",
      "Average Train Loss Epoch 35: 1896.59033203125;   Epoch time: 240\n",
      "Average Train Loss Epoch 36: 6030.39404296875;   Epoch time: 240\n",
      "Average Train Loss Epoch 37: 3936.76025390625;   Epoch time: 240\n",
      "Average Train Loss Epoch 38: 4522.9052734375;   Epoch time: 241\n",
      "Average Train Loss Epoch 39: 4267.36669921875;   Epoch time: 241\n",
      "Average Train Loss Epoch 40: 6969.74462890625;   Epoch time: 242\n",
      "Average Train Loss Epoch 41: 1915.844970703125;   Epoch time: 241\n",
      "Average Train Loss Epoch 42: 2654.83251953125;   Epoch time: 241\n",
      "Average Train Loss Epoch 43: 12121.3017578125;   Epoch time: 241\n",
      "Average Train Loss Epoch 44: 4121.63671875;   Epoch time: 241\n",
      "Average Train Loss Epoch 45: 3443.73974609375;   Epoch time: 241\n",
      "Average Train Loss Epoch 46: 2246.590087890625;   Epoch time: 241\n",
      "Average Train Loss Epoch 47: 2183.2744140625;   Epoch time: 240\n",
      "Average Train Loss Epoch 48: 6933.25634765625;   Epoch time: 240\n",
      "Average Train Loss Epoch 49: 9893.861328125;   Epoch time: 241\n",
      "Average Train Loss Epoch 50: 4417.23974609375;   Epoch time: 241\n",
      "Average Train Loss Epoch 51: 13407.126953125;   Epoch time: 241\n",
      "Average Train Loss Epoch 52: 3082.251953125;   Epoch time: 240\n",
      "Average Train Loss Epoch 53: 3132.05322265625;   Epoch time: 240\n",
      "Average Train Loss Epoch 54: 6615.69873046875;   Epoch time: 240\n",
      "Average Train Loss Epoch 55: 3996.66650390625;   Epoch time: 240\n",
      "Average Train Loss Epoch 56: 4646.11083984375;   Epoch time: 240\n",
      "Average Train Loss Epoch 57: 2541.285888671875;   Epoch time: 240\n",
      "Average Train Loss Epoch 58: 9750.6044921875;   Epoch time: 240\n",
      "Average Train Loss Epoch 59: 4401.7197265625;   Epoch time: 241\n",
      "Average Train Loss Epoch 60: 11032.3310546875;   Epoch time: 241\n",
      "Average Train Loss Epoch 61: 6630.57861328125;   Epoch time: 240\n",
      "Average Train Loss Epoch 62: 3670.163330078125;   Epoch time: 240\n",
      "Average Train Loss Epoch 63: 736496.25;   Epoch time: 241\n",
      "Average Train Loss Epoch 64: 3806.57568359375;   Epoch time: 241\n",
      "Average Train Loss Epoch 65: 2667.18359375;   Epoch time: 240\n",
      "Average Train Loss Epoch 66: 1433.5050048828125;   Epoch time: 240\n",
      "Average Train Loss Epoch 67: 2462.646240234375;   Epoch time: 240\n",
      "Average Train Loss Epoch 68: 2088.8193359375;   Epoch time: 240\n",
      "Average Train Loss Epoch 69: 2112.28759765625;   Epoch time: 240\n",
      "Average Train Loss Epoch 70: 3025.381103515625;   Epoch time: 240\n",
      "Average Train Loss Epoch 71: 2086.44580078125;   Epoch time: 240\n",
      "Average Train Loss Epoch 72: 2314.109375;   Epoch time: 241\n",
      "Average Train Loss Epoch 73: 1396.579345703125;   Epoch time: 240\n",
      "Average Train Loss Epoch 74: 2186.726318359375;   Epoch time: 241\n",
      "Average Train Loss Epoch 75: 2778.14208984375;   Epoch time: 240\n",
      "Average Train Loss Epoch 76: 22804.6015625;   Epoch time: 240\n",
      "Average Train Loss Epoch 77: 3307.459228515625;   Epoch time: 240\n",
      "Average Train Loss Epoch 78: 5792.75390625;   Epoch time: 242\n",
      "Average Train Loss Epoch 79: 24564.396484375;   Epoch time: 241\n",
      "Average Train Loss Epoch 80: 2437.527099609375;   Epoch time: 240\n",
      "Average Train Loss Epoch 81: 5719.87451171875;   Epoch time: 241\n",
      "Average Train Loss Epoch 82: 16938.814453125;   Epoch time: 241\n",
      "Average Train Loss Epoch 83: 3501.31005859375;   Epoch time: 240\n",
      "Average Train Loss Epoch 84: 2093.5380859375;   Epoch time: 240\n",
      "Average Train Loss Epoch 85: 4544.5458984375;   Epoch time: 240\n",
      "Average Train Loss Epoch 86: 2046.4776611328125;   Epoch time: 240\n",
      "Average Train Loss Epoch 87: 2715.539794921875;   Epoch time: 240\n",
      "Average Train Loss Epoch 88: 1662.237548828125;   Epoch time: 240\n",
      "Average Train Loss Epoch 89: 3677.029541015625;   Epoch time: 240\n",
      "Average Train Loss Epoch 90: 3346.205322265625;   Epoch time: 240\n",
      "Average Train Loss Epoch 91: 4785.5029296875;   Epoch time: 241\n",
      "Average Train Loss Epoch 92: 10589.982421875;   Epoch time: 240\n",
      "Average Train Loss Epoch 93: 11080.7763671875;   Epoch time: 240\n",
      "Average Train Loss Epoch 94: 2013.8629150390625;   Epoch time: 240\n",
      "Average Train Loss Epoch 95: 4685.45263671875;   Epoch time: 240\n",
      "Average Train Loss Epoch 96: 2939.5380859375;   Epoch time: 240\n",
      "Average Train Loss Epoch 97: 2976.3251953125;   Epoch time: 240\n",
      "Average Train Loss Epoch 98: 3107.52197265625;   Epoch time: 240\n",
      "Average Train Loss Epoch 99: 4070.419677734375;   Epoch time: 240\n",
      "Average Train Loss Epoch 100: 2268.71533203125;   Epoch time: 240\n",
      "Average Train Loss Epoch 101: 5790.71728515625;   Epoch time: 240\n",
      "Average Train Loss Epoch 102: 3037.235595703125;   Epoch time: 240\n",
      "Average Train Loss Epoch 103: 4511.9580078125;   Epoch time: 240\n",
      "Average Train Loss Epoch 104: 2725.0380859375;   Epoch time: 240\n",
      "Average Train Loss Epoch 105: 2286.686767578125;   Epoch time: 240\n",
      "Average Train Loss Epoch 106: 7318.91259765625;   Epoch time: 239\n",
      "Average Train Loss Epoch 107: 3149.125732421875;   Epoch time: 239\n",
      "Average Train Loss Epoch 108: 6183.634765625;   Epoch time: 240\n",
      "Average Train Loss Epoch 109: 2203.084716796875;   Epoch time: 240\n",
      "Average Train Loss Epoch 110: 7313.52392578125;   Epoch time: 240\n",
      "Average Train Loss Epoch 111: 4010.252685546875;   Epoch time: 240\n",
      "Average Train Loss Epoch 112: 2588.75439453125;   Epoch time: 240\n",
      "Average Train Loss Epoch 113: 2491.943603515625;   Epoch time: 240\n",
      "Average Train Loss Epoch 114: 1779.822021484375;   Epoch time: 240\n",
      "Average Train Loss Epoch 115: 1864.306884765625;   Epoch time: 240\n",
      "Average Train Loss Epoch 116: 2876.81982421875;   Epoch time: 239\n",
      "Average Train Loss Epoch 117: 3316.84521484375;   Epoch time: 240\n",
      "Average Train Loss Epoch 118: 1611.72216796875;   Epoch time: 240\n",
      "Average Train Loss Epoch 119: 8708.1953125;   Epoch time: 240\n",
      "Average Train Loss Epoch 120: 23865.51953125;   Epoch time: 240\n",
      "Average Train Loss Epoch 121: 39089.75;   Epoch time: 240\n",
      "Average Train Loss Epoch 122: 1694.572509765625;   Epoch time: 240\n",
      "Average Train Loss Epoch 123: 2486.542236328125;   Epoch time: 240\n",
      "Average Train Loss Epoch 124: 7643.41064453125;   Epoch time: 240\n",
      "Average Train Loss Epoch 125: 11616.802734375;   Epoch time: 240\n",
      "Average Train Loss Epoch 126: 1534.8551025390625;   Epoch time: 240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss Epoch 127: 2368.1220703125;   Epoch time: 240\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(model_score_step(bb_dict, noisy, gm, gu))\n\u001b[1;32m     16\u001b[0m opti\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 17\u001b[0m \u001b[43mtrain_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m opti\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     20\u001b[0m running_tloss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m train_loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\n",
      "File \u001b[0;32m~/miniconda3/envs/se33/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/se33/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "e_start= 0\n",
    "model_path = make_save_folder(name=f'full_diff_score_t_only')\n",
    "save_per=10\n",
    "avg_vloss=0\n",
    "avg_tloss=0\n",
    "\n",
    "for e in range(e_start, e_start+num_epochs):\n",
    "    \n",
    "    running_tloss = 0 \n",
    "    start = time.time()\n",
    "    for i, bb_dict in enumerate(train_dL):\n",
    "        noisy = se3d.forward_marginal(bb_dict,t_vec=None)\n",
    "        train_loss = torch.sum(model_score_step(bb_dict, noisy, gm, gu))\n",
    "\n",
    "        opti.zero_grad()\n",
    "        train_loss.backward()\n",
    "        opti.step()\n",
    "\n",
    "        running_tloss += train_loss.detach().cpu()\n",
    "    \n",
    "    end = time.time()\n",
    "    avg_tloss = running_tloss/(i+1)\n",
    "    print(f'Average Train Loss Epoch {e}: {avg_tloss};   Epoch time: {end-start:.0f}')\n",
    "    \n",
    "    \n",
    "    if e %save_per==save_per-1:\n",
    "        save_chkpt(f'log/{model_path}', gu, opti, e, B, avg_vloss, avg_tloss)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4816038",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "se33",
   "language": "python",
   "name": "se33"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
