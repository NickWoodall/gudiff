{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a02a1c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import util.npose_util as nu\n",
    "import pathlib\n",
    "import dgl\n",
    "from dgl import backend as F\n",
    "import torch_geometric\n",
    "from torch.utils.data import random_split, DataLoader, Dataset\n",
    "from typing import Dict\n",
    "from torch import Tensor\n",
    "from dgl import DGLGraph\n",
    "from torch import nn\n",
    "from chemical import cos_ideal_NCAC #from RoseTTAFold2\n",
    "from torch import einsum\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf443f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from se3_transformer.model.basis import get_basis, update_basis_with_fused\n",
    "from se3_transformer.model.transformer_reset import Sequential, SE3Transformer\n",
    "from se3_transformer.model.layers.attentiontopK import AttentionBlockSE3\n",
    "from se3_transformer.model.layers.linear import LinearSE3\n",
    "from se3_transformer.model.layers.convolution import ConvSE3, ConvSE3FuseLevel\n",
    "from se3_transformer.model.layers.norm import NormSE3\n",
    "from se3_transformer.model.layers.pooling import GPooling\n",
    "from se3_transformer.runtime.utils import str2bool, to_cuda\n",
    "from se3_transformer.model.fiber import Fiber\n",
    "from se3_transformer.model.transformer import get_populated_edge_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2511634d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Useful numbers\n",
    "# N [-1.45837285,  0 , 0]\n",
    "# CA [0., 0., 0.]\n",
    "# C [0.55221403, 1.41890368, 0.        ]\n",
    "# CB [ 0.52892494, -0.77445692, -1.19923854]\n",
    "\n",
    "if ( hasattr(os, 'ATOM_NAMES') ):\n",
    "    assert( hasattr(os, 'PDB_ORDER') )\n",
    "\n",
    "    ATOM_NAMES = os.ATOM_NAMES\n",
    "    PDB_ORDER = os.PDB_ORDER\n",
    "else:\n",
    "    ATOM_NAMES=['N', 'CA', 'CB', 'C', 'O']\n",
    "    PDB_ORDER = ['N', 'CA', 'C', 'O', 'CB']\n",
    "\n",
    "_byte_atom_names = []\n",
    "_atom_names = []\n",
    "for i, atom_name in enumerate(ATOM_NAMES):\n",
    "    long_name = \" \" + atom_name + \"       \"\n",
    "    _atom_names.append(long_name[:4])\n",
    "    _byte_atom_names.append(atom_name.encode())\n",
    "\n",
    "    globals()[atom_name] = i\n",
    "\n",
    "R = len(ATOM_NAMES)\n",
    "\n",
    "if ( \"N\" not in globals() ):\n",
    "    N = -1\n",
    "if ( \"C\" not in globals() ):\n",
    "    C = -1\n",
    "if ( \"CB\" not in globals() ):\n",
    "    CB = -1\n",
    "\n",
    "\n",
    "_pdb_order = []\n",
    "for name in PDB_ORDER:\n",
    "    _pdb_order.append( ATOM_NAMES.index(name) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e213e885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path_str  = 'data/h4_ca_coords.npz'\n",
    "# test_limit = 1028\n",
    "# rr = np.load(data_path_str)\n",
    "# ca_coords = [rr[f] for f in rr.files][0][:test_limit,:,:3]\n",
    "# ca_coords.shape\n",
    "\n",
    "# getting N-Ca, Ca-C vectors to add as typeI features\n",
    "#apa = apart helices for test/train split\n",
    "#tog = together helices for test/train split\n",
    "apa_path_str  = 'data/h4_apa_coords.npz'\n",
    "tog_path_str  = 'data/h4_tog_coords.npz'\n",
    "\n",
    "#grab the first 3 atoms which are N,CA,C\n",
    "test_limit = 1028\n",
    "rr = np.load(apa_path_str)\n",
    "coords_apa = [rr[f] for f in rr.files][0][:test_limit,:]\n",
    "\n",
    "rr = np.load(tog_path_str)\n",
    "coords_tog = [rr[f] for f in rr.files][0][:test_limit,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4c2ed33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_npose_from_coords(coords_in):\n",
    "    \"\"\"Use N, CA, C coordinates to generate O an CB atoms\"\"\"\n",
    "    rot_mat_cat = np.ones(sum((coords_in.shape[:-1], (1,)), ()))\n",
    "    \n",
    "    coords = np.concatenate((coords_in,rot_mat_cat),axis=-1)\n",
    "    \n",
    "    npose = np.ones((coords_in.shape[0]*5,4)) #5 is atoms per res\n",
    "\n",
    "    by_res = npose.reshape(-1, 5, 4)\n",
    "    \n",
    "    if ( \"N\" in ATOM_NAMES ):\n",
    "        by_res[:,N,:3] = coords_in[:,0,:3]\n",
    "    if ( \"CA\" in ATOM_NAMES ):\n",
    "        by_res[:,CA,:3] = coords_in[:,1,:3]\n",
    "    if ( \"C\" in ATOM_NAMES ):\n",
    "        by_res[:,C,:3] = coords_in[:,2,:3]\n",
    "    if ( \"O\" in ATOM_NAMES ):\n",
    "        by_res[:,O,:3] = nu.build_O(npose)\n",
    "    if ( \"CB\" in ATOM_NAMES ):\n",
    "        tpose = nu.tpose_from_npose(npose)\n",
    "        by_res[:,CB,:] = nu.build_CB(tpose)\n",
    "\n",
    "    return npose\n",
    "\n",
    "def dump_coord_pdb(coords_in, fileOut='fileOut.pdb'):\n",
    "    \n",
    "    npose =  build_npose_from_coords(coords_in)\n",
    "    nu.dump_npdb(npose,fileOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "508327fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.])\n",
      "tensor([0.6000, 0.8000, 0.4000, 0.9000, 0.3000, 1.0000, 0.2000, 1.0000, 0.1000,\n",
      "        1.0000, 0.1000, 1.0000])\n",
      "tensor([1.0000, 0.2000, 0.8000, 0.6000, 0.6000, 0.8000, 0.4000, 0.9000, 0.3000,\n",
      "        1.0000, 0.2000, 1.0000])\n",
      "tensor([ 0.9000, -0.5000,  1.0000,  0.2000,  0.8000,  0.6000,  0.6000,  0.8000,\n",
      "         0.4000,  0.9000,  0.3000,  1.0000])\n",
      "tensor([ 0.4000, -0.9000,  1.0000, -0.3000,  1.0000,  0.3000,  0.8000,  0.7000,\n",
      "         0.6000,  0.8000,  0.4000,  0.9000])\n"
     ]
    }
   ],
   "source": [
    "#goal define edges of\n",
    "#connected backbone 1, \n",
    "#unconnected atoms 0,\n",
    "\n",
    "\n",
    "def get_midpoint(ep_in):\n",
    "    \"\"\"Get midpoint, of each batched set of points\"\"\"\n",
    "    \n",
    "    #calculate midpoint\n",
    "    midpoint = ep_in.sum(axis=1)/np.repeat(ep_in.shape[1], ep_in.shape[2])\n",
    "    \n",
    "    return midpoint\n",
    "\n",
    "\n",
    "def normalize_points(input_xyz, print_dist=False):\n",
    "    \n",
    "    #broadcast to distance matrix [Batch, M, R3] to [Batch,M,1, R3] to [Batch,1,M, R3] to [Batch, M,M, R3] \n",
    "    vec_diff = input_xyz[...,None,:]-input_xyz[...,None,:,:]\n",
    "    dist = np.sqrt(np.sum(np.square(vec_diff),axis=len(input_xyz.shape)))\n",
    "    furthest_dist = np.max(dist)\n",
    "    centroid  = get_midpoint(input_xyz)\n",
    "    if print_dist:\n",
    "        print(f'largest distance {furthest_dist:0.1f}')\n",
    "    \n",
    "    xyz_mean_zero = input_xyz - centroid[:,None,:]\n",
    "    return xyz_mean_zero/furthest_dist\n",
    "\n",
    "def normalize(v):\n",
    "    norm = np.linalg.norm(v)\n",
    "    if norm == 0: \n",
    "        return v\n",
    "    return v / norm\n",
    "\n",
    "def define_graph_edges(n_nodes):\n",
    "    #connected backbone\n",
    "\n",
    "    con_v1 = np.arange(n_nodes-1) #vertex 1 of edges in chronological order\n",
    "    con_v2 = np.arange(1,n_nodes) #vertex 2 of edges in chronological order\n",
    "\n",
    "    ind = con_v1*(n_nodes-1)+con_v2-1 #account for removed self connections (-1)\n",
    "\n",
    "\n",
    "    #unconnected backbone\n",
    "\n",
    "    nodes = np.arange(n_nodes)\n",
    "    v1 = np.repeat(nodes,n_nodes-1) #starting vertices, same number repeated for each edge\n",
    "\n",
    "    start_v2 = np.repeat(np.arange(n_nodes)[None,:],n_nodes,axis=0)\n",
    "    diag_ind = np.diag_indices(n_nodes)\n",
    "    start_v2[diag_ind] = -1 #diagonal of matrix is self connections which we remove (self connections are managed by SE3 Conv channels)\n",
    "    v2 = start_v2[start_v2>-0.5] #remove diagonal and flatten\n",
    "\n",
    "    edge_data = torch.zeros(len(v2))\n",
    "    edge_data[ind] = 1\n",
    "    \n",
    "    return v1,v2,edge_data, ind\n",
    "\n",
    "def make_pe_encoding(n_nodes=65, embed_dim = 12, scale = 1000, cast_type=torch.float32, print_out=False):\n",
    "    #positional encoding of node\n",
    "    i_array = np.arange(1,(embed_dim/2)+1)\n",
    "    wk = (1/(scale**(i_array*2/embed_dim)))\n",
    "    t_array = np.arange(n_nodes)\n",
    "    si = torch.tensor(np.sin(wk*t_array.reshape((-1,1))))\n",
    "    ci = torch.tensor(np.cos(wk*t_array.reshape((-1,1))))\n",
    "    pe = torch.stack((si,ci),axis=2).reshape(t_array.shape[0],embed_dim).type(cast_type)\n",
    "    \n",
    "    if print_out == True:\n",
    "        for x in range(int(n_nodes/12)):\n",
    "            print(np.round(pe[x],1))\n",
    "    \n",
    "    return pe\n",
    "    \n",
    "    \n",
    "#v1,v2,edge_data, ind = define_graph_edges(n_nodes)\n",
    "#norm_p = normalize_points(ca_coords,print_dist=True)\n",
    "pe = make_pe_encoding(n_nodes=65, embed_dim = 12, scale = 10, print_out=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9979efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#v1,v2,edge_data, ind = define_graph_edges(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "321dc5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#?dgl.nn.pytorch.KNNGraph, nearest neighbor graph maker\n",
    "def define_graph(batch_size=8,n_nodes=65):\n",
    "    \n",
    "    v1,v2,edge_data, ind = define_graph_edges(n_nodes)\n",
    "    pe = make_pe_encoding(n_nodes=n_nodes)\n",
    "    \n",
    "    graphList = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        \n",
    "        g = dgl.graph((v1,v2))\n",
    "        g.edata['con'] = edge_data\n",
    "        g.ndata['pe'] = pe\n",
    "\n",
    "        graphList.append(g)\n",
    "        \n",
    "    batched_graph = dgl.batch(graphList)\n",
    "\n",
    "    return batched_graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cc4e2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_UGraph(n_nodes, batch_size, cast_type=torch.float32, cuda_out=True ):\n",
    "    \n",
    "    v1,v2,edge_data, ind = define_graph_edges(n_nodes)\n",
    "    #pe = make_pe_encoding(n_nodes=n_nodes)#pe e\n",
    "    \n",
    "    graphList = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        \n",
    "        g = dgl.graph((v1,v2))\n",
    "        g.edata['con'] = edge_data.type(cast_type).reshape((-1,1))\n",
    "        g.ndata['pos'] = torch.zeros((n_nodes,3),dtype=torch.float32)\n",
    "\n",
    "        graphList.append(g)\n",
    "        \n",
    "    batched_graph = dgl.batch(graphList)\n",
    "    \n",
    "    if cuda_out:\n",
    "        return to_cuda(batched_graph)\n",
    "    else:\n",
    "        return batched_graph\n",
    "\n",
    "def get_edge_features(graph,edge_feature_dim=1):\n",
    "    return {'0': graph.edata['con'][:, :edge_feature_dim, None]}\n",
    "\n",
    "class Graph_4H_Dataset(Dataset):\n",
    "    def __init__(self, coordinates, cast_type=torch.float32):\n",
    "                                    #prots,#length_prot in aa, #residues/aa, #xyz per atom\n",
    "            \n",
    "        #alphaFold reduce by 10\n",
    "        coordinates = coordinates/10\n",
    "            \n",
    "        self.ca_coords = coordinates[:,:,CA,:]\n",
    "        #unsqueeze to stack together later\n",
    "        self.N_CA_vec = torch.tensor(coordinates[:,:,N,:] - coordinates[:,:,CA,:], dtype=cast_type).unsqueeze(2)\n",
    "        self.C_CA_vec = torch.tensor(coordinates[:,:,C,:] - coordinates[:,:,CA,:], dtype=cast_type).unsqueeze(2)\n",
    "        \n",
    "        #set mean to zero and max_distance between points to 1, is this necessary? since se3 transforms distances\n",
    "        #self.norm_ca = normalize_points(self.ca_coords)\n",
    "        \n",
    "        \n",
    "        n_nodes = self.ca_coords.shape[1] \n",
    "        \n",
    "        v1,v2,edge_data, ind = define_graph_edges(n_nodes)\n",
    "        pe = make_pe_encoding(n_nodes=n_nodes)\n",
    "\n",
    "        graphList = []\n",
    "\n",
    "        for i,c in enumerate(self.ca_coords):\n",
    "\n",
    "            g = dgl.graph((v1,v2))\n",
    "            g.edata['con'] = edge_data.type(cast_type).reshape((-1,1))\n",
    "            g.ndata['pe'] = pe\n",
    "            g.ndata['pos'] = torch.tensor(c,dtype=cast_type)\n",
    "            g.ndata['bb_ori'] = torch.cat((self.N_CA_vec[i],self.C_CA_vec[i]),axis=1)\n",
    "            graphList.append(g)\n",
    "        \n",
    "        self.graphList = graphList\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graphList)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.graphList[idx]\n",
    "\n",
    "def _get_relative_pos(graph_in: dgl.DGLGraph) -> torch.Tensor:\n",
    "    x = graph_in.ndata['pos']\n",
    "    src, dst = graph_in.edges()\n",
    "    rel_pos = x[dst] - x[src]\n",
    "    return rel_pos\n",
    "    \n",
    "#needs to be done\n",
    "class H4_DataModule():\n",
    "    \"\"\"\n",
    "    Datamodule wrapping hGen data set. 8 Helical endpoints defining a four helix protein.\n",
    "    \"\"\"\n",
    "    #8 long positional encoding\n",
    "    NODE_FEATURE_DIM_0 = 12\n",
    "    EDGE_FEATURE_DIM = 1 # 0 or 1 helix or loop\n",
    "    NODE_FEATURE_DIM_1 = 2\n",
    "    \n",
    "\n",
    "    def __init__(self,\n",
    "                 coords: np.array, batch_size=8):\n",
    "        \n",
    "        self.GraphDatasetObj = Graph_4H_Dataset(coords)\n",
    "        self.gds = DataLoader(self.GraphDatasetObj, batch_size=batch_size, shuffle=True, drop_last=True,\n",
    "                              collate_fn=self._collate)\n",
    "        \n",
    "    \n",
    "        \n",
    "    def _collate(self, graphs):\n",
    "        batched_graph = dgl.batch(graphs)\n",
    "        #reshape that batched graph to redivide into the individual graphs\n",
    "        edge_feats = {'0': batched_graph.edata['con'][:, :self.EDGE_FEATURE_DIM, None]}\n",
    "        batched_graph.edata['rel_pos'] = _get_relative_pos(batched_graph)\n",
    "        # get node features\n",
    "        node_feats = {'0': batched_graph.ndata['pe'][:, :self.NODE_FEATURE_DIM_0, None],\n",
    "                      '1': batched_graph.ndata['bb_ori'][:,:self.NODE_FEATURE_DIM_1, :3]}\n",
    "        \n",
    "        return (batched_graph, node_feats, edge_feats)\n",
    "    \n",
    "class GaussianNoise(nn.Module):\n",
    "    \"\"\"Gaussian noise regularizer.\n",
    "\n",
    "    Args:\n",
    "        sigma (float, optional): relative standard deviation used to generate the\n",
    "            noise. Relative means that it will be multiplied by the magnitude of\n",
    "            the value your are adding the noise to. This means that sigma can be\n",
    "            the same regardless of the scale of the vector.\n",
    "        is_relative_detach (bool, optional): whether to detach the variable before\n",
    "            computing the scale of the noise. If `False` then the scale of the noise\n",
    "            won't be seen as a constant but something to optimize: this will bias the\n",
    "            network to generate vectors with smaller values.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sigma=0.1):\n",
    "        super().__init__()\n",
    "        self.sigma = sigma\n",
    "        self.noise = torch.tensor(0,dtype=torch.float)\n",
    "\n",
    "    def forward(self,x,scale=1.0):\n",
    "        if self.sigma != 0:\n",
    "            #without modifer mult, mean=0, std_dev=1\n",
    "            sampled_noise = (self.noise.repeat(*x.size()).normal_() * self.sigma*scale).to(x.device)\n",
    "            x = x + sampled_noise\n",
    "        return x, sampled_noise\n",
    "    \n",
    "\n",
    "def rigid_from_3_points(N, Ca, C, non_ideal=False, eps=1e-8):\n",
    "    #N, Ca, C - [B,L, 3]\n",
    "    #R - [B,L, 3, 3], det(R)=1, inv(R) = R.T, R is a rotation matrix\n",
    "    B,L = N.shape[:2]\n",
    "    \n",
    "    v1 = C-Ca\n",
    "    v2 = N-Ca\n",
    "    e1 = v1/(torch.norm(v1, dim=-1, keepdim=True)+eps)\n",
    "    u2 = v2-(torch.einsum('bli, bli -> bl', e1, v2)[...,None]*e1)\n",
    "    e2 = u2/(torch.norm(u2, dim=-1, keepdim=True)+eps)\n",
    "    e3 = torch.cross(e1, e2, dim=-1)\n",
    "    R = torch.cat([e1[...,None], e2[...,None], e3[...,None]], axis=-1) #[B,L,3,3] - rotation matrix\n",
    "    \n",
    "    if non_ideal:\n",
    "        v2 = v2/(torch.norm(v2, dim=-1, keepdim=True)+eps)\n",
    "        cosref = torch.clamp( torch.sum(e1*v2, dim=-1), min=-1.0, max=1.0) # cosine of current N-CA-C bond angle\n",
    "        costgt = cos_ideal_NCAC.item()\n",
    "        cos2del = torch.clamp( cosref*costgt + torch.sqrt((1-cosref*cosref)*(1-costgt*costgt)+eps), min=-1.0, max=1.0 )\n",
    "        cosdel = torch.sqrt(0.5*(1+cos2del)+eps)\n",
    "        sindel = torch.sign(costgt-cosref) * torch.sqrt(1-0.5*(1+cos2del)+eps)\n",
    "        Rp = torch.eye(3, device=N.device).repeat(B,L,1,1)\n",
    "        Rp[:,:,0,0] = cosdel\n",
    "        Rp[:,:,0,1] = -sindel\n",
    "        Rp[:,:,1,0] = sindel\n",
    "        Rp[:,:,1,1] = cosdel\n",
    "    \n",
    "        R = torch.einsum('blij,bljk->blik', R,Rp)\n",
    "\n",
    "    return R, Ca\n",
    "\n",
    "def get_t(N, Ca, C, non_ideal=False, eps=1e-5):\n",
    "    I,B,L=N.shape[:3]\n",
    "    Rs,Ts = rigid_from_3_points(N.view(I*B,L,3), Ca.view(I*B,L,3), C.view(I*B,L,3), non_ideal=non_ideal, eps=eps)\n",
    "    Rs = Rs.view(I,B,L,3,3)\n",
    "    Ts = Ts.view(I,B,L,3)\n",
    "    t = Ts[:,:,None] - Ts[:,:,:,None] # t[0,1] = residue 0 -> residue 1 vector\n",
    "    return einsum('iblkj, iblmk -> iblmj', Rs, t) # (I,B,L,L,3)\n",
    "def FAPE_loss(pred, true,  d_clamp=10.0, d_clamp_inter=30.0, A=10.0, gamma=1.0, eps=1e-6):\n",
    "    '''\n",
    "    Calculate Backbone FAPE loss from RosettaTTAFold\n",
    "    https://github.com/uw-ipd/RoseTTAFold2/blob/main/network/loss.py\n",
    "    Input:\n",
    "        - pred: predicted coordinates (I, B, L, n_atom, 3)\n",
    "        - true: true coordinates (B, L, n_atom, 3)\n",
    "    Output: str loss\n",
    "    '''\n",
    "    I = pred.shape[0]\n",
    "    true = true.unsqueeze(0)\n",
    "    t_tilde_ij = get_t(true[:,:,:,0], true[:,:,:,1], true[:,:,:,2])\n",
    "    t_ij = get_t(pred[:,:,:,0], pred[:,:,:,1], pred[:,:,:,2])\n",
    "\n",
    "    difference = torch.sqrt(torch.square(t_tilde_ij-t_ij).sum(dim=-1) + eps)\n",
    "    eij_label = difference[-1].clone().detach()\n",
    "\n",
    "    clamp = torch.zeros_like(difference)\n",
    "\n",
    "    # intra vs inter#me coded\n",
    "    clamp[:,True] = d_clamp\n",
    "\n",
    "    difference = torch.clamp(difference, max=clamp)\n",
    "    loss = difference / A # (I, B, L, L)\n",
    "\n",
    "    # calculate masked loss (ignore missing regions when calculate loss)\n",
    "    loss = (loss[:,True]).sum(dim=-1) / (torch.ones_like(loss).sum()+eps) # (I)\n",
    "\n",
    "    # weighting loss\n",
    "    w_loss = torch.pow(torch.full((I,), gamma, device=pred.device), torch.arange(I, device=pred.device))\n",
    "    w_loss = torch.flip(w_loss, (0,))\n",
    "    w_loss = w_loss / w_loss.sum()\n",
    "\n",
    "    tot_loss = (w_loss * loss).sum()\n",
    "    \n",
    "    return tot_loss, loss.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11c6127f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def init_lecun_normal(module, scale=1.0):\n",
    "    def truncated_normal(uniform, mu=0.0, sigma=1.0, a=-2, b=2):\n",
    "        normal = torch.distributions.normal.Normal(0, 1)\n",
    "\n",
    "        alpha = (a - mu) / sigma\n",
    "        beta = (b - mu) / sigma\n",
    "\n",
    "        alpha_normal_cdf = normal.cdf(torch.tensor(alpha))\n",
    "        p = alpha_normal_cdf + (normal.cdf(torch.tensor(beta)) - alpha_normal_cdf) * uniform\n",
    "\n",
    "        v = torch.clamp(2 * p - 1, -1 + 1e-8, 1 - 1e-8)\n",
    "        x = mu + sigma * np.sqrt(2) * torch.erfinv(v)\n",
    "        x = torch.clamp(x, a, b)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def sample_truncated_normal(shape, scale=1.0):\n",
    "        stddev = np.sqrt(scale/shape[-1])/.87962566103423978  # shape[-1] = fan_in\n",
    "        return stddev * truncated_normal(torch.rand(shape))\n",
    "\n",
    "    module.weight = torch.nn.Parameter( (sample_truncated_normal(module.weight.shape)) )\n",
    "    return module\n",
    "\n",
    "def init_lecun_normal_param(weight, scale=1.0):\n",
    "    def truncated_normal(uniform, mu=0.0, sigma=1.0, a=-2, b=2):\n",
    "        normal = torch.distributions.normal.Normal(0, 1)\n",
    "\n",
    "        alpha = (a - mu) / sigma\n",
    "        beta = (b - mu) / sigma\n",
    "\n",
    "        alpha_normal_cdf = normal.cdf(torch.tensor(alpha))\n",
    "        p = alpha_normal_cdf + (normal.cdf(torch.tensor(beta)) - alpha_normal_cdf) * uniform\n",
    "\n",
    "        v = torch.clamp(2 * p - 1, -1 + 1e-8, 1 - 1e-8)\n",
    "        x = mu + sigma * np.sqrt(2) * torch.erfinv(v)\n",
    "        x = torch.clamp(x, a, b)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def sample_truncated_normal(shape, scale=1.0):\n",
    "        stddev = np.sqrt(scale/shape[-1])/.87962566103423978  # shape[-1] = fan_in\n",
    "        return stddev * truncated_normal(torch.rand(shape))\n",
    "\n",
    "    weight = torch.nn.Parameter( (sample_truncated_normal(weight.shape)) )\n",
    "    return weight\n",
    "\n",
    "class SE3TransformerWrapper(nn.Module):\n",
    "    \"\"\"SE(3) equivariant GCN with attention\"\"\"\n",
    "    def __init__(self, batch_size=16, num_channels=8, num_degrees=3):\n",
    "        super().__init__()     \n",
    "        \n",
    "\n",
    "        kwargs = dict()\n",
    "        kwargs['pooling'] = None\n",
    "        kwargs['num_layers'] = 4\n",
    "        kwargs['num_heads'] = 8\n",
    "        kwargs['channels_div'] =torch.tensor(1,dtype=torch.int32)\n",
    "        self.se3 = SE3Transformer(\n",
    "                                    fiber_in=Fiber({0:12,1:2}),\n",
    "                                    fiber_hidden=Fiber.create(num_degrees,num_channels),\n",
    "                                    fiber_out=Fiber({1:3}),\n",
    "                                    fiber_edge=Fiber({0: 1}),\n",
    "                                    tensor_cores=False,\n",
    "                                    num_degrees=num_degrees,\n",
    "                                    num_channels=num_channels,\n",
    "                                    use_layer_norm=True, **kwargs)\n",
    "\n",
    "        self.reset_parameter()\n",
    "\n",
    "    def reset_parameter(self):\n",
    "\n",
    "        # make sure linear layer before ReLu are initialized with kaiming_normal_\n",
    "        for n, p in self.se3.named_parameters():\n",
    "            if \"bias\" in n:\n",
    "                nn.init.zeros_(p)\n",
    "            elif len(p.shape) == 1:\n",
    "                continue\n",
    "            else:\n",
    "                if \"radial_func\" not in n:\n",
    "                    p = init_lecun_normal_param(p) \n",
    "                else:\n",
    "                    if \"net.6\" in n:\n",
    "                        nn.init.zeros_(p)\n",
    "                    else:\n",
    "                        nn.init.kaiming_normal_(p, nonlinearity='relu')\n",
    "\n",
    "        ## make last layers to be zero-initialized\n",
    "        #nn.init.zeros_(self.se3.graph_modules[-1].project.weights['0'])\n",
    "        #if self.l1_out > 0:\n",
    "        #    nn.init.zeros_(self.se3.graph_modules[-1].project.weights['1'])\n",
    "        # make last layers to be zero-initialized\n",
    "        #nn.init.zeros_(self.se3.graph_modules[-1].weights['0'])\n",
    "        #if self.l1_out > 0:\n",
    "        nn.init.zeros_(self.se3.graph_modules[-1].weights['1'])\n",
    "\n",
    "    def forward(self, batched_graph, node_features, edge_features):\n",
    "        return self.se3(batched_graph, node_features, edge_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a920f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noise_pred_true(batched_graph, node_feats, gauss_noise, edge_feats, trans, B, L=65):\n",
    "    \n",
    "    true_pos = batched_graph.ndata['pos'].clone()\n",
    "\n",
    "    #add vectors for N and C atomts\n",
    "    CA_t  = true_pos.reshape(B, L, 3)\n",
    "    NC_t = CA_t + node_feats['1'][:,0,:].reshape(B, L, 3)\n",
    "    CC_t = CA_t + node_feats['1'][:,1,:].reshape(B, L, 3)\n",
    "    true =  torch.cat((NC_t,CA_t,CC_t),dim=2).reshape(B,L,3,3)\n",
    "\n",
    "    \n",
    "    batched_graph.ndata['pos'], noise = gauss_noise.forward(batched_graph.ndata['pos'])\n",
    "    \n",
    "    CA_n = batched_graph.ndata['pos'].clone().reshape(B, L, 3)\n",
    "    NC_n = CA_n + node_feats['1'][:,0,:].reshape(B, L, 3)\n",
    "    CC_n = CA_n + node_feats['1'][:,1,:].reshape(B, L, 3)\n",
    "    noise_xyz =  torch.cat((NC_n,CA_n,CC_n),dim=2).reshape(B,L,3,3)\n",
    "\n",
    "\n",
    "    shift= trans.forward(batched_graph, node_feats,edge_feats)\n",
    "    #offset = shift['1'].reshape(B, L, 3)\n",
    "    #pred = torch.add(Ts.reshape(B, L, 3), batched_graph.ndata['pos'].reshape(B, L, 3))\n",
    "    \n",
    "    CA_p = shift['1'][:,0,:].reshape(B, L, 3)+batched_graph.ndata['pos'].reshape(B, L, 3)\n",
    "    NC_p = CA_p + node_feats['1'][:,0,:].reshape(B, L, 3)\n",
    "    CC_p = CA_p + node_feats['1'][:,1,:].reshape(B, L, 3)\n",
    "    pred = torch.cat((NC_p,CA_p,CC_p),dim=2).reshape(B,L,3,3)\n",
    "    \n",
    "    return true.to('cpu').numpy()*10, noise_xyz.to('cpu').numpy()*10, pred.detach().to('cpu').numpy()*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c3e96f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(batched_graph, node_feats, gauss_noise, edge_feats, trans):\n",
    "    \n",
    "    true_pos = batched_graph.ndata['pos'].clone()\n",
    "\n",
    "    #add vectors for \n",
    "    CA_t  = true_pos.reshape(B, L, 3)\n",
    "    NC_t = CA_t + node_feats['1'][:,0,:].reshape(B, L, 3)\n",
    "    CC_t = CA_t + node_feats['1'][:,1,:].reshape(B, L, 3)\n",
    "    true =  torch.cat((NC_t,CA_t,CC_t),dim=2).reshape(B,L,3,3)\n",
    "\n",
    "    \n",
    "    batched_graph.ndata['pos'], noise = gauss_noise.forward(batched_graph.ndata['pos'])\n",
    "\n",
    "\n",
    "    shift= trans.forward(batched_graph, node_feats,edge_feats)\n",
    "    #offset = shift['1'].reshape(B, L, 3)\n",
    "    #pred = torch.add(Ts.reshape(B, L, 3), batched_graph.ndata['pos'].reshape(B, L, 3))\n",
    "    \n",
    "    CA_p = shift['1'][:,0,:].reshape(B, L, 3)+batched_graph.ndata['pos'].reshape(B, L, 3)\n",
    "    NC_p = CA_p + shift['1'][:,1,:].reshape(B, L, 3)+node_feats['1'][:,0,:].reshape(B, L, 3)\n",
    "    CC_p = CA_p + shift['1'][:,2,:].reshape(B, L, 3)+node_feats['1'][:,1,:].reshape(B, L, 3)\n",
    "    pred = torch.cat((NC_p,CA_p,CC_p),dim=2).reshape(B,L,3,3)\n",
    "\n",
    "    tloss, loss = FAPE_loss(pred.unsqueeze(0), true)\n",
    "    \n",
    "    opti.zero_grad()\n",
    "    tloss.backward()\n",
    "    opti.step()\n",
    "    \n",
    "    return tloss.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b5423e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25a595cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.cuda.current_device()\n",
    "\n",
    "batch_size = 16\n",
    "num_channels = 8\n",
    "num_degrees = 3\n",
    "\n",
    "kwargs = dict()\n",
    "kwargs['pooling'] = None\n",
    "kwargs['num_layers'] = 4\n",
    "kwargs['num_heads'] = 8\n",
    "kwargs['channels_div'] =torch.tensor(1,dtype=torch.int32)\n",
    "\n",
    "#channels dive = channels/num_heads\n",
    "\n",
    "# model = SE3Transformer(\n",
    "#         fiber_in=Fiber({0:12,1:2}),\n",
    "#         fiber_hidden=Fiber.create(num_degrees,num_channels),\n",
    "#         fiber_out=Fiber({1:1}),\n",
    "#         fiber_edge=Fiber({0: 1}),\n",
    "#         tensor_cores=False,\n",
    "#         num_degrees=num_degrees,\n",
    "#         num_channels=num_channels,\n",
    "#         **kwargs\n",
    "#     ).to(device)\n",
    "\n",
    "\n",
    "model = SE3TransformerWrapper().to(device)\n",
    "opti = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21c00a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0226, device='cuda:0')\n",
      "tensor(0.0226, device='cuda:0')\n",
      "tensor(0.0226, device='cuda:0')\n",
      "tensor(0.0225, device='cuda:0')\n",
      "tensor(0.0225, device='cuda:0')\n",
      "tensor(0.0226, device='cuda:0')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m     nf \u001b[38;5;241m=\u001b[39m to_cuda(node_feats)\n\u001b[1;32m     11\u001b[0m     ef \u001b[38;5;241m=\u001b[39m to_cuda(edge_feats)\n\u001b[0;32m---> 13\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mef\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     lsum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m out\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, inp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dm\u001b[38;5;241m.\u001b[39mgds):\n",
      "Cell \u001b[0;32mIn[12], line 15\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(batched_graph, node_feats, gauss_noise, edge_feats, trans)\u001b[0m\n\u001b[1;32m      9\u001b[0m true \u001b[38;5;241m=\u001b[39m  torch\u001b[38;5;241m.\u001b[39mcat((NC_t,CA_t,CC_t),dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(B,L,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     12\u001b[0m batched_graph\u001b[38;5;241m.\u001b[39mndata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpos\u001b[39m\u001b[38;5;124m'\u001b[39m], noise \u001b[38;5;241m=\u001b[39m gauss_noise\u001b[38;5;241m.\u001b[39mforward(batched_graph\u001b[38;5;241m.\u001b[39mndata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpos\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 15\u001b[0m shift\u001b[38;5;241m=\u001b[39m \u001b[43mtrans\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatched_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_feats\u001b[49m\u001b[43m,\u001b[49m\u001b[43medge_feats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#offset = shift['1'].reshape(B, L, 3)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#pred = torch.add(Ts.reshape(B, L, 3), batched_graph.ndata['pos'].reshape(B, L, 3))\u001b[39;00m\n\u001b[1;32m     19\u001b[0m CA_p \u001b[38;5;241m=\u001b[39m shift[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m][:,\u001b[38;5;241m0\u001b[39m,:]\u001b[38;5;241m.\u001b[39mreshape(B, L, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m+\u001b[39mbatched_graph\u001b[38;5;241m.\u001b[39mndata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpos\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(B, L, \u001b[38;5;241m3\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 97\u001b[0m, in \u001b[0;36mSE3TransformerWrapper.forward\u001b[0;34m(self, batched_graph, node_features, edge_features)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, batched_graph, node_features, edge_features):\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mse3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatched_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_features\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/se33/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/c/Users/nwood/OneDrive/Desktop/gudiff/se3_transformer/model/transformer_reset.py:150\u001b[0m, in \u001b[0;36mSE3Transformer.forward\u001b[0;34m(self, graph, node_feats, edge_feats, basis, compute_gradients)\u001b[0m\n\u001b[1;32m    145\u001b[0m basis \u001b[38;5;241m=\u001b[39m update_basis_with_fused(basis, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_degree, use_pad_trick\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensor_cores \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory,\n\u001b[1;32m    146\u001b[0m                                 fully_fused\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuse_level \u001b[38;5;241m==\u001b[39m ConvSE3FuseLevel\u001b[38;5;241m.\u001b[39mFULL)\n\u001b[1;32m    148\u001b[0m edge_feats \u001b[38;5;241m=\u001b[39m get_populated_edge_features(graph\u001b[38;5;241m.\u001b[39medata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrel_pos\u001b[39m\u001b[38;5;124m'\u001b[39m], edge_feats)\n\u001b[0;32m--> 150\u001b[0m node_feats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph_modules\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_feats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_feats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbasis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbasis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooling \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooling_module(node_feats, graph\u001b[38;5;241m=\u001b[39mgraph)\n",
      "File \u001b[0;32m~/miniconda3/envs/se33/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/c/Users/nwood/OneDrive/Desktop/gudiff/se3_transformer/model/transformer_reset.py:47\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input, *args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m---> 47\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/se33/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/c/Users/nwood/OneDrive/Desktop/gudiff/se3_transformer/model/layers/attention.py:158\u001b[0m, in \u001b[0;36mAttentionBlockSE3.forward\u001b[0;34m(self, node_features, edge_features, graph, basis)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m nvtx_range(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAttentionBlockSE3\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m nvtx_range(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeys / values\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 158\u001b[0m         fused_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_key_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbasis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m         key, value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_key_value_from_fused(fused_key_value)\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m nvtx_range(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mqueries\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/se33/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/c/Users/nwood/OneDrive/Desktop/gudiff/se3_transformer/model/layers/convolution.py:290\u001b[0m, in \u001b[0;36mConvSE3.forward\u001b[0;34m(self, node_feats, edge_feats, graph, basis)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m nvtx_range(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConvSE3\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    289\u001b[0m     invariant_edge_feats \u001b[38;5;241m=\u001b[39m edge_feats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 290\u001b[0m     src, dst \u001b[38;5;241m=\u001b[39m \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medges\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m     out \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    292\u001b[0m     in_features \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/se33/lib/python3.9/site-packages/dgl/view.py:179\u001b[0m, in \u001b[0;36mHeteroEdgeView.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    178\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return all the edges.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_edges\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "B=batch_size\n",
    "L=65\n",
    "dm = H4_DataModule(coords_tog,batch_size=B)\n",
    "gn = GaussianNoise(sigma=0.1).to('cuda')\n",
    "for e in range(300):\n",
    "    lsum = 0\n",
    "    for i, inp in enumerate(dm.gds):\n",
    "        batched_graph, node_feats, edge_feats = inp\n",
    "        bg = to_cuda(batched_graph)\n",
    "        nf = to_cuda(node_feats)\n",
    "        ef = to_cuda(edge_feats)\n",
    "\n",
    "        out = train_step(bg, nf, gn, ef,model)\n",
    "        lsum += out\n",
    "        \n",
    "    \n",
    "    for x, inp in enumerate(dm.gds):\n",
    "        batched_graph, node_feats, edge_feats = inp\n",
    "        bg = to_cuda(batched_graph)\n",
    "        nf = to_cuda(node_feats)\n",
    "        ef = to_cuda(edge_feats)\n",
    "        true, noise, pred = get_noise_pred_true(bg, nf, gn, ef, model, B, L=65)\n",
    "        dump_coord_pdb(true[0],fileOut=f'output/true_{e}.pdb')\n",
    "        dump_coord_pdb(noise[0],fileOut=f'output/noise_{e}.pdb')\n",
    "        dump_coord_pdb(pred[0],fileOut=f'output/pred_{e}.pdb')\n",
    "        break\n",
    "\n",
    "    print(lsum/i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "df29a391",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, inp in enumerate(dm.gds):\n",
    "    batched_graph, node_feats, edge_feats = inp\n",
    "    bg = to_cuda(batched_graph)\n",
    "    nf = to_cuda(node_feats)\n",
    "    ef = to_cuda(edge_feats)\n",
    "    true, noise, pred = get_noise_pred_true(bg, nf, gn, ef, model, B, L=65)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "83acd786",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_coord_pdb(true[0],fileOut='output/true.pdb')\n",
    "dump_coord_pdb(noise[0],fileOut='output/noise.pdb')\n",
    "dump_coord_pdb(pred[0],fileOut='output/pred.pdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a52b59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c4d236d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 65, 3])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nf['1'][:,0,:].reshape(B, L, 3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d95423c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1296e445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "260/65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe054fde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce32639d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_graph.ndata['pos'] = gauss_noise.forward(batched_graph.ndata['pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb9b7c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(batched_graph, node_feats, edge_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "534c7237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([260, 1, 3])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['1'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08017d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb5010a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "se33",
   "language": "python",
   "name": "se33"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
