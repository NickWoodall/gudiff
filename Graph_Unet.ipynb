{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a02a1c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import dgl\n",
    "from dgl import backend as F\n",
    "import torch_geometric\n",
    "from torch.utils.data import random_split, DataLoader, Dataset\n",
    "from typing import Dict\n",
    "from torch import Tensor\n",
    "from dgl import DGLGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60196380",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0cd0b9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#?torch_geometric.nn.pool.ASAPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf443f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from se3_transformer.model.basis import get_basis, update_basis_with_fused\n",
    "from se3_transformer.model.transformer import Sequential\n",
    "from se3_transformer.model.layers.attentiontopK import AttentionBlockSE3\n",
    "from se3_transformer.model.layers.linear import LinearSE3\n",
    "from se3_transformer.model.layers.convolution import ConvSE3, ConvSE3FuseLevel\n",
    "from se3_transformer.model.layers.norm import NormSE3\n",
    "from se3_transformer.model.layers.pooling import GPooling\n",
    "from se3_transformer.runtime.utils import str2bool, to_cuda\n",
    "from se3_transformer.model.fiber import Fiber\n",
    "from se3_transformer.model.transformer import get_populated_edge_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a687357b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e213e885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 65, 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path_str  = 'data/h4_ca_coords.npz'\n",
    "test_limit = 128\n",
    "rr = np.load(data_path_str)\n",
    "ca_coords = [rr[f] for f in rr.files][0][:test_limit,:,:3]\n",
    "ca_coords.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "508327fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "largest distance 32.8\n",
      "tensor([0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.])\n",
      "tensor([0.6000, 0.8000, 0.4000, 0.9000, 0.3000, 1.0000, 0.2000, 1.0000, 0.1000,\n",
      "        1.0000, 0.1000, 1.0000])\n",
      "tensor([1.0000, 0.2000, 0.8000, 0.6000, 0.6000, 0.8000, 0.4000, 0.9000, 0.3000,\n",
      "        1.0000, 0.2000, 1.0000])\n",
      "tensor([ 0.9000, -0.5000,  1.0000,  0.2000,  0.8000,  0.6000,  0.6000,  0.8000,\n",
      "         0.4000,  0.9000,  0.3000,  1.0000])\n",
      "tensor([ 0.4000, -0.9000,  1.0000, -0.3000,  1.0000,  0.3000,  0.8000,  0.7000,\n",
      "         0.6000,  0.8000,  0.4000,  0.9000])\n"
     ]
    }
   ],
   "source": [
    "#goal define edges of\n",
    "#connected backbone 1, \n",
    "#unconnected atoms 0,\n",
    "\n",
    "\n",
    "def get_midpoint(ep_in):\n",
    "    \"\"\"Get midpoint, of each batched set of points\"\"\"\n",
    "    \n",
    "    #calculate midpoint\n",
    "    midpoint = ep_in.sum(axis=1)/np.repeat(ep_in.shape[1], ep_in.shape[2])\n",
    "    \n",
    "    return midpoint\n",
    "\n",
    "\n",
    "def normalize_points(input_xyz, print_dist=False):\n",
    "    \n",
    "    #broadcast to distance matrix [Batch, M, R3] to [Batch,M,1, R3] to [Batch,1,M, R3] to [Batch, M,M, R3] \n",
    "    vec_diff = input_xyz[...,None,:]-input_xyz[...,None,:,:]\n",
    "    dist = np.sqrt(np.sum(np.square(vec_diff),axis=len(input_xyz.shape)))\n",
    "    furthest_dist = np.max(dist)\n",
    "    centroid  = get_midpoint(input_xyz)\n",
    "    if print_dist:\n",
    "        print(f'largest distance {furthest_dist:0.1f}')\n",
    "    \n",
    "    xyz_mean_zero = input_xyz - centroid[:,None,:]\n",
    "    return xyz_mean_zero/furthest_dist\n",
    "\n",
    "\n",
    "\n",
    "def define_graph_edges(n_nodes):\n",
    "    #connected backbone\n",
    "\n",
    "    con_v1 = np.arange(n_nodes-1) #vertex 1 of edges in chronological order\n",
    "    con_v2 = np.arange(1,n_nodes) #vertex 2 of edges in chronological order\n",
    "\n",
    "    ind = con_v1*(n_nodes-1)+con_v2-1 #account for removed self connections (-1)\n",
    "\n",
    "\n",
    "    #unconnected backbone\n",
    "\n",
    "    nodes = np.arange(n_nodes)\n",
    "    v1 = np.repeat(nodes,n_nodes-1) #starting vertices, same number repeated for each edge\n",
    "\n",
    "    start_v2 = np.repeat(np.arange(n_nodes)[None,:],n_nodes,axis=0)\n",
    "    diag_ind = np.diag_indices(n_nodes)\n",
    "    start_v2[diag_ind] = -1 #diagonal of matrix is self connections which we remove (self connections are managed elsewhere)\n",
    "    v2 = start_v2[start_v2>-0.5] #remove diagonal and flatten\n",
    "\n",
    "    edge_data = torch.zeros(len(v2))\n",
    "    edge_data[ind] = 1\n",
    "    \n",
    "    return v1,v2,edge_data, ind\n",
    "\n",
    "def make_pe_encoding(n_nodes=65, embed_dim = 12, scale = 1000, cast_type=torch.float32, print_out=False):\n",
    "    #positional encoding of node\n",
    "    i_array = np.arange(1,(embed_dim/2)+1)\n",
    "    wk = (1/(scale**(i_array*2/embed_dim)))\n",
    "    t_array = np.arange(n_nodes)\n",
    "    si = torch.tensor(np.sin(wk*t_array.reshape((-1,1))))\n",
    "    ci = torch.tensor(np.cos(wk*t_array.reshape((-1,1))))\n",
    "    pe = torch.stack((si,ci),axis=2).reshape(t_array.shape[0],embed_dim).type(cast_type)\n",
    "    \n",
    "    if print_out == True:\n",
    "        for x in range(int(n_nodes/12)):\n",
    "            print(np.round(pe[x],1))\n",
    "    \n",
    "    return pe\n",
    "    \n",
    "    \n",
    "#v1,v2,edge_data, ind = define_graph_edges(n_nodes)\n",
    "norm_p = normalize_points(ca_coords,print_dist=True)\n",
    "pe = make_pe_encoding(n_nodes=65, embed_dim = 12, scale = 10, print_out=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d9979efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#v1,v2,edge_data, ind = define_graph_edges(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "321dc5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#?dgl.nn.pytorch.KNNGraph, nearest neighbor graph maker\n",
    "def define_graph(batch_size=8,n_nodes=65):\n",
    "    \n",
    "    v1,v2,edge_data, ind = define_graph_edges(n_nodes)\n",
    "    pe = make_pe_encoding(n_nodes=n_nodes)\n",
    "    \n",
    "    graphList = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        \n",
    "        g = dgl.graph((v1,v2))\n",
    "        g.edata['con'] = edge_data\n",
    "        g.ndata['pe'] = pe\n",
    "\n",
    "        graphList.append(g)\n",
    "        \n",
    "    batched_graph = dgl.batch(graphList)\n",
    "\n",
    "    return batched_graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5cc4e2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_UGraph(n_nodes, batch_size, cast_type=torch.float32 ):\n",
    "    \n",
    "    v1,v2,edge_data, ind = define_graph_edges(n_nodes)\n",
    "    #pe = make_pe_encoding(n_nodes=n_nodes)#pe e\n",
    "    \n",
    "    graphList = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        \n",
    "        g = dgl.graph((v1,v2))\n",
    "        g.edata['con'] = edge_data.type(cast_type).reshape((-1,1))\n",
    "        g.ndata['pos'] = torch.zeros((n_nodes,3),dtype=torch.float32)\n",
    "\n",
    "        graphList.append(g)\n",
    "        \n",
    "    batched_graph = dgl.batch(graphList)\n",
    "\n",
    "    return batched_graph\n",
    "\n",
    "class Graph_4H_Dataset(Dataset):\n",
    "    def __init__(self, ca_coordinates, limit=1000, cast_type=torch.float32):\n",
    "        \n",
    "        self.ca_coords = ca_coordinates\n",
    "        self.norm_ca = normalize_points(ca_coordinates)\n",
    "        \n",
    "        n_nodes = self.ca_coords.shape[1] \n",
    "        \n",
    "        v1,v2,edge_data, ind = define_graph_edges(n_nodes)\n",
    "        pe = make_pe_encoding(n_nodes=n_nodes)\n",
    "\n",
    "        graphList = []\n",
    "\n",
    "        for i,c in enumerate(self.norm_ca):\n",
    "\n",
    "            g = dgl.graph((v1,v2))\n",
    "            g.edata['con'] = edge_data.type(cast_type).reshape((-1,1))\n",
    "            g.ndata['pe'] = pe\n",
    "            g.ndata['pos'] = torch.tensor(c,dtype=torch.float32)\n",
    "\n",
    "            graphList.append(g)\n",
    "        \n",
    "        self.graphList = graphList\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graphList)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.graphList[idx]\n",
    "\n",
    "def _get_relative_pos(graph_in: dgl.DGLGraph) -> torch.Tensor:\n",
    "    x = graph_in.ndata['pos']\n",
    "    src, dst = graph_in.edges()\n",
    "    rel_pos = x[dst] - x[src]\n",
    "    return rel_pos\n",
    "    \n",
    "#needs to be done\n",
    "class H4_DataModule():\n",
    "    \"\"\"\n",
    "    Datamodule wrapping hGen data set. 8 Helical endpoints defining a four helix protein.\n",
    "    \"\"\"\n",
    "    #8 long positional encoding\n",
    "    NODE_FEATURE_DIM = 12\n",
    "    EDGE_FEATURE_DIM = 1 # 0 or 1 helix or loop\n",
    "\n",
    "    def __init__(self,\n",
    "                 ca_coords: np.array, batch_size=8):\n",
    "        \n",
    "        self.GraphDatasetObj = Graph_4H_Dataset(ca_coords)\n",
    "        self.gds = DataLoader(self.GraphDatasetObj, batch_size=batch_size, shuffle=True, drop_last=True,\n",
    "                              collate_fn=self._collate)\n",
    "        \n",
    "    \n",
    "        \n",
    "    def _collate(self, graphs):\n",
    "        batched_graph = dgl.batch(graphs)\n",
    "        #reshape that batched graph to redivide into the individual graphs\n",
    "        edge_feats = {'0': batched_graph.edata['con'][:, :self.EDGE_FEATURE_DIM, None]}\n",
    "        batched_graph.edata['rel_pos'] = _get_relative_pos(batched_graph)\n",
    "        # get node features\n",
    "        node_feats = {'0': batched_graph.ndata['pe'][:, :self.NODE_FEATURE_DIM, None]}\n",
    "        \n",
    "        return (batched_graph, node_feats, edge_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e69a49f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topK_se3(graph, feat, xi, k):\n",
    "    #remove this read from graph code, since se3 transformer natively uses pulled out feats from graph\n",
    "    # READOUT_ON_ATTRS = {\n",
    "    #     \"nodes\": (\"ndata\", \"batch_num_nodes\", \"number_of_nodes\"),\n",
    "    #     \"edges\": (\"edata\", \"batch_num_edges\", \"number_of_edges\"),\n",
    "    # }\n",
    "    # _, batch_num_objs_attr, _ = READOUT_ON_ATTRS[\"nodes\"]\n",
    "\n",
    "    # #this is a fancy way of saying 'batch_num_nodes\n",
    "    # data = getattr(bg, \"nodes\")[None].data\n",
    "    # if F.ndim(data[feat]) > 2:\n",
    "    #     raise DGLError(\n",
    "    #         \"Only support {} feature `{}` with dimension less than or\"\n",
    "    #         \" equal to 2\".format(typestr, feat)\n",
    "    #     )\n",
    "    # feat = data[feat]\n",
    "\n",
    "\n",
    "    hidden_size = feat.shape[-1]\n",
    "    batch_num_objs = getattr(bg, 'batch_num_nodes')(None)\n",
    "    batch_size = len(batch_num_objs)\n",
    "    descending = True\n",
    "\n",
    "    length = max(max(F.asnumpy(batch_num_objs)), k) #max k or batch of nodes size\n",
    "    fill_val = -float(\"inf\") if descending else float(\"inf\")\n",
    "    \n",
    "    feat_y = F.pad_packed_tensor(\n",
    "        feat, batch_num_objs, fill_val, l_min=k\n",
    "    )  # (batch_size, l, d)\n",
    "\n",
    "    order = F.argsort(feat_y, 1, descending=descending)\n",
    "    topk_indices_unsort_batch = F.slice_axis(order, 1, 0, k)\n",
    "    #sort to matches original connectivity with define_graph_edges, likely change but probably won't hurt now\n",
    "    topk_indices, tpk_ind = torch.sort(topk_indices_unsort_batch,dim=1) \n",
    "\n",
    "    #get batch shifts\n",
    "    feat_ = F.reshape(feat_y, (-1,))\n",
    "    shift = F.repeat(\n",
    "        F.arange(0, batch_size), k * hidden_size, -1\n",
    "    ) * length * hidden_size + F.cat(\n",
    "        [F.arange(0, hidden_size)] * batch_size * k, -1\n",
    "    )\n",
    "    \n",
    "    shift = F.copy_to(shift, F.context(feat))\n",
    "    topk_indices_ = F.reshape(topk_indices, (-1,)) * hidden_size + shift\n",
    "    #trainable params gather\n",
    "    out_y = F.reshape(F.gather_row(feat_, topk_indices_), (batch_size*k, -1))\n",
    "    out_y = F.replace_inf_with_zero(out_y)\n",
    "    #nodes features gather\n",
    "    out_xi = F.reshape(F.gather_row(xi, topk_indices_), (batch_size*k, -1))\n",
    "    out_xi = F.replace_inf_with_zero(out_xi)\n",
    "    return out_y, out_xi, topk_indices_\n",
    "\n",
    "\n",
    "class TopK_Pool(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    https://arxiv.org/pdf/1905.05178.pdf\n",
    "    Project Node Features to 1D for topK pooling using trainable weights\n",
    "    Only type '0' features coded, would need to mix features between degrees, avoiding this for now\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, fiber_in: Fiber, k=5):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "        fiber_out = Fiber({0: 1}) #convert to 1D of nodes\n",
    "        self.weights = torch.nn.ParameterDict({\n",
    "            str(degree_out): torch.nn.Parameter(\n",
    "                torch.randn(channels_out, fiber_in[degree_out]) / np.sqrt(fiber_in[degree_out]))\n",
    "            for degree_out, channels_out in fiber_out\n",
    "        })\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, features: Dict[str, Tensor], graph: DGLGraph) -> Dict[str, Tensor]:\n",
    "        #add topK selection, sigmoid, return nodes\n",
    "        yi = {\n",
    "            degree: torch.div(self.weights[degree] @ features[degree], self.weights[degree].norm())\n",
    "            for degree, weight in self.weights.items()\n",
    "        }\n",
    "        y_selected, feats_selected, topk_indices_batched = topK_se3(graph, yi['0'], features['0'], self.k)\n",
    "        return torch.sigmoid(y_selected)*feats_selected, topk_indices_batched\n",
    "    \n",
    "class Unpool(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Place features into torch.zeros array\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, features: Dict[str, Tensor], graph: DGLGraph, idx: Tensor, u_features: Dict[str, Tensor]):\n",
    "        idx_count = 0\n",
    "        out_feats = {}\n",
    "        for key,val in features.items():\n",
    "            new_h = val.new_zeros([graph.num_nodes(), val.shape[1], 1])\n",
    "            out_feats[key] = F.scatter_row(new_h,idx[idx_count],val)\n",
    "            idx_count +=1\n",
    "        return out_feats\n",
    "    \n",
    "class Latent_Unpool(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Duplicate Latent onto Graph\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, features: Dict[str, Tensor], graph: DGLGraph, u_features: Dict[str, Tensor]):\n",
    "        out_feats = {}\n",
    "        for key,val in features.items():\n",
    "            new_h = val.repeat_interleave(int(graph.num_nodes()/val.shape[0]),0)\n",
    "            out_feats[key] = torch.add(new_h.unsqueeze(-1),u_features[key])\n",
    "        return out_feats\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "64c7f35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphUnet(nn.Module):\n",
    "\n",
    "    def __init__(self, ks, in_dim, out_dim, dim, act, drop_p):\n",
    "        super(GraphUnet, self).__init__()\n",
    "        self.ks = ks\n",
    "        self.bottom_gcn = GCN(dim, dim, act, drop_p)\n",
    "        self.down_gcns = nn.ModuleList()\n",
    "        self.up_gcns = nn.ModuleList()\n",
    "        self.pools = nn.ModuleList()\n",
    "        self.unpools = nn.ModuleList()\n",
    "        self.l_n = len(ks)\n",
    "        for i in range(self.l_n):\n",
    "            self.down_gcns.append(GCN(dim, dim, act, drop_p))\n",
    "            self.up_gcns.append(GCN(dim, dim, act, drop_p))\n",
    "            self.pools.append(Pool(ks[i], dim, drop_p))\n",
    "            self.unpools.append(Unpool(dim, dim, drop_p))\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        adj_ms = []\n",
    "        indices_list = []\n",
    "        down_outs = []\n",
    "        hs = []\n",
    "        org_h = h\n",
    "        for i in range(self.l_n):\n",
    "            h = self.down_gcns[i](g, h)\n",
    "            adj_ms.append(g)\n",
    "            down_outs.append(h)\n",
    "            g, h, idx = self.pools[i](g, h)\n",
    "            indices_list.append(idx)\n",
    "        h = self.bottom_gcn(g, h)\n",
    "        for i in range(self.l_n):\n",
    "            up_idx = self.l_n - i - 1\n",
    "            g, idx = adj_ms[up_idx], indices_list[up_idx]\n",
    "            g, h = self.unpools[i](g, h, down_outs[up_idx], idx)\n",
    "            h = self.up_gcns[i](g, h)\n",
    "            h = h.add(down_outs[up_idx])\n",
    "            hs.append(h)\n",
    "        h = h.add(org_h)\n",
    "        hs.append(h)\n",
    "        return hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "71dcb7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def prep_for_gcn(graph, xyz_pos, edge_feats_in, idx, max_degree=3):\n",
    "    \n",
    "    src, dst = graph.edges()\n",
    "    \n",
    "    new_pos = F.gather_row(xyz_pos, idx)\n",
    "    rel_pos = F.gather_row(new_pos,dst) - F.gather_row(new_pos,src) \n",
    "    \n",
    "    basis_out = get_basis(rel_pos, max_degree=max_degree,\n",
    "                                   compute_gradients=True,\n",
    "                                   use_pad_trick=False)\n",
    "    basis_out = update_basis_with_fused(basis_out, max_degree, use_pad_trick=False,\n",
    "                                            fully_fused=False)\n",
    "    edge_feats_out = get_populated_edge_features(rel_pos, edge_feats_in)\n",
    "    return basis_out, edge_feats_out\n",
    "\n",
    "class Sequential(torch.nn.Sequential):\n",
    "    \"\"\" Sequential module with arbitrary forward args and kwargs. Used to pass graph, basis and edge features. \"\"\"\n",
    "\n",
    "    def forward(self, input, *args, **kwargs):\n",
    "        for module in self:\n",
    "            input = module(input, *args, **kwargs)\n",
    "        return input\n",
    "\n",
    "class GraphUNet(torch.nn.Module):\n",
    "    def __init__(self,ks = [5],\n",
    "                 batch_size = 8,\n",
    "                 in_dim=12,\n",
    "                 ndf_mult=12,\n",
    "                 max_degree=3,\n",
    "                 num_heads = 8,\n",
    "                 channels_div=2,\n",
    "                 batchsize=8,\n",
    "                 max_nodes = 65):\n",
    "        super().__init__()\n",
    "        self.edge_feature_dim = 1\n",
    "        \n",
    "        \n",
    "        self.ks = ks\n",
    "        \n",
    "        self.down_gcns = nn.ModuleList()\n",
    "        self.up_gcns = nn.ModuleList()\n",
    "        self.pools = nn.ModuleList()\n",
    "        self.unpools = nn.ModuleList()\n",
    "        \n",
    "        self.l_n = len(ks)\n",
    "        \n",
    "        out_dim = in_dim*ndf_mult\n",
    "        \n",
    "        for i in range(self.l_n):\n",
    "            self.down_gcns.append(AttentionBlockSE3( fiber_in= Fiber({0: in_dim}),\n",
    "                                                     fiber_out  = Fiber({0: out_dim}),\n",
    "                                                     fiber_edge = Fiber({0: self.edge_feature_dim}),\n",
    "                                                     num_heads=num_heads,\n",
    "                                                     channels_div=channels_div,\n",
    "                                                     use_layer_norm=True,\n",
    "                                                     max_degree=max_degree,\n",
    "                                                     fuse_level=ConvSE3FuseLevel.NONE,\n",
    "                                                     low_memory='True'))\n",
    "        \n",
    "            self.pools.append(TopK_Pool(Fiber({0: out_dim}), k=ks[i]))\n",
    "                                  \n",
    "            in_dim = out_dim\n",
    "            out_dim = in_dim*ndf_mult\n",
    "                                  \n",
    "        self.bottom_gcn = AttentionBlockSE3( fiber_in= Fiber({0: in_dim}),\n",
    "                                                     fiber_out  = Fiber({0: out_dim}),\n",
    "                                                     fiber_edge = Fiber({0: self.edge_feature_dim}),\n",
    "                                                     num_heads=num_heads,\n",
    "                                                     channels_div=channels_div,\n",
    "                                                     use_layer_norm=True,\n",
    "                                                     max_degree=max_degree,\n",
    "                                                     fuse_level=ConvSE3FuseLevel.NONE,\n",
    "                                                     low_memory='True')\n",
    "        \n",
    "        self.global_pool = GPooling(pool='avg', feat_type=0)\n",
    "        self.latent_unpool = Latent_Unpool()\n",
    "        \n",
    "        in_dim = out_dim\n",
    "        out_dim = out_dim/ndf_mult\n",
    "                                          \n",
    "        for i in range(self.l_n,0,-1):\n",
    "            self.up_gcns.append(AttentionBlockSE3( fiber_in= Fiber({0: in_dim}),\n",
    "                                                     fiber_out  = Fiber({0: out_dim}),\n",
    "                                                     fiber_edge = Fiber({0: self.edge_feature_dim}),\n",
    "                                                     num_heads=num_heads,\n",
    "                                                     channels_div=channels_div,\n",
    "                                                     use_layer_norm=True,\n",
    "                                                     max_degree=max_degree,\n",
    "                                                     fuse_level=ConvSE3FuseLevel.NONE,\n",
    "                                                     low_memory='True'))\n",
    "        \n",
    "            self.unpools.append(Unpool())\n",
    "            \n",
    "            in_dim = out_dim\n",
    "            out_dim = out_dim/ndf_mult\n",
    "            \n",
    "        self.top_gcn = AttentionBlockSE3( fiber_in= Fiber({0: in_dim}),\n",
    "                                                     fiber_out  = Fiber({0: out_dim}),\n",
    "                                                     fiber_edge = Fiber({0: self.edge_feature_dim}),\n",
    "                                                     num_heads=num_heads,\n",
    "                                                     channels_div=channels_div,\n",
    "                                                     use_layer_norm=True,\n",
    "                                                     max_degree=max_degree,\n",
    "                                                     fuse_level=ConvSE3FuseLevel.NONE,\n",
    "                                                     low_memory='True')\n",
    "        \n",
    "        self.graph_list = [define_UGraph(max_nodes, batch_size, cast_type=torch.float32 )]\n",
    "        for i in range(self.l_n):\n",
    "            max_nodes = ks[i]\n",
    "            self.graph_list.append(define_UGraph(max_nodes, batch_size, cast_type=torch.float32 ))\n",
    "            \n",
    "\n",
    "            \n",
    "#     def forward(self, node_feats, rel_pos, ed):\n",
    "        \n",
    "#         indices_list = []\n",
    "#         down_outs = []\n",
    "        \n",
    "#         for i in range(self.l_n):\n",
    "#             feats\n",
    "        \n",
    "#         def forward(self, g, h):\n",
    "#         adj_ms = []\n",
    "#         indices_list = []\n",
    "#         down_outs = []\n",
    "#         hs = []\n",
    "#         org_h = h\n",
    "#         for i in range(self.l_n):\n",
    "            \n",
    "#             basis = get_basis(rel_pos, max_degree=max_degree,\n",
    "#                                    compute_gradients=True,\n",
    "#                                    use_pad_trick=False)\n",
    "            \n",
    "#             h = self.down_gcns[i](self.graph_list[i], h)\n",
    "            \n",
    "            \n",
    "#             adj_ms.append(g)\n",
    "#             down_outs.append(h)\n",
    "#             g, h, idx = self.pools[i](g, h)\n",
    "#             indices_list.append(idx)\n",
    "#         h = self.bottom_gcn(g, h)\n",
    "#         for i in range(self.l_n):\n",
    "#             up_idx = self.l_n - i - 1\n",
    "#             g, idx = adj_ms[up_idx], indices_list[up_idx]\n",
    "#             g, h = self.unpools[i](g, h, down_outs[up_idx], idx)\n",
    "#             h = self.up_gcns[i](g, h)\n",
    "#             h = h.add(down_outs[up_idx])\n",
    "#             hs.append(h)\n",
    "#         h = h.add(org_h)\n",
    "#         hs.append(h)\n",
    "#         return hs\n",
    "                                \n",
    "        \n",
    "        \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "203936be",
   "metadata": {},
   "outputs": [],
   "source": [
    "gu = GraphUNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c0e71e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, inp in enumerate(dm.gds):\n",
    "    batched_graph, node_feats, edge_feats = inp\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e8c56f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes=520, num_edges=33280,\n",
       "      ndata_schemes={'pos': Scheme(shape=(3,), dtype=torch.float32)}\n",
       "      edata_schemes={'con': Scheme(shape=(1,), dtype=torch.float32)})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = batched_graph.ndata['pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ad83003c",
   "metadata": {},
   "outputs": [],
   "source": [
    "basis_out, edge_feats_out = prep_for_gcn(gu.graph_list[0], pos, edge_feats, gu.graph_list[0].nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "abebc3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nwoodall/miniconda3/envs/se33/lib/python3.9/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
      "/home/nwoodall/miniconda3/envs/se33/lib/python3.9/site-packages/dgl/backend/pytorch/tensor.py:449: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), (\n"
     ]
    }
   ],
   "source": [
    "out = gu.down_gcns[0].forward(node_feats, edge_feats_out,graph=gu.graph_list[0],basis=basis_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99825a0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c52b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, inp in enumerate(dm.gds):\n",
    "    batched_graph, node_feats, edge_feats = inp\n",
    "    break\n",
    "    \n",
    "bg = to_cuda(batched_graph)\n",
    "edge_feats = to_cuda(edge_feats)\n",
    "node_feats = to_cuda(node_feats)\n",
    "\n",
    "basis = get_basis(bg.edata['rel_pos'], max_degree=max_degree,\n",
    "                                   compute_gradients=True,\n",
    "                                   use_pad_trick=False)\n",
    "\n",
    "#need to add basis fused here?\n",
    "\n",
    "basis = update_basis_with_fused(basis, max_degree, use_pad_trick=False,\n",
    "                                        fully_fused=False)\n",
    "\n",
    "#concatenate on the distances of the edge based on 'rel pos'\n",
    "edge_feats_cat = get_populated_edge_features(bg.edata['rel_pos'], edge_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2c98c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3ccf34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = H4_DataModule(ca_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d86c1e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nodes = 65\n",
    "NODE_FEATURE_DIM = 12\n",
    "EDGE_FEATURE_DIM = 1 # probably expand to [2] one hot primary connect \n",
    "num_degrees = 4 # how many levels of spherical harmonics to use\n",
    "num_channels = 8 # how many\n",
    "num_heads = 4\n",
    "channels_div = 2\n",
    "max_degree = 4\n",
    "\n",
    "use_layer_norm = True\n",
    "\n",
    "fuse_level = ConvSE3FuseLevel.NONE\n",
    "\n",
    "fiber_in=Fiber({0: NODE_FEATURE_DIM})\n",
    "fiber_hidden=Fiber({0: num_degrees * num_channels})\n",
    "fiber_edge=Fiber({0: EDGE_FEATURE_DIM})\n",
    "fiber_out = Fiber({0: num_degrees * num_channels}) # can this be arbitrary, or projected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "982dfb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "ablock = AttentionBlockSE3(fiber_in=fiber_in,\n",
    "               fiber_out=fiber_hidden,\n",
    "               fiber_edge=fiber_edge,\n",
    "               num_heads=num_heads,\n",
    "               channels_div=channels_div,\n",
    "               use_layer_norm=use_layer_norm,\n",
    "               max_degree=max_degree,\n",
    "               fuse_level=fuse_level,\n",
    "               low_memory='True')\n",
    "acuda = ablock.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3dce2b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = TopK_Pool(fiber_hidden)\n",
    "tk_cuda = tk.to('cuda')\n",
    "# tblock = [ablock,tk]\n",
    "# model = Sequential(*tblock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "055cf9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, inp in enumerate(dm.gds):\n",
    "    batched_graph, node_feats, edge_feats = inp\n",
    "    break\n",
    "    \n",
    "bg = to_cuda(batched_graph)\n",
    "edge_feats = to_cuda(edge_feats)\n",
    "node_feats = to_cuda(node_feats)\n",
    "\n",
    "basis = get_basis(bg.edata['rel_pos'], max_degree=max_degree,\n",
    "                                   compute_gradients=True,\n",
    "                                   use_pad_trick=False)\n",
    "\n",
    "#need to add basis fused here?\n",
    "\n",
    "basis = update_basis_with_fused(basis, max_degree, use_pad_trick=False,\n",
    "                                        fully_fused=False)\n",
    "\n",
    "#concatenate on the distances of the edge based on 'rel pos'\n",
    "edge_feats_cat = get_populated_edge_features(bg.edata['rel_pos'], edge_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c373348",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nwoodall/miniconda3/envs/se33/lib/python3.9/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
      "/home/nwoodall/miniconda3/envs/se33/lib/python3.9/site-packages/dgl/backend/pytorch/tensor.py:449: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), (\n"
     ]
    }
   ],
   "source": [
    "out = acuda.forward(node_feats, edge_feats_cat,graph=bg,basis=basis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9e1a32ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_feat_1, inde = tk.forward(out, bg)\n",
    "ndf1 = {}\n",
    "ndf1['0'] = node_feat_1.unsqueeze(-1)\n",
    "\n",
    "new_pos = F.gather_row(bg.ndata['pos'], inde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9081390",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0457cf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define subgraph 1, and rel pos indices \n",
    "\n",
    "bg_pool1 = define_UGraph(tk.k,batch_size=8) \n",
    "src, dst = bg_pool1.edges()\n",
    "src_pool1 = to_cuda(src)\n",
    "dst_pool1 = to_cuda(dst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "12da583e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#\n",
    "edge_feats_1 = {'0': bg_pool1.edata['con'][:, :1, None]}\n",
    "\n",
    "edge_feats_1 = to_cuda(edge_feats_1)\n",
    "\n",
    "\n",
    "rel_pos_pool1 = F.gather_row(new_pos,dst_pool1) - F.gather_row(new_pos,src_pool1) \n",
    "#rel_pos_pool1 = \n",
    "\n",
    "edge_feats_1_cat = get_populated_edge_features(rel_pos_pool1, edge_feats_1)\n",
    "basis_1 = get_basis(rel_pos_pool1, max_degree=max_degree,\n",
    "                                   compute_gradients=True,\n",
    "                                   use_pad_trick=False)\n",
    "basis_1 = update_basis_with_fused(basis_1, max_degree, use_pad_trick=False,\n",
    "                                        fully_fused=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "53627267",
   "metadata": {},
   "outputs": [],
   "source": [
    "NODE_FEATURE_DIM = 32\n",
    "EDGE_FEATURE_DIM = 1 # \n",
    "num_degrees = 4 # how many levels of spherical harmonics to use\n",
    "num_channels = 16 # how many\n",
    "num_heads = 8\n",
    "channels_div = 2\n",
    "max_degree = 4\n",
    "\n",
    "use_layer_norm = True\n",
    "fuse_level = ConvSE3FuseLevel.NONE\n",
    "\n",
    "\n",
    "fiber_in2=Fiber({0: NODE_FEATURE_DIM})\n",
    "fiber_hidden2=Fiber({0: num_degrees * num_channels*4})\n",
    "fiber_edge2=Fiber({0: EDGE_FEATURE_DIM})\n",
    "#fiber_out2 = Fiber({0: num_degrees * num_channels * 4}) # can this be arbitrary, or projected\n",
    "ablock2 = AttentionBlockSE3(fiber_in=fiber_in2,\n",
    "               fiber_out=fiber_hidden2,\n",
    "               fiber_edge=fiber_edge2,\n",
    "               num_heads=num_heads,\n",
    "               channels_div=channels_div,\n",
    "               use_layer_norm=use_layer_norm,\n",
    "               max_degree=max_degree,\n",
    "               fuse_level=fuse_level,\n",
    "               low_memory='True')\n",
    "acuda2 = ablock2.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "58ca1735",
   "metadata": {},
   "outputs": [],
   "source": [
    "out2 = acuda2.forward(ndf1, edge_feats_1_cat, graph=to_cuda(bg_pool1), basis=basis_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d5cbf2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#i think i need to build a new graph after the pool, ugh, ugh ,ugh\n",
    "#unpooling should be easier just add onto old."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aca7f740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 256, 1])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2['0'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "56efbd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_pooling_module = GPooling(pool='max', feat_type=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b2ee7c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_pool = global_pooling_module(out2, to_cuda(bg_pool1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e5552624",
   "metadata": {},
   "outputs": [],
   "source": [
    "lp = Latent_Unpool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "be0859a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'u_features'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbg_pool1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mout2\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'u_features'"
     ]
    }
   ],
   "source": [
    "lp.forward(latent_pool, bg_pool1,out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9186d6ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8506f0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_pooling_module = GPooling(pool='max', feat_type=0)\n",
    "latent_pool = global_pooling_module(out2, to_cuda(bg_pool1))\n",
    "node_feat_up1 = latent_pool.repeat_interleave(5,0) #copy pool to all new nodes\n",
    "node_feat_up1  = torch.add(node_feat_up1.unsqueeze(-1),out2['0']) #unet add "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "53f4ba63",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_feat_up1 = latent_pool.repeat_interleave(5,0) #copy pool to all new nodes\n",
    "node_feat_up1  = torch.add(node_feat_up1.unsqueeze(-1),out2['0']) #unet add \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ad34c923",
   "metadata": {},
   "outputs": [],
   "source": [
    "NODE_FEATURE_DIM = 256\n",
    "EDGE_FEATURE_DIM = 1 # \n",
    "num_degrees = 4 # how many levels of spherical harmonics to use\n",
    "num_channels = 4 # how many\n",
    "num_heads = 4\n",
    "channels_div = 1\n",
    "max_degree = 4\n",
    "\n",
    "use_layer_norm = True\n",
    "fuse_level = ConvSE3FuseLevel.NONE\n",
    "\n",
    "\n",
    "fiber_in3=Fiber({0: NODE_FEATURE_DIM})\n",
    "fiber_hidden3=Fiber({0: num_degrees * num_channels*2})\n",
    "fiber_edge3=Fiber({0: EDGE_FEATURE_DIM})\n",
    "#fiber_out2 = Fiber({0: num_degrees * num_channels * 4}) # can this be arbitrary, or projected\n",
    "ablock3 = AttentionBlockSE3(fiber_in=fiber_in3,\n",
    "               fiber_out=fiber_hidden3,\n",
    "               fiber_edge=fiber_edge3,\n",
    "               num_heads=num_heads,\n",
    "               channels_div=channels_div,\n",
    "               use_layer_norm=use_layer_norm,\n",
    "               max_degree=max_degree,\n",
    "               fuse_level=fuse_level,\n",
    "               low_memory='True')\n",
    "acuda3 = ablock3.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "22c19a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "out3 = acuda3.forward({'0':node_feat_up1}, edge_feats_1_cat, graph=to_cuda(bg_pool1), basis=basis_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "311ff1ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 32, 1])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out3['0'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e1ef0781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([520, 32, 1])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#unpool \n",
    "\n",
    "node_feat_up2 = torch.zeros((bg.num_nodes(), out3['0'].shape[1],1 )).to('cuda')\n",
    "dd=F.scatter_row(node_feat_up2,inde,out3['0'])\n",
    "\n",
    "#add U-net\n",
    "pad= dd.shape[1]-node_feats['0'].shape[1]\n",
    "unet_add = torch.cat((node_feats['0'], torch.zeros(node_feats['0'].shape[0],pad ,1).to('cuda')), 1)\n",
    "\n",
    "torch.add(unet_add,dd).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "4bfa8ede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([520, 32, 1])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upool = Unpool()\n",
    "inde_list = [inde]\n",
    "upool(out3,bg,inde_list,node_feats)['0'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d20106b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([520, 32, 1]) torch.Size([520, 20, 1])\n"
     ]
    }
   ],
   "source": [
    "indelist = [inde]\n",
    "idx_count = 0\n",
    "out_feats = {}\n",
    "for key,val in out3.items():\n",
    "    new_h = val.new_zeros([bg.num_nodes(), val.shape[1],1])\n",
    "    pad = val.new_zeros([bg.num_nodes(), new_h.shape[1]-node_feats[key].shape[1],1])\n",
    "    print(new_h.shape,pad.shape)\n",
    "    F.scatter_row(new_h,indelist[idx_count],val)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d5fff63b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inde.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7be6edb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unpool(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Place features into torch.zeros array\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, features: Dict[str, Tensor], graph: DGLGraph, idx: Tensor, u_features: Dict[str, Tensor]):\n",
    "        idx_count = 0\n",
    "        out_feats = {}\n",
    "        for key,val in features.items():\n",
    "            new_h = val.new_zeros([graph.num_nodes(), val.shape[1], 1])\n",
    "            pad = val.new_zeros([graph.num_nodes(), new_h.shape[1]-u_features[key].shape[1],1])\n",
    "            out_feats[key] = torch.add(F.scatter_row(new_h,idx[idx_count],val),torch.cat((u_features[key],pad),1))\n",
    "            idx_count +=1\n",
    "        return out_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "24d75eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_add = torch.cat((node_feats['0'], torch.zeros(node_feats['0'].shape[0],dd.shape[1]-node_feats['0'].shape[1],1).to('cuda')), 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1761af37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 45,  46,  47,  48,  49, 110, 111, 112, 113, 114, 175, 176, 177, 178,\n",
       "        179, 240, 241, 242, 243, 244, 305, 306, 307, 308, 309, 370, 371, 372,\n",
       "        373, 374, 435, 436, 437, 438, 439, 500, 501, 502, 503, 504],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "725d1b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "?torch.scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526b4467",
   "metadata": {},
   "outputs": [],
   "source": [
    "F.scatter_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "31e61428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 32, 1])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out3['0'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3960a4d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 256, 1])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2['0'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6681886e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 256])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_feat_up1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8db81890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 256])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_pool.repeat_interleave(5,0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bb29fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unpooling\n",
    "\n",
    "#need torch.zeros size of previous graph\n",
    "torch.zeros((bg_pool1.num_nodes,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2c19257d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes=40, num_edges=160,\n",
       "      ndata_schemes={'pe': Scheme(shape=(12,), dtype=torch.float32), 'pos': Scheme(shape=(3,), dtype=torch.float32)}\n",
       "      edata_schemes={'con': Scheme(shape=(1,), dtype=torch.float32)})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bg_pool1.num_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7e53d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f350f97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1432f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a1826a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c26e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, features: Dict[str, Tensor], graph: DGLGraph, **kwargs) -> Tensor:\n",
    "        pooled = self.pool(graph, features[str(self.feat_type)])\n",
    "        return pooled.squeeze(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c86773",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remake without pulling from graph?\n",
    "\n",
    "READOUT_ON_ATTRS = {\n",
    "    \"nodes\": (\"ndata\", \"batch_num_nodes\", \"number_of_nodes\"),\n",
    "    \"edges\": (\"edata\", \"batch_num_edges\", \"number_of_edges\"),\n",
    "}\n",
    "\n",
    "def _topk_on(graph, typestr, feat, k, descending, sortby, ntype_or_etype):\n",
    "    \"\"\"Internal function to take graph-wise top-k node/edge features of\n",
    "    field :attr:`feat` in :attr:`graph` ranked by keys at given\n",
    "    index :attr:`sortby`. If :attr:`descending` is set to False, return the\n",
    "    k smallest elements instead.\n",
    "\n",
    "    Parameters\n",
    "    ---------\n",
    "    graph : DGLGraph\n",
    "        The graph\n",
    "    typestr : str\n",
    "        'nodes' or 'edges'\n",
    "    feat : str\n",
    "        The feature field name.\n",
    "    k : int\n",
    "        The :math:`k` in \"top-:math`k`\".\n",
    "    descending : bool\n",
    "        Controls whether to return the largest or smallest elements,\n",
    "         defaults to True.\n",
    "    sortby : int\n",
    "        The key index we sort :attr:`feat` on, if set to None, we sort\n",
    "        the whole :attr:`feat`.\n",
    "    ntype_or_etype : str, tuple of str\n",
    "        Node/edge type.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sorted_feat : Tensor\n",
    "        A tensor with shape :math:`(B, K, D)`, where\n",
    "        :math:`B` is the batch size of the input graph.\n",
    "    sorted_idx : Tensor\n",
    "        A tensor with shape :math:`(B, K)`(:math:`(B, K, D)` if sortby\n",
    "        is set to None), where\n",
    "        :math:`B` is the batch size of the input graph, :math:`D`\n",
    "        is the feature size.\n",
    "\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    If an example has :math:`n` nodes/edges and :math:`n<k`, in the first\n",
    "    returned tensor the :math:`n+1` to :math:`k`th rows would be padded\n",
    "    with all zero; in the second returned tensor, the behavior of :math:`n+1`\n",
    "    to :math:`k`th elements is not defined.\n",
    "    \"\"\"\n",
    "    _, batch_num_objs_attr, _ = READOUT_ON_ATTRS[typestr]\n",
    "    data = getattr(graph, typestr)[ntype_or_etype].data\n",
    "    if F.ndim(data[feat]) > 2:\n",
    "        raise DGLError(\n",
    "            \"Only support {} feature `{}` with dimension less than or\"\n",
    "            \" equal to 2\".format(typestr, feat)\n",
    "        )\n",
    "    feat = data[feat]\n",
    "    hidden_size = F.shape(feat)[-1]\n",
    "    batch_num_objs = getattr(graph, batch_num_objs_attr)(ntype_or_etype)\n",
    "    batch_size = len(batch_num_objs)\n",
    "    length = max(max(F.asnumpy(batch_num_objs)), k)\n",
    "    fill_val = -float(\"inf\") if descending else float(\"inf\")\n",
    "    feat_ = F.pad_packed_tensor(\n",
    "        feat, batch_num_objs, fill_val, l_min=k\n",
    "    )  # (batch_size, l, d)\n",
    "\n",
    "    if F.backend_name == \"pytorch\" and sortby is not None:\n",
    "        # PyTorch's implementation of top-K\n",
    "        keys = feat_[..., sortby]  # (batch_size, l)\n",
    "        return _topk_torch(keys, k, descending, feat_)\n",
    "    else:\n",
    "        # Fallback to framework-agnostic implementation of top-K\n",
    "        if sortby is not None:\n",
    "            keys = F.squeeze(F.slice_axis(feat_, -1, sortby, sortby + 1), -1)\n",
    "            order = F.argsort(keys, -1, descending=descending)\n",
    "        else:\n",
    "            order = F.argsort(feat_, 1, descending=descending)\n",
    "        topk_indices = F.slice_axis(order, 1, 0, k)\n",
    "\n",
    "        if sortby is not None:\n",
    "            feat_ = F.reshape(feat_, (batch_size * length, -1))\n",
    "            shift = F.repeat(F.arange(0, batch_size) * length, k, -1)\n",
    "            shift = F.copy_to(shift, F.context(feat))\n",
    "            topk_indices_ = F.reshape(topk_indices, (-1,)) + shift\n",
    "        else:\n",
    "            feat_ = F.reshape(feat_, (-1,))\n",
    "            shift = F.repeat(\n",
    "                F.arange(0, batch_size), k * hidden_size, -1\n",
    "            ) * length * hidden_size + F.cat(\n",
    "                [F.arange(0, hidden_size)] * batch_size * k, -1\n",
    "            )\n",
    "            shift = F.copy_to(shift, F.context(feat))\n",
    "            topk_indices_ = F.reshape(topk_indices, (-1,)) * hidden_size + shift\n",
    "        out = F.reshape(F.gather_row(feat_, topk_indices_), (batch_size, k, -1))\n",
    "        out = F.replace_inf_with_zero(out)\n",
    "        return out, topk_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4d4b0195",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(8):\n",
    "    for y in range(30):\n",
    "        if len(np.unique(sort_ind[x][:,y].cpu().numpy())) != 30:\n",
    "            print(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0c27f477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2111, -0.0900,  0.7318,  0.2344, -0.0669,  0.0927,  0.5824,  0.4390,\n",
       "          0.0690,  0.5615,  0.7202,  0.1591,  0.5839,  0.1566,  0.3246,  0.1450,\n",
       "          0.8620,  0.5528,  1.3103,  0.5718,  0.4452, -0.5404,  0.4007,  0.3480,\n",
       "          0.4214,  0.7397,  0.1821,  0.0995,  0.1473,  0.8472,  0.4826,  0.7203],\n",
       "        [ 1.2044, -0.0912,  0.7244,  0.2296, -0.1206,  0.0902,  0.5798,  0.4309,\n",
       "          0.0607,  0.5256,  0.7131,  0.1099,  0.5822,  0.1490,  0.3212,  0.1406,\n",
       "          0.8589,  0.5515,  1.3042,  0.5710,  0.4410, -0.5436,  0.3697,  0.3449,\n",
       "          0.4145,  0.7275,  0.1739,  0.0936,  0.1362,  0.8349,  0.4770,  0.7097],\n",
       "        [ 1.2012, -0.0977,  0.7028,  0.2276, -0.1264,  0.0872,  0.5759,  0.4240,\n",
       "          0.0556,  0.5194,  0.6933,  0.0538,  0.5749,  0.1320,  0.3177,  0.1326,\n",
       "          0.8483,  0.5320,  1.2934,  0.5513,  0.4327, -0.5531,  0.3374,  0.3360,\n",
       "          0.4096,  0.7245,  0.1727,  0.0884,  0.1138,  0.8332,  0.4558,  0.6915],\n",
       "        [ 1.1841, -0.0988,  0.6926,  0.2158, -0.1276,  0.0816,  0.5654,  0.3994,\n",
       "          0.0481,  0.5165,  0.6760,  0.0088,  0.5497,  0.1087,  0.3112,  0.1164,\n",
       "          0.8411,  0.5276,  1.2759,  0.5454,  0.4011, -0.5657,  0.3129,  0.3291,\n",
       "          0.3883,  0.6883,  0.1496,  0.0744,  0.0823,  0.8324,  0.4443,  0.6572],\n",
       "        [ 1.1757, -0.1016,  0.6879,  0.2071, -0.1334,  0.0805,  0.5621,  0.3876,\n",
       "          0.0469,  0.5091,  0.6333,  0.0045,  0.5085,  0.0788,  0.3016,  0.1011,\n",
       "          0.8172,  0.4893,  1.2559,  0.5111,  0.3874, -0.5815,  0.3119,  0.3125,\n",
       "          0.3805,  0.6783,  0.1490,  0.0630,  0.0374,  0.8213,  0.4017,  0.6260],\n",
       "        [ 1.1486, -0.1158,  0.6847,  0.1954, -0.1402,  0.0755,  0.5435,  0.3458,\n",
       "          0.0375,  0.5007,  0.6153,  0.0024,  0.4568,  0.0390,  0.2957,  0.0708,\n",
       "          0.8081,  0.4856,  1.2293,  0.5049,  0.3873, -0.6020,  0.3097,  0.3002,\n",
       "          0.3460,  0.6287,  0.1163,  0.0441, -0.0096,  0.8178,  0.3889,  0.5733],\n",
       "        [ 1.1418, -0.1215,  0.6688,  0.1718, -0.1445,  0.0564,  0.5380,  0.3345,\n",
       "          0.0218,  0.4898,  0.5426, -0.0097,  0.3950,  0.0026,  0.2817,  0.0544,\n",
       "          0.7738,  0.4322,  1.1982,  0.4570,  0.3731, -0.6247,  0.3056,  0.2565,\n",
       "          0.3367,  0.6271,  0.1090,  0.0252, -0.0101,  0.8005,  0.3870,  0.5728],\n",
       "        [ 1.1029, -0.1348,  0.6647,  0.1699, -0.1461,  0.0438,  0.5243,  0.3101,\n",
       "          0.0105,  0.4730,  0.5368, -0.0129,  0.3247, -0.0544,  0.2721,  0.0111,\n",
       "          0.7617,  0.4246,  1.1697,  0.4510,  0.3713, -0.6546,  0.3013,  0.2014,\n",
       "          0.2917,  0.6245,  0.0769,  0.0045, -0.0181,  0.7973,  0.3867,  0.5237],\n",
       "        [ 1.0956, -0.1427,  0.6516,  0.1631, -0.1464,  0.0416,  0.5147,  0.2743,\n",
       "         -0.0015,  0.4607,  0.4500, -0.0222,  0.3233, -0.0891,  0.2573,  0.0025,\n",
       "          0.7225,  0.3721,  1.1239,  0.4018,  0.3559, -0.6753,  0.2907,  0.1418,\n",
       "          0.2816,  0.6060,  0.0537, -0.0245, -0.0453,  0.7853,  0.3616,  0.4700],\n",
       "        [ 1.0602, -0.1474,  0.6515,  0.1589, -0.1604,  0.0022,  0.5085,  0.2731,\n",
       "         -0.0040,  0.4451,  0.4283, -0.0398,  0.3149, -0.1644,  0.2437, -0.0210,\n",
       "          0.7080,  0.3432,  1.1016,  0.3705,  0.3456, -0.7164,  0.2882,  0.0796,\n",
       "          0.2292,  0.6032,  0.0485, -0.0408, -0.0644,  0.7827,  0.3539,  0.4538]],\n",
       "       device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sort_feat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a8a64ef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 32])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sort_ind[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d214fe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5efeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SE3TransformerPooled(\n",
    "        fiber_in=Fiber({0: dm.NODE_FEATURE_DIM}),\n",
    "        fiber_out=Fiber({0: num_degrees * num_channels}),\n",
    "        fiber_edge=Fiber({0: dm.EDGE_FEATURE_DIM}),\n",
    "        output_dim=1,\n",
    "        tensor_cores=using_tensor_cores(False),\n",
    "        num_degrees=num_degrees,\n",
    "        num_channels=num_channels,\n",
    "        **kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a22797c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "?dgl.nn.pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e9e983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataset from pdb using npose utils\n",
    "# import util.npose_util as nu\n",
    "# import os\n",
    "# import numpy as np\n",
    "\n",
    "# model_direc = '/mnt/c/Users/nwood/OneDrive/Desktop/hTest/HelixGen_master/data/4H_dataset/models/'\n",
    "\n",
    "# fL = os.listdir(model_direc)\n",
    "# coords = np.zeros((len(fL),65*5,4)) #65 aa, 5 atoms per aa\n",
    "# for i,file in enumerate(fL):\n",
    "#     coords[i] = nu.npose_from_file(f'{model_direc}/{file}')\n",
    "\n",
    "# coords_out = coords.reshape((27894,65,5,4))[...,:3]\n",
    "# ca_coords = coords_out.reshape((27894,65,5,3))[:,:,1,:]\n",
    "\n",
    "# np.savez_compressed('../gudiff/data/h4_coords.npz',coords_out)\n",
    "# np.savez_compressed('../gudiff/data/h4_ca_coords.npz',ca_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cca76781",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 47\u001b[0m\n\u001b[0;32m     42\u001b[0m     batched_graph \u001b[38;5;241m=\u001b[39m dgl\u001b[38;5;241m.\u001b[39mbatch(graphList)\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m batched_graph\n\u001b[1;32m---> 47\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mGraphDataset\u001b[39;00m(\u001b[43mDataset\u001b[49m):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, ep_file : pathlib\u001b[38;5;241m.\u001b[39mPath, limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m):\n\u001b[0;32m     49\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_path \u001b[38;5;241m=\u001b[39m ep_file\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def normalize_pc(points):\n",
    "    \"\"\"Center at Zero Divide furtherst points\"\"\"\n",
    "    centroid = np.mean(points, axis=0)\n",
    "    points -= centroid\n",
    "    #since the points are centered zero, the furthest points is the abs value di\n",
    "    furthest_distance = np.max(np.sqrt(np.sum(abs(points)**2,axis=-1)))\n",
    "    points /= furthest_distance\n",
    "\n",
    "    return points, furthest_distance\n",
    "    \n",
    "def make_pe_encoding(i_pos=8, embed_dim = 8, scale = 10, cast_type=torch.float32):\n",
    "    #positional encoding of node\n",
    "    i_array = np.arange(1,(embed_dim/2)+1)\n",
    "    wk = (1/(scale**(i_array*2/embed_dim)))\n",
    "    t_array = np.arange(i_pos)\n",
    "    si = torch.tensor(np.sin(wk*t_array.reshape((-1,1))))\n",
    "    ci = torch.tensor(np.cos(wk*t_array.reshape((-1,1))))\n",
    "    pe = torch.stack((si,ci),axis=2).reshape(t_array.shape[0],embed_dim).type(cast_type)\n",
    "    return pe\n",
    "\n",
    "\n",
    "def make_graph_struct(batch_size=32, n_nodes = 8):\n",
    "    # make a fake graph to be filled with generator outputs\n",
    "    \n",
    "    v1 = np.arange(n_nodes-1) #vertex 1 of edges in chronological order\n",
    "    v2 = np.arange(1,n_nodes) #vertex 2 of edges in chronological order\n",
    "\n",
    "    ss = np.zeros(len(v1),dtype=np.int32)\n",
    "    ss[np.arange(ss.shape[0])%2==0]=1  #alternate 0,1 for helix, loop, helix, etc\n",
    "    ss = ss[:,None] #unsqueeze\n",
    "    \n",
    "    pe = make_pe_encoding(i_pos=8, embed_dim = 8, scale = 10, cast_type=torch.float32)\n",
    "\n",
    "    graphList = []\n",
    "    for i in range(batch_size):\n",
    "        g = dgl.graph((v1,v2))\n",
    "        g.edata['ss'] = torch.tensor(ss,dtype=torch.float32)\n",
    "        g.ndata['pe'] = pe\n",
    "\n",
    "        graphList.append(g)\n",
    "\n",
    "    batched_graph = dgl.batch(graphList)\n",
    "\n",
    "    return batched_graph\n",
    "\n",
    "\n",
    "class GraphDataset(Dataset):\n",
    "    def __init__(self, ep_file : pathlib.Path, limit=1000):\n",
    "        self.data_path = ep_file\n",
    "        rr = np.load(self.data_path)\n",
    "        ep = [rr[f] for f in rr.files][0][:1000]\n",
    "        \n",
    "        #need to save furthest distance to regen later\n",
    "        #maybe consider small change for next steps\n",
    "        ep, self.furthest_distance = normalize_pc(ep.reshape((-1,3)))\n",
    "        self.ep = ep.reshape((-1,8,3))\n",
    "        \n",
    "        \n",
    "        v1 = np.arange(self.ep.shape[1]-1) #vertex 1 of edges in chronological order\n",
    "        v2 = np.arange(1,self.ep.shape[1]) #vertex 2 of edges in chronological order\n",
    "\n",
    "        ss = np.zeros(len(v1))\n",
    "        ss[np.arange(ss.shape[0])%2==0]=1  #alternate 0,1 for helix, loop, helix, etc\n",
    "        ss = ss[:,None] #unsqueeze\n",
    "\n",
    "        #positional encoding of node\n",
    "        pe = make_pe_encoding(i_pos=8, embed_dim = 8, scale = 10, cast_type=torch.float32)\n",
    "\n",
    "        graphList = []\n",
    "\n",
    "        for i,c in enumerate(self.ep):\n",
    "\n",
    "            g = dgl.graph((v1,v2))\n",
    "            g.ndata['pos'] = torch.tensor(c,dtype=torch.float32)\n",
    "            g.edata['ss'] = torch.tensor(ss,dtype=torch.float32)\n",
    "            g.ndata['pe'] = pe\n",
    "\n",
    "            graphList.append(g)\n",
    "        \n",
    "        self.graphList = graphList\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graphList)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.graphList[idx]\n",
    "\n",
    "    \n",
    "class HGenDataModule():\n",
    "    \"\"\"\n",
    "    Datamodule wrapping hGen data set. 8 Helical endpoints defining a four helix protein.\n",
    "    \"\"\"\n",
    "    #8 long positional encoding\n",
    "    NODE_FEATURE_DIM = 8\n",
    "    EDGE_FEATURE_DIM = 1 # 0 or 1 helix or loop\n",
    "\n",
    "    def __init__(self,\n",
    "                 data_dir: pathlib.Path, batch_size=32):\n",
    "        \n",
    "        self.data_dir = data_dir \n",
    "        self.GraphDatasetObj = GraphDataset(self.data_dir)\n",
    "        self.gds = DataLoader(self.GraphDatasetObj,batch_size=batch_size, shuffle=True, drop_last=True,\n",
    "                              collate_fn=self._collate)\n",
    "        \n",
    "    \n",
    "        \n",
    "    def _collate(self, graphs):\n",
    "        batched_graph = dgl.batch(graphs)\n",
    "        #reshape that batched graph to redivide into the individual graphs\n",
    "        edge_feats = {'0': batched_graph.edata['ss'][:, :self.EDGE_FEATURE_DIM, None]}\n",
    "        batched_graph.edata['rel_pos'] = _get_relative_pos(batched_graph)\n",
    "        # get node features\n",
    "        node_feats = {'0': batched_graph.ndata['pe'][:, :self.NODE_FEATURE_DIM, None]}\n",
    "        \n",
    "        return (batched_graph, node_feats, edge_feats)\n",
    "    \n",
    "def eval_gen(batch_size=8,z=12):\n",
    "    \n",
    "    in_z = torch.randn((batch_size,z), device='cuda',dtype = torch.float32)\n",
    "    out = hg(in_z)*31\n",
    "    out = out.reshape((-1,8,3)).detach().cpu().numpy()\n",
    "    \n",
    "    return eval_endpoints(out)\n",
    "    \n",
    "    \n",
    "\n",
    "def eval_endpoints(ep_in): \n",
    "    \n",
    "    ep = ep_in.reshape((-1,8,3))\n",
    "\n",
    "    v1 = np.arange(ep.shape[1]-1) #vertex 1 of edges in chronological order\n",
    "    v2 = np.arange(1,ep.shape[1]) #vertex 2 of edges in chronological order\n",
    "\n",
    "    hLL = np.linalg.norm(ep[:,v1]-ep[:,v2],axis=2)\n",
    "\n",
    "    hLoc = np.array([0,2,4,6])\n",
    "    lLoc = np.array([1,3,5])\n",
    "\n",
    "    return np.mean(hLL[:,hLoc]), np.mean(hLL[:,lLoc])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "4079c5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnpoolDep(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Place features into torch.zeros array\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, features: Dict[str, Tensor], graph: DGLGraph, idx: Tensor, u_features: Dict[str, Tensor]):\n",
    "        idx_count = 0\n",
    "        out_feats = {}\n",
    "        for key,val in features.items():\n",
    "            new_h = val.new_zeros([graph.num_nodes(), val.shape[1], 1])\n",
    "            pad = val.new_zeros([graph.num_nodes(), new_h.shape[1]-u_features[key].shape[1],1])\n",
    "            out_feats[key] = torch.add(F.scatter_row(new_h,idx[idx_count],val),torch.cat((u_features[key],pad),1))\n",
    "            idx_count +=1\n",
    "        return out_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210427a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "se33",
   "language": "python",
   "name": "se33"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
