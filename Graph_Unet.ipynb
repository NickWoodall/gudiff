{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "a02a1c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import dgl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "e213e885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 65, 3)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path_str  = 'data/bcov_ca.npz'\n",
    "test_limit = 128\n",
    "rr = np.load(data_path_str)\n",
    "ca_coords = [rr[f] for f in rr.files][0][:test_limit,:,:3]\n",
    "ca_coords.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "6271f244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_midpoint(ep_in):\n",
    "    \"\"\"Get midpoint, of each batched set of points\"\"\"\n",
    "    \n",
    "    #calculate midpoint\n",
    "    midpoint = ep_in.sum(axis=1)/np.repeat(ep_in.shape[1], ep_in.shape[2])\n",
    "    \n",
    "    return midpoint\n",
    "\n",
    "\n",
    "def normalize_points(input_xyz, print_dist=False):\n",
    "    \n",
    "    #broadcast to distance matrix [Batch, M, R3] to [Batch,M,1, R3] to [Batch,1,M, R3] to [Batch, M,M, R3] \n",
    "    vec_diff = input_xyz[...,None,:]-input_xyz[...,None,:,:]\n",
    "    dist = np.sqrt(np.sum(np.square(vec_diff),axis=len(input_xyz.shape)))\n",
    "    furthest_dist = np.max(dist)\n",
    "    centroid  = get_midpoint(input_xyz)\n",
    "    if print_dist:\n",
    "        print(f'largest distance {furthest_dist:0.1f}')\n",
    "    \n",
    "    xyz_mean_zero = input_xyz - centroid[:,None,:]\n",
    "    return xyz_mean_zero/furthest_dist\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "9f20e97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "largest distance 32.8\n"
     ]
    }
   ],
   "source": [
    "norm_p = normalize_points(ca_coords,print_dist=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "508327fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#goal define edges of\n",
    "#connected backbone 1, \n",
    "#unconnected atoms 0,\n",
    "\n",
    "def define_graph_edges(n_nodes):\n",
    "    #connected backbone\n",
    "\n",
    "    n_nodes = norm_p[0].shape[0]\n",
    "\n",
    "    con_v1 = np.arange(n_nodes-1) #vertex 1 of edges in chronological order\n",
    "    con_v2 = np.arange(1,n_nodes) #vertex 2 of edges in chronological order\n",
    "\n",
    "    ind = con_v1*(n_nodes-1)+con_v2-1 #account for removed self connections (-1)\n",
    "\n",
    "\n",
    "    #unconnected backbone\n",
    "\n",
    "    nodes = np.arange(n_nodes)\n",
    "    v1 = np.repeat(nodes,n_nodes-1) #starting vertices, same number repeated for each edge\n",
    "\n",
    "    start_v2 = np.repeat(np.arange(n_nodes)[None,:],n_nodes,axis=0)\n",
    "    diag_ind = np.diag_indices(n_nodes)\n",
    "    start_v2[diag_ind] = -1 #diagonal of matrix is self connections which we remove (self connections are managed elsewhere)\n",
    "    v2 = start_v2[start_v2>-0.5] #remove diagonal and flatten\n",
    "\n",
    "    edge_data = torch.zeros(len(u_v2))\n",
    "    edge_data[ind] = 1\n",
    "    \n",
    "    return v1,v2,edge_data, ind\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "7ec7ad83",
   "metadata": {},
   "outputs": [],
   "source": [
    "v1,v2,edge_data, ind = define_graph_edges(n_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "04b3dbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pe_encoding(n_nodes=65, embed_dim = 12, scale = 1000, cast_type=torch.float32, print_out=False):\n",
    "    #positional encoding of node\n",
    "    i_array = np.arange(1,(embed_dim/2)+1)\n",
    "    wk = (1/(scale**(i_array*2/embed_dim)))\n",
    "    t_array = np.arange(n_nodes)\n",
    "    si = torch.tensor(np.sin(wk*t_array.reshape((-1,1))))\n",
    "    ci = torch.tensor(np.cos(wk*t_array.reshape((-1,1))))\n",
    "    pe = torch.stack((si,ci),axis=2).reshape(t_array.shape[0],embed_dim).type(cast_type)\n",
    "    \n",
    "    if print_out == True:\n",
    "        for x in range(int(n_nodes/12)):\n",
    "            print(np.round(pe[x],1))\n",
    "    \n",
    "    return pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "780db87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.])\n",
      "tensor([0.3000, 1.0000, 0.1000, 1.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "        1.0000, 0.0000, 1.0000])\n",
      "tensor([0.6000, 0.8000, 0.2000, 1.0000, 0.1000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "        1.0000, 0.0000, 1.0000])\n",
      "tensor([0.8000, 0.6000, 0.3000, 1.0000, 0.1000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "        1.0000, 0.0000, 1.0000])\n",
      "tensor([1.0000, 0.3000, 0.4000, 0.9000, 0.1000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "        1.0000, 0.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "pe = make_pe_encoding(n_nodes=65,print_out=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "321dc5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_graph(batch_size=8,n_nodes=65):\n",
    "    \n",
    "    v1,v2,edge_data, ind = define_graph_edges(n_nodes)\n",
    "    pe = make_pe_encoding(n_nodes=65)\n",
    "    \n",
    "    graphList = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        \n",
    "        g = dgl.graph((v1,v2))\n",
    "        g.edata['con'] = edge_data\n",
    "        g.ndata['pe'] = pe\n",
    "\n",
    "        graphList.append(g)\n",
    "        \n",
    "    batched_graph = dgl.batch(graphList)\n",
    "\n",
    "    return batched_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "9d60b160",
   "metadata": {},
   "outputs": [],
   "source": [
    "bg = define_graph(batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca76781",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def normalize_pc(points):\n",
    "    \"\"\"Center at Zero Divide furtherst points\"\"\"\n",
    "    centroid = np.mean(points, axis=0)\n",
    "    points -= centroid\n",
    "    #since the points are centered zero, the furthest points is the abs value di\n",
    "    furthest_distance = np.max(np.sqrt(np.sum(abs(points)**2,axis=-1)))\n",
    "    points /= furthest_distance\n",
    "\n",
    "    return points, furthest_distance\n",
    "    \n",
    "def make_pe_encoding(i_pos=8, embed_dim = 8, scale = 10, cast_type=torch.float32):\n",
    "    #positional encoding of node\n",
    "    i_array = np.arange(1,(embed_dim/2)+1)\n",
    "    wk = (1/(scale**(i_array*2/embed_dim)))\n",
    "    t_array = np.arange(i_pos)\n",
    "    si = torch.tensor(np.sin(wk*t_array.reshape((-1,1))))\n",
    "    ci = torch.tensor(np.cos(wk*t_array.reshape((-1,1))))\n",
    "    pe = torch.stack((si,ci),axis=2).reshape(t_array.shape[0],embed_dim).type(cast_type)\n",
    "    return pe\n",
    "\n",
    "\n",
    "def make_graph_struct(batch_size=32, n_nodes = 8):\n",
    "    # make a fake graph to be filled with generator outputs\n",
    "    \n",
    "    v1 = np.arange(n_nodes-1) #vertex 1 of edges in chronological order\n",
    "    v2 = np.arange(1,n_nodes) #vertex 2 of edges in chronological order\n",
    "\n",
    "    ss = np.zeros(len(v1),dtype=np.int32)\n",
    "    ss[np.arange(ss.shape[0])%2==0]=1  #alternate 0,1 for helix, loop, helix, etc\n",
    "    ss = ss[:,None] #unsqueeze\n",
    "    \n",
    "    pe = make_pe_encoding(i_pos=8, embed_dim = 8, scale = 10, cast_type=torch.float32)\n",
    "\n",
    "    graphList = []\n",
    "    for i in range(batch_size):\n",
    "        g = dgl.graph((v1,v2))\n",
    "        g.edata['ss'] = torch.tensor(ss,dtype=torch.float32)\n",
    "        g.ndata['pe'] = pe\n",
    "\n",
    "        graphList.append(g)\n",
    "\n",
    "    batched_graph = dgl.batch(graphList)\n",
    "\n",
    "    return batched_graph\n",
    "\n",
    "\n",
    "class GraphDataset(Dataset):\n",
    "    def __init__(self, ep_file : pathlib.Path, limit=1000):\n",
    "        self.data_path = ep_file\n",
    "        rr = np.load(self.data_path)\n",
    "        ep = [rr[f] for f in rr.files][0][:1000]\n",
    "        \n",
    "        #need to save furthest distance to regen later\n",
    "        #maybe consider small change for next steps\n",
    "        ep, self.furthest_distance = normalize_pc(ep.reshape((-1,3)))\n",
    "        self.ep = ep.reshape((-1,8,3))\n",
    "        \n",
    "        \n",
    "        v1 = np.arange(self.ep.shape[1]-1) #vertex 1 of edges in chronological order\n",
    "        v2 = np.arange(1,self.ep.shape[1]) #vertex 2 of edges in chronological order\n",
    "\n",
    "        ss = np.zeros(len(v1))\n",
    "        ss[np.arange(ss.shape[0])%2==0]=1  #alternate 0,1 for helix, loop, helix, etc\n",
    "        ss = ss[:,None] #unsqueeze\n",
    "\n",
    "        #positional encoding of node\n",
    "        pe = make_pe_encoding(i_pos=8, embed_dim = 8, scale = 10, cast_type=torch.float32)\n",
    "\n",
    "        graphList = []\n",
    "\n",
    "        for i,c in enumerate(self.ep):\n",
    "\n",
    "            g = dgl.graph((v1,v2))\n",
    "            g.ndata['pos'] = torch.tensor(c,dtype=torch.float32)\n",
    "            g.edata['ss'] = torch.tensor(ss,dtype=torch.float32)\n",
    "            g.ndata['pe'] = pe\n",
    "\n",
    "            graphList.append(g)\n",
    "        \n",
    "        self.graphList = graphList\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graphList)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.graphList[idx]\n",
    "\n",
    "    \n",
    "class HGenDataModule():\n",
    "    \"\"\"\n",
    "    Datamodule wrapping hGen data set. 8 Helical endpoints defining a four helix protein.\n",
    "    \"\"\"\n",
    "    #8 long positional encoding\n",
    "    NODE_FEATURE_DIM = 8\n",
    "    EDGE_FEATURE_DIM = 1 # 0 or 1 helix or loop\n",
    "\n",
    "    def __init__(self,\n",
    "                 data_dir: pathlib.Path, batch_size=32):\n",
    "        \n",
    "        self.data_dir = data_dir \n",
    "        self.GraphDatasetObj = GraphDataset(self.data_dir)\n",
    "        self.gds = DataLoader(self.GraphDatasetObj,batch_size=batch_size, shuffle=True, drop_last=True,\n",
    "                              collate_fn=self._collate)\n",
    "        \n",
    "    \n",
    "        \n",
    "    def _collate(self, graphs):\n",
    "        batched_graph = dgl.batch(graphs)\n",
    "        #reshape that batched graph to redivide into the individual graphs\n",
    "        edge_feats = {'0': batched_graph.edata['ss'][:, :self.EDGE_FEATURE_DIM, None]}\n",
    "        batched_graph.edata['rel_pos'] = _get_relative_pos(batched_graph)\n",
    "        # get node features\n",
    "        node_feats = {'0': batched_graph.ndata['pe'][:, :self.NODE_FEATURE_DIM, None]}\n",
    "        \n",
    "        return (batched_graph, node_feats, edge_feats)\n",
    "    \n",
    "def eval_gen(batch_size=8,z=12):\n",
    "    \n",
    "    in_z = torch.randn((batch_size,z), device='cuda',dtype = torch.float32)\n",
    "    out = hg(in_z)*31\n",
    "    out = out.reshape((-1,8,3)).detach().cpu().numpy()\n",
    "    \n",
    "    return eval_endpoints(out)\n",
    "    \n",
    "    \n",
    "\n",
    "def eval_endpoints(ep_in): \n",
    "    \n",
    "    ep = ep_in.reshape((-1,8,3))\n",
    "\n",
    "    v1 = np.arange(ep.shape[1]-1) #vertex 1 of edges in chronological order\n",
    "    v2 = np.arange(1,ep.shape[1]) #vertex 2 of edges in chronological order\n",
    "\n",
    "    hLL = np.linalg.norm(ep[:,v1]-ep[:,v2],axis=2)\n",
    "\n",
    "    hLoc = np.array([0,2,4,6])\n",
    "    lLoc = np.array([1,3,5])\n",
    "\n",
    "    return np.mean(hLL[:,hLoc]), np.mean(hLL[:,lLoc])\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gudiff",
   "language": "python",
   "name": "gudiff"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
