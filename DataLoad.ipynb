{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c740047",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import logging\n",
    "import pathlib\n",
    "from typing import List\n",
    "import dgl\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import collections.abc as container_abcs\n",
    "#from apex.optimizers import FusedAdam, FusedLAMB\n",
    "from torch.nn.modules.loss import _Loss\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.optim import Optimizer\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "from se3_transformer.data_loading import QM9DataModule\n",
    "from se3_transformer.model import SE3TransformerPooled\n",
    "from se3_transformer.model.fiber import Fiber\n",
    "from se3_transformer.runtime import gpu_affinity\n",
    "from se3_transformer.runtime.arguments import PARSER\n",
    "from se3_transformer.runtime.callbacks import QM9MetricCallback, QM9LRSchedulerCallback, BaseCallback, \\\n",
    "    PerformanceCallback\n",
    "from se3_transformer.runtime.inference import evaluate\n",
    "from se3_transformer.runtime.loggers import LoggerCollection, DLLogger, WandbLogger, Logger\n",
    "from se3_transformer.runtime.utils import to_cuda, get_local_rank, init_distributed, seed_everything, \\\n",
    "    using_tensor_cores, increase_l2_fetch_granularity\n",
    "import helix.helix_bb as hh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc7e5ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split, DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57dba110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_midpoint(ep_in):\n",
    "    \"\"\"Get midpoints of input endpoints, 2 points per helix\"\"\"\n",
    "    \n",
    "    #calculate midpoint\n",
    "    midpoint = ep_in.sum(axis=1)/np.repeat(ind_ep.shape[1], ind_ep.shape[2])\n",
    "    \n",
    "    return midpoint\n",
    "\n",
    "\n",
    "\n",
    "def normalize_pc(points):\n",
    "    \"\"\"Center at Zero Divide furtherst points\"\"\"\n",
    "    centroid = np.mean(points, axis=0)\n",
    "    points -= centroid\n",
    "    furthest_distance = np.max(np.sqrt(np.sum(abs(points)**2,axis=-1)))\n",
    "    points /= furthest_distance\n",
    "\n",
    "    return points, furthest_distance\n",
    "    \n",
    "def make_pe_encoding(i_pos=8, embed_dim = 8, scale = 10, cast_type=torch.float32):\n",
    "    #positional encoding of node\n",
    "    i_array = np.arange(1,(embed_dim/2)+1)\n",
    "    wk = (1/(scale**(i_array*2/embed_dim)))\n",
    "    t_array = np.arange(i_pos)\n",
    "    si = torch.tensor(np.sin(wk*t_array.reshape((-1,1))))\n",
    "    ci = torch.tensor(np.cos(wk*t_array.reshape((-1,1))))\n",
    "    pe = torch.stack((si,ci),axis=2).reshape(t_array.shape[0],embed_dim).type(cast_type)\n",
    "    return pe\n",
    "\n",
    "\n",
    "def make_graph_struct(batch_size=32, n_nodes = 8):\n",
    "    # make a fake graph to be filled with generator outputs\n",
    "    \n",
    "    v1 = np.arange(n_nodes-1) #vertex 1 of edges in chronological order\n",
    "    v2 = np.arange(1,n_nodes) #vertex 2 of edges in chronological order\n",
    "\n",
    "    ss = np.zeros(len(v1),dtype=np.int32)\n",
    "    ss[np.arange(ss.shape[0])%2==0]=1  #alternate 0,1 for helix, loop, helix, etc\n",
    "    ss = ss[:,None] #unsqueeze\n",
    "    \n",
    "    pe = make_pe_encoding(i_pos=8, embed_dim = 8, scale = 10, cast_type=torch.float32)\n",
    "\n",
    "    graphList = []\n",
    "    for i in range(batch_size):\n",
    "        g = dgl.graph((v1,v2))\n",
    "        g.edata['ss'] = torch.tensor(ss,dtype=torch.float32)\n",
    "        g.ndata['pe'] = pe\n",
    "\n",
    "        graphList.append(g)\n",
    "\n",
    "    batched_graph = dgl.batch(graphList)\n",
    "\n",
    "    return batched_graph\n",
    "\n",
    "\n",
    "class GraphDataset(Dataset):\n",
    "    def __init__(self, ep_file : pathlib.Path, limit=1000):\n",
    "        self.data_path = ep_file\n",
    "        rr = np.load(self.data_path)\n",
    "        ep = [rr[f] for f in rr.files][0][:1000]\n",
    "        \n",
    "        #need to save furthest distance to regen later\n",
    "        #maybe consider small change for next steps\n",
    "        ep, self.furthest_distance = normalize_pc(ep.reshape((-1,3)))\n",
    "        self.ep = ep.reshape((-1,8,3))\n",
    "        \n",
    "        \n",
    "        v1 = np.arange(self.ep.shape[1]-1) #vertex 1 of edges in chronological order\n",
    "        v2 = np.arange(1,self.ep.shape[1]) #vertex 2 of edges in chronological order\n",
    "\n",
    "        ss = np.zeros(len(v1))\n",
    "        ss[np.arange(ss.shape[0])%2==0]=1  #alternate 0,1 for helix, loop, helix, etc\n",
    "        ss = ss[:,None] #unsqueeze\n",
    "\n",
    "        #positional encoding of node\n",
    "        pe = make_pe_encoding(i_pos=8, embed_dim = 8, scale = 10, cast_type=torch.float32)\n",
    "\n",
    "        graphList = []\n",
    "\n",
    "        for i,c in enumerate(self.ep):\n",
    "\n",
    "            g = dgl.graph((v1,v2))\n",
    "            g.ndata['pos'] = torch.tensor(c,dtype=torch.float32)\n",
    "            g.edata['ss'] = torch.tensor(ss,dtype=torch.float32)\n",
    "            g.ndata['pe'] = pe\n",
    "\n",
    "            graphList.append(g)\n",
    "        \n",
    "        self.graphList = graphList\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graphList)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.graphList[idx]\n",
    "\n",
    "    \n",
    "class HGenDataModule():\n",
    "    \"\"\"\n",
    "    Datamodule wrapping hGen data set. 8 Helical endpoints defining a four helix protein.\n",
    "    \"\"\"\n",
    "    #8 long positional encoding\n",
    "    NODE_FEATURE_DIM = 8\n",
    "    EDGE_FEATURE_DIM = 1 # 0 or 1 helix or loop\n",
    "\n",
    "    def __init__(self,\n",
    "                 data_dir: pathlib.Path, batch_size=32):\n",
    "        \n",
    "        self.data_dir = data_dir \n",
    "        self.GraphDatasetObj = GraphDataset(self.data_dir)\n",
    "        self.gds = DataLoader(self.GraphDatasetObj,batch_size=batch_size, shuffle=True, drop_last=True,\n",
    "                              collate_fn=self._collate)\n",
    "        \n",
    "    \n",
    "        \n",
    "    def _collate(self, graphs):\n",
    "        batched_graph = dgl.batch(graphs)\n",
    "        #reshape that batched graph to redivide into the individual graphs\n",
    "        edge_feats = {'0': batched_graph.edata['ss'][:, :self.EDGE_FEATURE_DIM, None]}\n",
    "        batched_graph.edata['rel_pos'] = _get_relative_pos(batched_graph)\n",
    "        # get node features\n",
    "        node_feats = {'0': batched_graph.ndata['pe'][:, :self.NODE_FEATURE_DIM, None]}\n",
    "        \n",
    "        return (batched_graph, node_feats, edge_feats)\n",
    "    \n",
    "def eval_gen(batch_size=8,z=12):\n",
    "    \n",
    "    in_z = torch.randn((batch_size,z), device='cuda',dtype = torch.float32)\n",
    "    out = hg(in_z)*31\n",
    "    out = out.reshape((-1,8,3)).detach().cpu().numpy()\n",
    "    \n",
    "    return eval_endpoints(out)\n",
    "    \n",
    "    \n",
    "\n",
    "def eval_endpoints(ep_in): \n",
    "    \n",
    "    ep = ep_in.reshape((-1,8,3))\n",
    "\n",
    "    v1 = np.arange(ep.shape[1]-1) #vertex 1 of edges in chronological order\n",
    "    v2 = np.arange(1,ep.shape[1]) #vertex 2 of edges in chronological order\n",
    "\n",
    "    hLL = np.linalg.norm(ep[:,v1]-ep[:,v2],axis=2)\n",
    "\n",
    "    hLoc = np.array([0,2,4,6])\n",
    "    lLoc = np.array([1,3,5])\n",
    "\n",
    "    return np.mean(hLL[:,hLoc]), np.mean(hLL[:,lLoc])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed98db5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "28455a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#num channels relates to self interaction of features of the same degree on the same node (1x1 convolution)\n",
    "#learnable skip connections, since nodes don't attend to themselves nedded\n",
    "\n",
    "\n",
    "#--num_degrees: Number of degrees to use. Hidden features will have types [0, ..., num_degrees - 1] (default: 4)\n",
    "#so num degrees is 3?\n",
    "# Fiber\n",
    "\n",
    "# A fiber can be viewed as a representation of a set of features of different types or degrees (positive integers), where each feature type transforms according to its rule.\n",
    "# In this repository, a fiber can be seen as a dictionary with degrees as keys and numbers of channels as values.\n",
    "\n",
    "#Edge feature dimension does not include rel_pos which is concatenated per forward pass\n",
    "\n",
    "#I believe channels needs to equal degrees times heads to match expansion by heads\n",
    "#with the self atention by channels \n",
    "def to_detach(x):\n",
    "    \"\"\" Try to convert a Tensor, a collection of Tensors or a DGLGraph to CUDA \"\"\"\n",
    "    if isinstance(x, Tensor):\n",
    "        return x.detach()\n",
    "    elif isinstance(x, tuple):\n",
    "        return (to_detach(v) for v in x)\n",
    "    elif isinstance(x, list):\n",
    "        return [to_detach(v) for v in x]\n",
    "    elif isinstance(x, dict):\n",
    "        return {k: to_detach(v) for k, v in x.items()}\n",
    "    else:\n",
    "        # DGLGraph or other objects\n",
    "        return x\n",
    "def _get_relative_pos(graph_in: dgl.DGLGraph) -> torch.Tensor:\n",
    "    x = graph_in.ndata['pos']\n",
    "    src, dst = graph_in.edges()\n",
    "    rel_pos = x[dst] - x[src]\n",
    "    return rel_pos\n",
    "\n",
    "class helixGen(nn.Module):\n",
    "    def __init__(self, input_z=12, hidden=64, output=24):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_z, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, output),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_xyz = self.linear_relu_stack(x)\n",
    "        return out_xyz.reshape((-1,3))\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "num_channels = 16\n",
    "num_degrees = 4\n",
    "\n",
    "kwargs = dict()\n",
    "kwargs['pooling'] = 'max'\n",
    "kwargs['num_layers'] = 4\n",
    "kwargs['num_heads'] = 8\n",
    "kwargs['channels_div'] =torch.tensor(2,dtype=torch.int32)\n",
    "\n",
    "#channels dive = channels/num_heads\n",
    "\n",
    "\n",
    "dm = HGenDataModule(pathlib.Path('data/ep_for_gmp.npz'),batch_size=batch_size)\n",
    "\n",
    "\n",
    "model = SE3TransformerPooled(\n",
    "        fiber_in=Fiber({0: dm.NODE_FEATURE_DIM}),\n",
    "        fiber_out=Fiber({0: num_degrees * num_channels}),\n",
    "        fiber_edge=Fiber({0: dm.EDGE_FEATURE_DIM}),\n",
    "        output_dim=1,\n",
    "        tensor_cores=using_tensor_cores(False),\n",
    "        num_degrees=num_degrees,\n",
    "        num_channels=num_channels,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "device=torch.cuda.current_device()\n",
    "\n",
    "hg = helixGen()\n",
    "hg.to(torch.float32)\n",
    "hg.to(device)\n",
    "\n",
    "model.to(device)\n",
    "model.to(torch.float32)\n",
    "loss_fn = nn.BCEWithLogitsLoss().to(device)\n",
    "#loss_fn = nn.MSELoss()\n",
    "d_opt = torch.optim.SGD(model.parameters(), lr=0.001, weight_decay=0.001)\n",
    "g_opt = torch.optim.SGD(hg.parameters(), lr=0.001, weight_decay=0.001)\n",
    "batched_graph = make_graph_struct(batch_size=batch_size)\n",
    "batched_graph = to_cuda(batched_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "db5068a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(input_z, input_real):\n",
    "    \n",
    "    batched_graph_real, node_feats_real, edge_feats_real = input_real\n",
    "    batched_graph_real = to_cuda(batched_graph_real)\n",
    "    node_feats_real = to_cuda(node_feats_real)\n",
    "    edge_feats_real = to_cuda(edge_feats_real)\n",
    "    \n",
    "    model.zero_grad()\n",
    "    hg.zero_grad()\n",
    "    #----------------compute the generators loss---------------------------\n",
    "    outpos = hg(input_z)\n",
    "\n",
    "    batched_graph.ndata['pos'] = outpos\n",
    "    batched_graph.edata['rel_pos'] = _get_relative_pos(batched_graph)\n",
    "    #these node and edges feats are static, the s03 transformer is order independent\n",
    "    #right now they just denote position\n",
    "    g_fake_out = model(batched_graph, node_feats_real, edge_feats_real, compute_gradients=True) \n",
    "    real_fake_targets = torch.ones(g_fake_out.shape[0],dtype=torch.float32, device='cuda')\n",
    "    g_loss = loss_fn(g_fake_out, real_fake_targets)\n",
    "    \n",
    "    #retain_graph=True\n",
    "    g_loss.backward()\n",
    "    g_opt.step()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #----------compute the discrimanators loss\n",
    "    model.zero_grad()\n",
    "    d_real_out = model(batched_graph_real, node_feats_real, edge_feats_real)\n",
    "    real_targets = torch.ones(d_real_out.shape[0],dtype=torch.float32, device='cuda')\n",
    "    d_loss_real = loss_fn(d_real_out, real_targets)\n",
    "    \n",
    "    batched_graph.edata['rel_pos'] = batched_graph.edata['rel_pos'].detach()\n",
    "    \n",
    "    d_fake_out = model(batched_graph, node_feats_real, edge_feats_real)\n",
    "    # I believe we can detach the input batch_graph position here, and not use retain_graph equals true\n",
    "    #also fix repeated used of node_feats_real, edge_feats_real\n",
    "    fake_targets = torch.zeros(d_fake_out.shape[0],dtype=torch.float32, device='cuda')\n",
    "    d_loss_fake = loss_fn(d_fake_out, fake_targets)\n",
    "    \n",
    "    d_loss = d_loss_real + d_loss_fake\n",
    "    \n",
    "    d_loss.backward()\n",
    "    d_opt.step()\n",
    "    \n",
    "    \n",
    "    #save probs here\n",
    "    #d_loss.detach(), g_loss.detach(),\n",
    "    \n",
    "    return  g_loss.detach(), d_loss.detach(), g_fake_out.detach(), d_fake_out.detach(), d_real_out.detach(), \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ea291cf2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 |  ET 0.03 min AvgLosses >> G/D 0.953/0.952 D Real :0.575 D Fake :0.573 Length EvaL (8.25626, 7.3440986)\n",
      "Epoch 001 |  ET 0.05 min AvgLosses >> G/D 0.947/0.945 D Real :0.569 D Fake :0.568 Length EvaL (7.8896675, 7.0250325)\n",
      "Epoch 002 |  ET 0.07 min AvgLosses >> G/D 0.942/0.944 D Real :0.566 D Fake :0.564 Length EvaL (7.863228, 6.9552994)\n",
      "Epoch 003 |  ET 0.09 min AvgLosses >> G/D 0.939/0.940 D Real :0.563 D Fake :0.561 Length EvaL (8.103188, 7.189209)\n",
      "Epoch 004 |  ET 0.11 min AvgLosses >> G/D 0.940/0.938 D Real :0.557 D Fake :0.556 Length EvaL (8.279875, 7.4276423)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, input_real \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dm\u001b[38;5;241m.\u001b[39mgds):\n\u001b[1;32m     10\u001b[0m     in_z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn((batch_size,\u001b[38;5;241m12\u001b[39m), device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m,dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m---> 12\u001b[0m     gloss, dloss, gfo, dfo, dfr \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_z\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_real\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     d_probs_real \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(torch\u001b[38;5;241m.\u001b[39msigmoid(dfr))\n\u001b[1;32m     14\u001b[0m     d_probs_fake \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(torch\u001b[38;5;241m.\u001b[39msigmoid(dfo))\n",
      "Cell \u001b[0;32mIn[62], line 17\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(input_z, input_real)\u001b[0m\n\u001b[1;32m     14\u001b[0m batched_graph\u001b[38;5;241m.\u001b[39medata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrel_pos\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m _get_relative_pos(batched_graph)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#these node and edges feats are static, the s03 transformer is order independent\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#right now they just denote position\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m g_fake_out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatched_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_feats_real\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_feats_real\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_gradients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \n\u001b[1;32m     18\u001b[0m real_fake_targets \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(g_fake_out\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     19\u001b[0m g_loss \u001b[38;5;241m=\u001b[39m loss_fn(g_fake_out, real_fake_targets)\n",
      "File \u001b[0;32m~/miniconda3/envs/torchGraph/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/c/Users/nwood/OneDrive/Desktop/SE3Transformer/se3_transformer/model/transformer.py:215\u001b[0m, in \u001b[0;36mSE3TransformerPooled.forward\u001b[0;34m(self, graph, node_feats, edge_feats, basis, compute_gradients)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, graph, node_feats, edge_feats, basis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, compute_gradients\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 215\u001b[0m     feats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_feats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_feats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbasis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mcompute_gradients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_gradients\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    216\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(feats)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m~/miniconda3/envs/torchGraph/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/c/Users/nwood/OneDrive/Desktop/SE3Transformer/se3_transformer/model/transformer.py:155\u001b[0m, in \u001b[0;36mSE3Transformer.forward\u001b[0;34m(self, graph, node_feats, edge_feats, basis, compute_gradients)\u001b[0m\n\u001b[1;32m    150\u001b[0m basis \u001b[38;5;241m=\u001b[39m update_basis_with_fused(basis, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_degree, use_pad_trick\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensor_cores \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory,\n\u001b[1;32m    151\u001b[0m                                 fully_fused\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuse_level \u001b[38;5;241m==\u001b[39m ConvSE3FuseLevel\u001b[38;5;241m.\u001b[39mFULL)\n\u001b[1;32m    153\u001b[0m edge_feats \u001b[38;5;241m=\u001b[39m get_populated_edge_features(graph\u001b[38;5;241m.\u001b[39medata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrel_pos\u001b[39m\u001b[38;5;124m'\u001b[39m], edge_feats)\n\u001b[0;32m--> 155\u001b[0m node_feats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph_modules\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_feats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_feats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbasis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbasis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooling \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooling_module(node_feats, graph\u001b[38;5;241m=\u001b[39mgraph)\n",
      "File \u001b[0;32m~/miniconda3/envs/torchGraph/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/c/Users/nwood/OneDrive/Desktop/SE3Transformer/se3_transformer/model/transformer.py:46\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input, *args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m---> 46\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/torchGraph/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/c/Users/nwood/OneDrive/Desktop/SE3Transformer/se3_transformer/model/layers/attention.py:162\u001b[0m, in \u001b[0;36mAttentionBlockSE3.forward\u001b[0;34m(self, node_features, edge_features, graph, basis)\u001b[0m\n\u001b[1;32m    159\u001b[0m     key, value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_key_value_from_fused(fused_key_value)\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m nvtx_range(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mqueries\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 162\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(value, key, query, graph)\n\u001b[1;32m    165\u001b[0m z_concat \u001b[38;5;241m=\u001b[39m aggregate_residual(node_features, z, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcat\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/torchGraph/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/c/Users/nwood/OneDrive/Desktop/SE3Transformer/se3_transformer/model/layers/linear.py:56\u001b[0m, in \u001b[0;36mLinearSE3.forward\u001b[0;34m(self, features, *args, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: Dict[\u001b[38;5;28mstr\u001b[39m, Tensor], \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Tensor]:\n\u001b[0;32m---> 56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     57\u001b[0m         degree: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights[degree] \u001b[38;5;241m@\u001b[39m features[degree]\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m degree, weight \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m     59\u001b[0m     }\n",
      "File \u001b[0;32m/mnt/c/Users/nwood/OneDrive/Desktop/SE3Transformer/se3_transformer/model/layers/linear.py:57\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: Dict[\u001b[38;5;28mstr\u001b[39m, Tensor], \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Tensor]:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m---> 57\u001b[0m         degree: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdegree\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdegree\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m degree, weight \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m     59\u001b[0m     }\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_losses = []\n",
    "all_d_vals = []\n",
    "epochs = 200\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    epoch_d_vals, epoch_losses = [],[]\n",
    "\n",
    "    for i, input_real in enumerate(dm.gds):\n",
    "        in_z = torch.randn((batch_size,12), device='cuda',dtype = torch.float32)\n",
    "\n",
    "        gloss, dloss, gfo, dfo, dfr = train_step(in_z, input_real)\n",
    "        d_probs_real = torch.mean(torch.sigmoid(dfr))\n",
    "        d_probs_fake = torch.mean(torch.sigmoid(dfo))\n",
    "        g_probs = torch.mean(torch.sigmoid(gfo))\n",
    "\n",
    "        epoch_losses.append((gloss.cpu().numpy(), dloss.cpu().numpy()))\n",
    "        epoch_d_vals.append((d_probs_real.cpu().numpy(), d_probs_fake.cpu().numpy()))\n",
    "        \n",
    "    all_losses.append(epoch_losses)\n",
    "    all_d_vals.append(epoch_d_vals)\n",
    "\n",
    "    track = f'Epoch {epoch:03d} |  ET {(time.time()-start_time)/60:.2f} min AvgLosses >> G/D '\n",
    "    track = f'{track}{(np.mean(all_losses[-1][0],axis=0)):.3f}/{(np.mean(all_losses[-1][1],axis=0)):.3f}'\n",
    "    track = f'{track} D Real :{(np.mean(all_d_vals[-1][0],axis=0)):.3f}'\n",
    "    track = f'{track} D Fake :{(np.mean(all_d_vals[-1][1],axis=0)):.3f}'\n",
    "    track = f'{track} Length EvaL {eval_gen(batch_size=batch_size)}'\n",
    "\n",
    "    print(track)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5701de9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24.367468, 10.302194)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_gen(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9f2bc10f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7168"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "896*8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "33027a1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31.228954134944612"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.GraphDatasetObj.furthest_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7150bfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_z = torch.randn((batch_size,12), device='cuda',dtype = torch.float32)\n",
    "out = hg(in_z)*31.228954134944612\n",
    "out = out.reshape((-1,8,3)).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6d0511d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e45551a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/ep_for_gmp.npz'\n",
    "rr = np.load(data_path)\n",
    "ep = [rr[f] for f in rr.files][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b1c72911",
   "metadata": {},
   "outputs": [],
   "source": [
    "gds= GraphDataset(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f0f4324d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31.228954134944612"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gds.furthest_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e33fc586",
   "metadata": {},
   "outputs": [],
   "source": [
    "ep2 = gds.ep*gds.furthest_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c990f135",
   "metadata": {},
   "outputs": [],
   "source": [
    "hep = hh.EP_Recon(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "2857d76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "npl = hep.to_npose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0317d224",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(len(npl)):\n",
    "    hh.nu.dump_npdb(npl[x],f'output/test_{x}.pdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "27143e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = (batched_graph_real.ndata['pos']*36).numpy().reshape((-1,8,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32194631",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bf56765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.077777 9.12193\n",
      "11.456908 8.926224\n",
      "11.475114 8.901618\n",
      "11.425886 9.046969\n",
      "11.976165 9.069525\n",
      "11.843756 9.040355\n",
      "11.139473 8.855804\n",
      "11.722401 9.133283\n",
      "11.81967 8.958594\n",
      "11.975039 9.1037855\n",
      "11.305781 8.875032\n",
      "11.437936 8.918839\n",
      "11.171131 8.927791\n",
      "11.85101 8.967086\n",
      "11.807449 9.046146\n",
      "11.386745 8.943198\n",
      "11.5919485 8.931197\n",
      "11.43671 9.010324\n",
      "12.039566 8.899229\n",
      "11.381808 9.015571\n",
      "11.148035 9.03046\n",
      "11.83581 9.007893\n",
      "11.907744 8.951764\n",
      "11.17659 8.871845\n",
      "11.521595 8.8635\n",
      "11.803186 8.921136\n",
      "10.994955 8.978087\n",
      "11.404093 8.949065\n",
      "11.466954 8.976123\n",
      "11.342091 9.141507\n",
      "11.6009865 8.899449\n",
      "11.377945 9.0340805\n",
      "11.199827 9.032464\n",
      "11.858041 9.061273\n",
      "11.615349 8.943565\n",
      "11.5568495 9.001395\n",
      "11.211485 9.044753\n",
      "11.289133 8.870537\n",
      "11.711969 8.875341\n",
      "11.706545 8.941649\n",
      "11.017423 8.992263\n",
      "10.645308 8.903129\n",
      "11.671707 8.967496\n",
      "11.30115 9.053582\n",
      "12.039323 9.041419\n",
      "11.751722 8.876072\n",
      "11.331128 8.96942\n",
      "11.78825 9.005442\n",
      "11.689248 8.999727\n",
      "11.587079 9.010528\n",
      "11.073433 8.976633\n",
      "11.86665 9.0105915\n",
      "11.370591 8.971354\n",
      "10.794506 8.962219\n",
      "12.039483 9.0113\n",
      "11.591343 8.993077\n",
      "11.443672 9.20391\n",
      "11.576511 9.124682\n",
      "11.431325 8.986287\n",
      "11.594555 8.979283\n",
      "12.060293 9.122714\n",
      "11.735792 9.074341\n",
      "10.988946 8.896601\n",
      "11.574106 8.886479\n",
      "11.469754 8.874898\n",
      "11.373888 8.894706\n",
      "11.188532 8.891124\n",
      "11.230394 8.95278\n",
      "11.911076 8.950272\n",
      "11.588358 8.941098\n",
      "11.704422 8.83852\n",
      "11.255772 9.009676\n",
      "11.726822 9.137403\n",
      "11.384993 8.971497\n",
      "11.385414 8.962512\n",
      "11.373658 8.978614\n",
      "11.386345 8.923814\n",
      "11.201086 8.880419\n",
      "11.957014 8.871698\n",
      "11.675163 8.989027\n",
      "11.793314 9.049361\n",
      "11.498714 9.013148\n",
      "11.362425 9.145825\n",
      "11.547128 9.075142\n",
      "11.582425 9.168096\n",
      "11.270912 8.931111\n",
      "11.77324 9.057792\n",
      "11.681408 8.872481\n",
      "11.740824 8.885184\n",
      "11.236685 8.982394\n",
      "10.737036 8.905733\n",
      "11.92549 9.037403\n",
      "12.025514 8.96891\n",
      "11.485959 8.955153\n",
      "10.786824 8.90537\n",
      "11.578144 8.984624\n",
      "11.592043 9.068267\n",
      "11.371784 9.050797\n",
      "11.016875 9.027065\n",
      "11.305288 9.029538\n",
      "10.704136 8.904286\n",
      "11.58637 8.959731\n",
      "11.406916 9.043735\n",
      "11.262113 8.907761\n",
      "11.672847 8.921119\n",
      "11.480429 9.095441\n",
      "12.005459 9.01605\n",
      "11.429461 8.896575\n",
      "11.9365225 9.029425\n",
      "11.460243 8.907409\n",
      "11.999769 8.975346\n",
      "11.676673 8.971515\n",
      "11.443087 8.903214\n",
      "11.690823 8.947801\n",
      "11.186104 8.931591\n",
      "11.183662 9.027011\n",
      "11.615176 9.076562\n",
      "11.94133 9.065392\n",
      "12.041948 9.150886\n",
      "11.650841 9.048626\n",
      "11.552223 9.033605\n",
      "11.56065 9.060104\n",
      "10.989655 8.991481\n",
      "11.616091 8.921852\n",
      "11.298707 8.955887\n",
      "11.136641 9.007478\n",
      "11.774191 8.990723\n",
      "11.357469 8.765552\n",
      "11.827074 9.030794\n",
      "11.021618 8.832528\n",
      "11.111261 8.970664\n",
      "11.629143 8.980485\n",
      "11.753532 9.004848\n",
      "10.832939 8.958784\n",
      "11.3108425 8.858317\n",
      "11.391316 8.939944\n",
      "11.693865 8.983641\n",
      "11.504462 8.992509\n",
      "11.457607 8.935979\n",
      "11.4326315 8.9857435\n",
      "11.158714 8.962918\n",
      "11.873138 8.988769\n",
      "11.407982 8.992497\n",
      "11.361485 9.035603\n",
      "11.066476 8.836681\n",
      "11.792089 8.965309\n",
      "11.308686 8.8583555\n",
      "11.521442 8.93954\n",
      "10.993665 8.79485\n",
      "11.237194 8.877314\n",
      "11.648457 8.998849\n",
      "11.47274 8.88399\n",
      "11.198974 9.083101\n",
      "11.277016 8.90836\n",
      "11.606829 9.056445\n",
      "11.616438 8.904292\n",
      "11.694523 8.950463\n",
      "10.430041 8.814271\n",
      "11.58322 9.040887\n",
      "11.709637 8.987149\n",
      "11.521208 9.006925\n",
      "12.010294 8.876221\n",
      "11.894693 9.000756\n",
      "11.655932 9.008618\n",
      "11.410353 8.924027\n",
      "11.873423 8.875662\n",
      "11.183311 8.897674\n",
      "11.289078 8.907959\n",
      "11.436979 8.921388\n",
      "11.401491 8.955693\n",
      "11.637544 9.077851\n",
      "11.082544 9.149844\n",
      "11.444849 8.966401\n",
      "11.2055235 8.934337\n",
      "11.933763 8.942157\n",
      "11.946895 8.893939\n",
      "11.2677 8.899386\n",
      "11.849335 8.845349\n",
      "11.787245 9.077552\n",
      "11.261642 8.899905\n",
      "11.654448 8.982821\n",
      "11.137062 8.931249\n",
      "11.910223 9.023535\n",
      "11.119568 8.938817\n",
      "12.081028 9.060182\n",
      "11.121898 9.076601\n",
      "11.486273 9.138046\n",
      "10.979761 9.018902\n",
      "11.291614 8.898034\n",
      "11.217211 8.916763\n",
      "11.531646 9.027219\n",
      "11.627987 8.912351\n",
      "12.178589 9.092293\n",
      "11.744268 8.924558\n",
      "10.9958935 8.91878\n",
      "11.53834 9.153199\n",
      "11.293509 8.997106\n",
      "11.564408 9.148082\n",
      "11.798501 9.026925\n",
      "11.407987 8.91723\n",
      "11.570237 9.10035\n",
      "11.676644 8.959084\n",
      "11.157422 8.873863\n",
      "11.793179 9.0667925\n",
      "11.204365 8.938586\n",
      "11.438442 9.042729\n",
      "11.732925 8.936317\n",
      "11.342213 8.934117\n",
      "11.149538 8.8500395\n",
      "11.856384 9.16082\n",
      "11.705303 8.86124\n",
      "11.591519 8.876977\n",
      "11.776478 8.95432\n",
      "10.903718 8.834579\n",
      "10.767187 9.006228\n",
      "11.407452 9.137351\n",
      "11.232889 9.0699005\n"
     ]
    }
   ],
   "source": [
    "for i, input_real in enumerate(dm.gds):\n",
    "    batched_graph_real, node_feats_real, edge_feats_real = input_real\n",
    "    h,l = eval_endpoints(batched_graph_real.ndata['pos']*36)\n",
    "    print(h,l)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb023811",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e81b79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576c5114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_state(model: nn.Module, optimizer: Optimizer, epoch: int, path: pathlib.Path, callbacks: List[BaseCallback]):\n",
    "    \"\"\" Saves model, optimizer and epoch states to path (only once per node) \"\"\"\n",
    "    if get_local_rank() == 0:\n",
    "        state_dict = model.module.state_dict() if isinstance(model, DistributedDataParallel) else model.state_dict()\n",
    "        checkpoint = {\n",
    "            'state_dict': state_dict,\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'epoch': epoch\n",
    "        }\n",
    "        for callback in callbacks:\n",
    "            callback.on_checkpoint_save(checkpoint)\n",
    "\n",
    "        torch.save(checkpoint, str(path))\n",
    "        logging.info(f'Saved checkpoint to {str(path)}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "51bcf1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_state(model: nn.Module, optimizer: Optimizer, path: pathlib.Path):\n",
    "    \"\"\" Loads model, optimizer and epoch states from path \"\"\"\n",
    "    checkpoint = torch.load(str(path), map_location={'cuda:0': f'cuda:0'})\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    logging.info(f'Loaded checkpoint from {str(path)}')\n",
    "    return checkpoint['epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "759a5943",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_state(model_save: nn.Module, optimizer_save: Optimizer, epoch: int, path: pathlib.Path):\n",
    "    \"\"\" Saves model, optimizer and epoch states to path (only once per node) \"\"\"\n",
    "\n",
    "    state_dict = model_save.state_dict()\n",
    "    checkpoint = {\n",
    "        'state_dict': state_dict,\n",
    "        'optimizer_state_dict': optimizer_save.state_dict(),\n",
    "        'epoch': epoch\n",
    "    }\n",
    "\n",
    "    torch.save(checkpoint, str(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5ba2aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b95c82cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_state(model, d_opt, 250, f'results/test1/d_check')\n",
    "save_state(hg, g_opt, 250, f'results/test1/g_check')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "8077cca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_state(model, d_opt, f'results/test1/d_check')\n",
    "load_state(hg, g_opt, f'results/test1/g_check')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cbee0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HGenTest():\n",
    "    \"\"\"\n",
    "    Datamodule wrapping hGen data set. 8 Helical endpoints defining a four helix protein.\n",
    "    \"\"\"\n",
    "    #8 long positional encoding\n",
    "    NODE_FEATURE_DIM = 8\n",
    "    EDGE_FEATURE_DIM = 1 # 0 or 1 helix or loop\n",
    "\n",
    "    def __init__(self,\n",
    "                 data_dir: pathlib.Path, batch_size=32):\n",
    "        \n",
    "        self.data_dir = data_dir \n",
    "        self.gds_fake = DataLoader(Fake_GraphDataset(self.data_dir),batch_size=32, shuffle=True, collate_fn=self._collate)\n",
    "\n",
    "        \n",
    "    def _collate(self, samples):\n",
    "        graphs_real, graphs_fake = map(list, zip(*samples))    \n",
    "\n",
    "        batched_graph_real = dgl.batch(graphs_real)\n",
    "        #reshape that batched graph to redivide into the individual graphs\n",
    "        edge_feats_real = {'0': batched_graph_real.edata['ss'][:, :self.EDGE_FEATURE_DIM, None]}\n",
    "        batched_graph_real.edata['rel_pos'] = _get_relative_pos(batched_graph_real)\n",
    "        # get node features\n",
    "        node_feats_real = {'0': batched_graph_real.ndata['pe'][:, :self.NODE_FEATURE_DIM, None]}\n",
    "        \n",
    "        batched_graph_fake = dgl.batch(graphs_fake)\n",
    "        #reshape that batched graph to redivide into the individual graphs\n",
    "        edge_feats_fake = {'0': batched_graph_fake.edata['ss'][:, :self.EDGE_FEATURE_DIM, None]}\n",
    "        batched_graph_fake.edata['rel_pos'] = _get_relative_pos(batched_graph_fake)\n",
    "        # get node features\n",
    "        node_feats_fake = {'0': batched_graph_fake.ndata['pe'][:, :self.NODE_FEATURE_DIM, None]}\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        return (batched_graph_real, node_feats_real, edge_feats_real, batched_graph_fake, node_feats_fake, edge_feats_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc06dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fake_GraphDataset(Dataset):\n",
    "    def __init__(self, ep_file : pathlib.Path, limit=1000):\n",
    "        self.data_path = ep_file\n",
    "        rr = np.load(data_path)\n",
    "        ep = [rr[f] for f in rr.files][0]\n",
    "        \n",
    "        ep = ep[:limit]\n",
    "        \n",
    "        v1 = np.arange(ep.shape[1]-1) #vertex 1 of edges in chronological order\n",
    "        v2 = np.arange(1,ep.shape[1]) #vertex 2 of edges in chronological order\n",
    "\n",
    "        ss = np.zeros(len(v1),dtype=np.int32)\n",
    "        ss[np.arange(ss.shape[0])%2==0]=1  #alternate 0,1 for helix, loop, helix, etc\n",
    "        ss = ss[:,None] #unsqueeze\n",
    "\n",
    "        #positional encoding of node\n",
    "        embed_dim = 8\n",
    "        scale = 10\n",
    "        i_array = np.arange(1,(embed_dim/2)+1)\n",
    "        wk = (1/(scale**(i_array*2/embed_dim)))\n",
    "        t_array = np.arange(ep.shape[1])\n",
    "        si = torch.tensor(np.sin(wk*t_array.reshape((-1,1))))\n",
    "        ci = torch.tensor(np.cos(wk*t_array.reshape((-1,1))))\n",
    "        pe = torch.stack((si,ci),axis=2).reshape(ep.shape[1],embed_dim).type(torch.float32)\n",
    "\n",
    "        graphList_real = []\n",
    "        graphList_fake = []\n",
    "        \n",
    "        #randomize\n",
    "        ep_f = ep.copy()\n",
    "        rng = np.random.default_rng()\n",
    "        rng.shuffle(ep_f,axis=1)\n",
    "        \n",
    "\n",
    "        for i,c in enumerate(ep):\n",
    "\n",
    "            g = dgl.graph((v1,v2))\n",
    "            g.ndata['pos'] = torch.tensor(c,dtype=torch.float32)\n",
    "            g.edata['ss'] = torch.tensor(ss,dtype=torch.float32)\n",
    "            g.ndata['pe'] = pe\n",
    "\n",
    "            graphList_real.append(g)\n",
    "            \n",
    "        for i,c in enumerate(ep_f):\n",
    "\n",
    "            g = dgl.graph((v1,v2))\n",
    "            g.ndata['pos'] = torch.tensor(c,dtype=torch.float32)\n",
    "            g.edata['ss'] = torch.tensor(ss,dtype=torch.float32)\n",
    "            g.ndata['pe'] = pe\n",
    "\n",
    "            graphList_fake.append(g)\n",
    "        \n",
    "        self.graphList_real = graphList_real\n",
    "        self.graphList_fake = graphList_fake\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graphList_real)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.graphList_real[idx], self.graphList_fake[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "440168db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_(data_module):\n",
    "    loss_acc = torch.zeros((1,), device='cuda')\n",
    "    for i, batch in enumerate(data_module.gds_fake):\n",
    "        batched_graph_real, node_feats_real, edge_feats_real, batched_graph_fake, node_feats_fake, edge_feats_fake = to_cuda(batch)\n",
    "\n",
    "        #print(node_feats_real['0'][0])\n",
    "        #print(edge_feats_real['0'][0])\n",
    "        #print(batched_graph_real.edata['rel_pos'])\n",
    "        pred_real = model(batched_graph_real, node_feats_real, edge_feats_real)\n",
    "        real_targets = torch.ones(pred_real.shape[0], dtype=torch.long, device='cuda')\n",
    "\n",
    "        loss_real = loss_fn(pred_real,real_targets)\n",
    "        \n",
    "        loss_real.backward()\n",
    "        optimizer.step()\n",
    "        model.zero_grad()\n",
    "\n",
    "\n",
    "        pred_fake = model(batched_graph_fake, node_feats_fake, edge_feats_fake)\n",
    "        fake_targets = torch.zeros(pred_fake.shape[0],dtype=torch.long, device='cuda')\n",
    "\n",
    "        loss_fake = loss_fn(pred_fake,fake_targets)\n",
    "\n",
    "        loss_fake.backward()\n",
    "        optimizer.step()\n",
    "        model.zero_grad()\n",
    "        \n",
    "        loss_acc += loss_fake.detach()\n",
    "        loss_acc += loss_real.detach()\n",
    "    return loss_acc / (i+1)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d9867d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a fake graph to be filled with generator outputs\n",
    "n_nodes = 8\n",
    "batch_size = 32\n",
    "\n",
    "v1 = np.arange(n_nodes-1) #vertex 1 of edges in chronological order\n",
    "v2 = np.arange(1,n_nodes) #vertex 2 of edges in chronological order\n",
    "\n",
    "ss = np.zeros(len(v1),dtype=np.int32)\n",
    "ss[np.arange(ss.shape[0])%2==0]=1  #alternate 0,1 for helix, loop, helix, etc\n",
    "ss = ss[:,None] #unsqueeze\n",
    "\n",
    "#positional encoding of node\n",
    "embed_dim = 8\n",
    "scale = 10\n",
    "i_array = np.arange(1,(embed_dim/2)+1)\n",
    "wk = (1/(scale**(i_array*2/embed_dim)))\n",
    "t_array = np.arange(ep.shape[1])\n",
    "si = torch.tensor(np.sin(wk*t_array.reshape((-1,1))))\n",
    "ci = torch.tensor(np.cos(wk*t_array.reshape((-1,1))))\n",
    "pe = torch.stack((si,ci),axis=2).reshape(n_nodes,embed_dim).type(torch.float32)\n",
    "\n",
    "graphList = []\n",
    "for i in range(batch_size):\n",
    "    g = dgl.graph((v1,v2))\n",
    "    g.edata['ss'] = torch.tensor(ss)\n",
    "    g.ndata['pe'] = pe\n",
    "    \n",
    "    graphList.append(g)\n",
    "    \n",
    "batched_graph = dgl.batch(graphList)\n",
    "#reshape that batched graph to redivide into the individual graphs\n",
    "edge_feats = {'0': batched_graph.edata['ss'][:, :8, None]}\n",
    "\n",
    "# get node features\n",
    "node_feats = {'0': batched_graph.ndata['pe'][:,:1, None]}\n",
    "  \n",
    "\n",
    "src = torch.tensor(v1,dtype=torch.long)\n",
    "dst = torch.tensor(v2,dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e1ee8f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.981705 11.658938\n",
      "26.098038 11.485578\n",
      "26.076225 11.557309\n",
      "26.028965 11.7545805\n",
      "26.1562 11.585332\n",
      "26.181648 11.615834\n",
      "26.119846 11.527458\n"
     ]
    }
   ],
   "source": [
    "for i, input_real in enumerate(dm.gds):\n",
    "    batched_graph_real, node_feats_real, edge_feats_real = input_real\n",
    "    h,l = eval_endpoints(batched_graph_real.ndata['pos']*36)\n",
    "    print(h,l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628cd555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_epochs = 100\n",
    "# for x in range(num_epochs):\n",
    "#     loss = train_epoch(dm)\n",
    "#     print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fcf253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tilBad = True\n",
    "# counter = 0\n",
    "# #shitty work around for div by zero\n",
    "# while tilBad and counter<len(gg) and counter<max:\n",
    "#     try:\n",
    "#         eprec = hh.EP_Recon(gg[:counter])\n",
    "#         arr = aa.to_npose()\n",
    "#         counter += 1\n",
    "#     except Exception:\n",
    "#         tilBad = False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "se3",
   "language": "python",
   "name": "se3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
