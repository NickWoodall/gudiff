{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7acea87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gudiff_model import PDBDataSet_GraphCon\n",
    "from gudiff_model.Graph_UNet import GraphUNet\n",
    "from data_rigid_diffuser.diffuser import FrameDiffNoise\n",
    "from se3_transformer.model.fiber import Fiber\n",
    "import torch\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "076c447f",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = {'batch_size'  : 16,\n",
    "              'topk'  : 4,\n",
    "            'stride'  : 4,\n",
    "                'KNN' : 30,\n",
    "          'num_heads' : 8,\n",
    "           'channels' : 32,\n",
    "       'channels_div' : 4,\n",
    "        'nodefeats_0': 32,\n",
    "        'nodefeats_1':  6,\n",
    "         'num_layers' : 1,\n",
    "     'num_layers_ca'  : 2,\n",
    "   'edge_feature_dim' : 1,\n",
    "  'latent_pool_type'  : 'avg',\n",
    "            't_size'  : 12,\n",
    "             'max_t'  : 0.2,\n",
    "               'mult' : 2,\n",
    "           'zero_lin' : True,\n",
    "          'use_tdeg1' : True,\n",
    "                'cuda': True,\n",
    "      'learning rate' : 0.0005,\n",
    "       'weight_decay' :  5e-6,\n",
    "        'device'      : 'cuda',\n",
    "        'num_epoch'   : 100,\n",
    "        'log_freq'    : 1000,\n",
    "        'ckpt_freq'   : 10000,\n",
    "        'early_chkpt' : 2,\n",
    "        'coord_scale' : 10.0,\n",
    "        'dataset_max' : 5000,\n",
    "        'meta_data_path' : '/mnt/h/datasets/bCov_4H/metadata.csv',\n",
    "        'sample_mode' : 'single_length',\n",
    "           'ckpt_dir' : 'GUN_checkpoints/',\n",
    "              'eval_dir' : 'Eval_Direc/',\n",
    "       }\n",
    "\n",
    "#check use_tdeg1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad33c215",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer_to(optim, device):\n",
    "    for param in optim.state.values():\n",
    "        # Not sure there are any global tensors in the state dict\n",
    "        if isinstance(param, torch.Tensor):\n",
    "            param.data = param.data.to(device)\n",
    "            if param._grad is not None:\n",
    "                param._grad.data = param._grad.data.to(device)\n",
    "        elif isinstance(param, dict):\n",
    "            for subparam in param.values():\n",
    "                if isinstance(subparam, torch.Tensor):\n",
    "                    subparam.data = subparam.data.to(device)\n",
    "                    if subparam._grad is not None:\n",
    "                        subparam._grad.data = subparam._grad.data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5eb8587",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "\n",
    "    def __init__(self,\n",
    "                 conf,\n",
    "                 ckpt_model=None,\n",
    "                 cur_step=None,\n",
    "                 cur_epoch=None,\n",
    "                 name='gu_null',\n",
    "                 cast_type=torch.float32,\n",
    "                 ckpt_opt=None):\n",
    "        \"\"\"Initialize experiment.\n",
    "        Args:\n",
    "            exp_cfg: Experiment configuration.\n",
    "        \"\"\"\n",
    "#         with open(config_path, 'r') as file:\n",
    "#             config = yaml.safe_load(file)\n",
    "#         conf = Struct(config)\n",
    "        #figure out logging\n",
    "        logging.basicConfig(filename='test.log', level=logging.INFO)\n",
    "        self._log = logging.getLogger(__name__)\n",
    "        \n",
    "\n",
    "        self.name=name\n",
    "        self._conf = conf\n",
    "        if conf['cuda']:\n",
    "            self.device = 'cuda'\n",
    "        else:\n",
    "            self.device = 'cpu'\n",
    "        \n",
    "        self.coord_scale = conf['coord_scale']\n",
    "        self.N_CA_dist = (PDBDataSet_GraphCon.N_CA_dist/self.coord_scale).to('cuda')\n",
    "        self.C_CA_dist = (PDBDataSet_GraphCon.C_CA_dist/self.coord_scale).to('cuda')\n",
    "        self.cast_type = cast_type\n",
    "        \n",
    "        self.num_epoch = conf['num_epoch']\n",
    "        self.log_freq = conf['log_freq']\n",
    "        self.ckpt_freq = conf['ckpt_freq']\n",
    "        self.early_ckpt = conf['early_chkpt']\n",
    "        \n",
    "        \n",
    "        self.meta_data_path = conf['meta_data_path']\n",
    "        self.sample_mode = conf['sample_mode']\n",
    "        self.B = conf['batch_size']\n",
    "        self.limit = conf['dataset_max']\n",
    "        \n",
    "        #graph properties\n",
    "        self.KNN = conf['KNN']\n",
    "        self.stride = conf['stride']\n",
    "        \n",
    "        #gudiff params\n",
    "        self.channels_start = conf['channels']\n",
    "        \n",
    "        \n",
    "        self._diffuser = FrameDiffNoise()\n",
    "        self._graphmaker =  PDBDataSet_GraphCon.Make_KNN_MP_Graphs(mp_stride = self.stride, \n",
    "                                                           coord_div = self.coord_scale, \n",
    "                                                           cast_type = self.cast_type, \n",
    "                                                           channels_start = self.channels_start,\n",
    "                                                           ndf1= conf['nodefeats_1'], \n",
    "                                                           ndf0= conf['nodefeats_0'],\n",
    "                                                           cuda=conf['cuda']) #cuda is bool True, mod at some point\n",
    "        #single_t dataset, for testing\n",
    "        # sd = smallPDBDataset(fdn , meta_data_path = '/mnt/h/datasets/bCov_4H/metadata.csv', \n",
    "        #                      filter_dict=False, maxlen=1000, input_t=0.05)\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        self._model = GraphUNet(fiber_start = Fiber({0:12, 1:2}),\n",
    "                                fiber_out = Fiber({1:2}),\n",
    "                                batch_size = self.B, \n",
    "                                num_layers_ca = conf['num_layers_ca'],\n",
    "                                k = conf['topk'],\n",
    "                                stride = conf['stride'],\n",
    "                                max_degree = 3,\n",
    "                                channels_div =  conf['channels_div'],\n",
    "                                num_heads = conf['num_heads'],\n",
    "                                num_layers = conf['num_layers'],\n",
    "                                edge_feature_dim = conf['edge_feature_dim'],\n",
    "                                latent_pool_type = conf['latent_pool_type'],\n",
    "                                t_size = conf['t_size'],\n",
    "                                zero_lin = conf['zero_lin'],\n",
    "                                use_tdeg1 = conf['use_tdeg1'],\n",
    "                                cuda = conf['cuda']).to(self.device) #cuda is bool True, mod at some point\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        num_parameters = sum(p.numel() for p in self._model.parameters())\n",
    "        self.num_parameters = num_parameters\n",
    "        self._log.info(f'Number of model parameters {num_parameters}')\n",
    "#         self._optimizer = EMA(0.980)\n",
    "#         for name, param in self._model.named_parameters():\n",
    "#             if param.requires_grad:\n",
    "#                 self._optimizer.register(name, param.data)\n",
    "\n",
    "        if ckpt_model is not None:\n",
    "            ckpt_model = {k.replace('module.', ''):v for k,v in ckpt_model.items()}\n",
    "            self._model.load_state_dict(ckpt_model, strict=True)\n",
    "        \n",
    "        \n",
    "        self._optimizer = torch.optim.Adam( self._model.parameters(),\n",
    "                                                       lr=conf['learning rate'],\n",
    "                                                       weight_decay=conf['weight_decay'])\n",
    "        if ckpt_opt is not None:\n",
    "            self._optimizer.load_state_dict(ckpt_opt)\n",
    "            optimizer_to(self._optimizer, self.device)\n",
    "        \n",
    "        \n",
    "        dt_string = datetime.now().strftime(\"%dD_%mM_%YY_%Hh_%Mm_%Ss\")\n",
    "        dt_string_short = datetime.now().strftime(\"%dD_%mM_%YY\")\n",
    "        self.ckpt_dir =  conf['ckpt_dir']\n",
    "        self.eval_dir = conf['eval_dir']\n",
    "        eval_name = f'{self.name}_{dt_string_short}'\n",
    "        if self.ckpt_dir is not None:\n",
    "            # Set-up checkpoint location\n",
    "            ckpt_dir = os.path.join(\n",
    "                 self.ckpt_dir,\n",
    "                 self.name,\n",
    "                 dt_string)\n",
    "            if not os.path.exists(ckpt_dir):\n",
    "                os.makedirs(ckpt_dir, exist_ok=True)\n",
    "            self.ckpt_dir = ckpt_dir\n",
    "            self._log.info(f'Checkpoints saved to: {ckpt_dir}')\n",
    "        else:  \n",
    "            self._log.info('Checkpoint not being saved.')\n",
    "            \n",
    "        if self.eval_dir is not None :\n",
    "            self.eval_dir = os.path.join(\n",
    "                self.eval_dir,\n",
    "                eval_name,\n",
    "                dt_string)\n",
    "            self.eval_dir = self.eval_dir\n",
    "            self._log.info(f'Evaluation saved to: {self.eval_dir}')\n",
    "        else:\n",
    "            self.eval_dir = os.devnull\n",
    "            self._log.info(f'Evaluation will not be saved.')\n",
    "    #         self._aux_data_history = deque(maxlen=100)\n",
    "    \n",
    "        if cur_epoch is None:\n",
    "            self.trained_epochs = 0\n",
    "        else:\n",
    "            self.trained_epochs = cur_epoch\n",
    "            \n",
    "        if cur_step is None:\n",
    "            self.trained_steps = 0\n",
    "        else:\n",
    "            self.trained_steps = cur_step\n",
    "            \n",
    "    @property\n",
    "    def diffuser(self):\n",
    "        return self._diffuser\n",
    "\n",
    "    @property\n",
    "    def model(self):\n",
    "        return self._model\n",
    "\n",
    "    @property\n",
    "    def conf(self):\n",
    "        return self._conf\n",
    "    \n",
    "    def create_dataset(self, fake_valid=True):\n",
    "        \n",
    "        \n",
    "        self.dataset = PDBDataSet_GraphCon.smallPDBDataset( self._diffuser , meta_data_path = self.meta_data_path, \n",
    "                             filter_dict=False, maxlen=self.limit)\n",
    "        \n",
    "        self.train_sample = PDBDataSet_GraphCon.TrainSampler(self.B, self.dataset, sample_mode='single_length')\n",
    "        \n",
    "        train_dL = torch.utils.data.DataLoader(self.dataset, sampler=self.train_sample,\n",
    "                                                     batch_size=self.B, shuffle=False, collate_fn=None)\n",
    "        \n",
    "        if fake_valid:\n",
    "            valid_dL = train_dL\n",
    "        else:\n",
    "            valid_dL = train_dL\n",
    "            #not implemented yet\n",
    "        \n",
    "        return train_dL, valid_dL\n",
    "    #unchecked\n",
    "    def start_training(self, return_logs=False):\n",
    "\n",
    "\n",
    "        self._model = self._model.to(self.device)\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "        self._model.train()\n",
    "        (train_loader, valid_loader) = self.create_dataset()\n",
    "\n",
    "        logs = []\n",
    "        print('number of epochs', self.num_epoch)\n",
    "        for epoch in range(self.trained_epochs, self.num_epoch+self.trained_epochs):\n",
    "            print('epoch', epoch)\n",
    "            print('mem_used',torch.cuda.memory_allocated('cuda:0'))\n",
    "            epoch_log = self.train_epoch(\n",
    "                train_loader,\n",
    "                valid_loader,\n",
    "                epoch=epoch,\n",
    "                return_logs=return_logs\n",
    "            )\n",
    "            if return_logs:\n",
    "                logs.append(epoch_log)\n",
    "\n",
    "        self._log.info('Done')\n",
    "        return logs\n",
    "    #unchecked\n",
    "    def train_epoch(self, train_loader, valid_loader,epoch=0, return_logs=False):\n",
    "        \n",
    "        log_lossses = defaultdict(list)\n",
    "    \n",
    "        global_logs = []\n",
    "        log_time = time.time()\n",
    "        step_time = time.time()\n",
    "        losskeeper = []\n",
    "        for train_feats in train_loader:\n",
    "            \n",
    "            #train_feats = tree.map_structure(lambda x: x.to(device), train_feats)\n",
    "            loss, aux_data = self.update_fn(train_feats)\n",
    "            for k,v in aux_data.items():\n",
    "                log_lossses[k].append(np.array(v))\n",
    "            \n",
    "            self.trained_steps += 1\n",
    "\n",
    "            \n",
    "            \n",
    "            # Logging to terminal\n",
    "            if self.trained_steps == 1 or self.trained_steps % self.log_freq == 0:\n",
    "                elapsed_time = time.time() - log_time\n",
    "                log_time = time.time()\n",
    "                step_per_sec = self.log_freq / elapsed_time\n",
    "                rolling_losses = tree.map_structure(np.mean, log_lossses)\n",
    "                loss_log = ' '.join([\n",
    "                    f'{k}={v[0]:.4f}'\n",
    "                    for k,v in rolling_losses.items() if 'batch' not in k\n",
    "                ])\n",
    "                \n",
    "                self._log.info(\n",
    "                    f'[{self.trained_steps}]: {loss_log}, steps/sec={step_per_sec:.5f}')\n",
    "                log_lossses = defaultdict(list)\n",
    "                \n",
    "                print(f'[{self.trained_steps}]: {loss_log}, steps/sec={step_per_sec:.5f}')\n",
    "                print(np.mean(losskeeper[-1000:]))\n",
    "\n",
    "            # Take checkpoint\n",
    "            \n",
    "            if self.ckpt_dir is not None and (\n",
    "                    (self.trained_steps % self.ckpt_freq) == 0\n",
    "                    or (self.early_ckpt and self.trained_steps == 2)\n",
    "                ):\n",
    "                ckpt_path = os.path.join(\n",
    "                    self.ckpt_dir, f'step_{self.trained_steps}.pth')\n",
    "                du.write_checkpoint(\n",
    "                    ckpt_path,\n",
    "                    copy.deepcopy(self.model.state_dict()),\n",
    "                    self._conf,\n",
    "                    copy.deepcopy(self._optimizer.state_dict()),\n",
    "                    self.trained_epochs,\n",
    "                    self.trained_steps,\n",
    "                    logger=self._log,\n",
    "                    use_torch=True\n",
    "                )\n",
    "                \n",
    "\n",
    "                # Run evaluation\n",
    "                self._log.info(f'Running evaluation of {ckpt_path}')\n",
    "                start_time = time.time()\n",
    "                eval_dir = os.path.join(self.eval_dir, f'step_{self.trained_steps}')\n",
    "                print('eval',eval_dir)\n",
    "                os.makedirs(eval_dir, exist_ok=True)\n",
    "                ckpt_metrics = self.eval_fn(valid_loader,eval_dir,epoch=epoch)\n",
    "                eval_time = time.time() - start_time\n",
    "                self._log.info(f'Finished evaluation in {eval_time:.2f}s')\n",
    "            else:\n",
    "                ckpt_metrics = None\n",
    "                eval_time = None\n",
    "\n",
    "\n",
    "            if torch.isnan(loss):                \n",
    "                raise Exception(f'NaN encountered')\n",
    "                \n",
    "    def update_fn(self, data):\n",
    "        \"\"\"Updates the state using some data and returns metrics.\"\"\"\n",
    "        self._optimizer.zero_grad()\n",
    "        loss, aux_data = self.loss_fn(data)\n",
    "        loss.backward()\n",
    "        self._optimizer.step()\n",
    "        loss_out = loss.detach().cpu()\n",
    "        return loss_out , aux_data\n",
    "    \n",
    "    def loss_fn(self, batch_feats, noised_dict, t_val=None):\n",
    "        \n",
    "        L = batch_feats['CA'].shape[1]\n",
    "        B = batch_feats['CA'].shape[0]\n",
    "        CA_t  = batch_feats['CA']\n",
    "        NC_t = CA_t +  batch_feats['N_CA']\n",
    "        CC_t = CA_t +  batch_feats['C_CA']\n",
    "        true =  torch.cat((NC_t,CA_t,CC_t),dim=2).reshape(B,L,3,3)\n",
    "\n",
    "        CA_n  = batch_feats['CA_noised'].reshape(B, L, 3)\n",
    "        NC_n = CA_n + batch_feats['N_CA_noised'].reshape(B, L, 3)\n",
    "        CC_n = CA_n + batch_feats['C_CA_noised'].reshape(B, L, 3)\n",
    "        noise_xyz =  torch.cat((NC_n,CA_n,CC_n),dim=2).reshape(B,L,3,3)\n",
    "\n",
    "        x = self._graphmaker.prep_for_network(noised_dict)\n",
    "        out = self._model(x, batch_feats['t'])\n",
    "        CA_p = out['1'][:,0,:].reshape(B, L, 3) + CA_n #translation of Calpha\n",
    "        Qs = out['1'][:,1,:] # rotation of frame\n",
    "        Qs = Qs.unsqueeze(1).repeat((1,2,1))\n",
    "        Qs = torch.cat((torch.ones((B*L,2,1),device=Qs.device),Qs),dim=-1).reshape(B,L,2,4)\n",
    "        Qs = normQ(Qs)\n",
    "        Rs = Qs2Rs(Qs)\n",
    "        N_C_to_Rot = torch.cat((noised_dict['N_CA'].reshape(B, L, 3),\n",
    "                                noised_dict['C_CA'].reshape(B, L, 3)),dim=2).reshape(B,L,2,1,3)\n",
    "\n",
    "        rot_vecs = einsum('bnkij,bnkhj->bnki',Rs, N_C_to_Rot)\n",
    "        NC_p = CA_p + rot_vecs[:,:,0,:]*N_CA_dist #remove bc who cares? me maybe\n",
    "        CC_p = CA_p + rot_vecs[:,:,1,:]*C_CA_dist #remove maybe\n",
    "\n",
    "        pred = torch.cat((NC_p,CA_p,CC_p),dim=2).reshape(B,L,3,3)\n",
    "\n",
    "        tloss, loss = FAPE_loss(pred.unsqueeze(0), true, batch_feats['score_scale'])\n",
    "\n",
    "        return tloss, loss #final_loss, aux_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa505c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = Experiment(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b20d7957",
   "metadata": {},
   "outputs": [],
   "source": [
    "tl, vl = exp.create_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f398761",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
