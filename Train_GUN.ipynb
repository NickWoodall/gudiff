{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a02a1c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "#clear memory better\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "import numpy as np\n",
    "import util.npose_util as nu\n",
    "import util.framediff_utils as du\n",
    "import util.pdb_writer \n",
    "import os\n",
    "import pathlib\n",
    "import dgl\n",
    "import copy\n",
    "from dgl import backend as F\n",
    "import torch_geometric\n",
    "from torch.utils.data import random_split, DataLoader, Dataset\n",
    "from typing import Dict\n",
    "from torch import Tensor\n",
    "from dgl import DGLGraph\n",
    "from torch import nn\n",
    "from chemical import cos_ideal_NCAC #from RoseTTAFold2\n",
    "from torch import einsum\n",
    "import time\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "import tree\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "936c747c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gudiff_model.Graph_UNet_Null import  GraphUNet_Null\n",
    "from gudiff_model import Data_Graph\n",
    "from gudiff_model.Data_Graph_Null import build_npose_from_coords, dump_coord_pdb, define_graph_edges, make_pe_encoding\n",
    "from gudiff_model.Data_Graph_Null import  Make_nullKNN_MP_Graphs\n",
    "from data_rigid_diffuser.diffuser import FrameDiffNoise\n",
    "from se3_transformer.model.FAPE_Loss import FAPE_loss, Qs2Rs, normQ\n",
    "from se3_transformer.model.FAPE_Loss import FAPE_loss_null, FAPE_loss_real\n",
    "from se3_transformer.model.fiber import Fiber\n",
    "from se3_transformer.runtime.utils import str2bool, to_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3062c7e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d298f50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#indices for, unsure if needed\n",
    "CA = Data_Graph.CA\n",
    "N = Data_Graph.N\n",
    "C = Data_Graph.C\n",
    "\n",
    "# #find better way to incorporate coord_scale\n",
    "\n",
    "#needed\n",
    "N_CA_dist = (Data_Graph.N_CA_dist/10.).to('cuda')\n",
    "C_CA_dist = (Data_Graph.C_CA_dist/10.).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27011d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3b7b32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "705e5873",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer_to(optim, device):\n",
    "    for param in optim.state.values():\n",
    "        # Not sure there are any global tensors in the state dict\n",
    "        if isinstance(param, torch.Tensor):\n",
    "            param.data = param.data.to(device)\n",
    "            if param._grad is not None:\n",
    "                param._grad.data = param._grad.data.to(device)\n",
    "        elif isinstance(param, dict):\n",
    "            for subparam in param.values():\n",
    "                if isinstance(subparam, torch.Tensor):\n",
    "                    subparam.data = subparam.data.to(device)\n",
    "                    if subparam._grad is not None:\n",
    "                        subparam._grad.data = subparam._grad.data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72ce141d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "\n",
    "    def __init__(self,\n",
    "                 config_path= 'test.config',\n",
    "                 config_path_diffuser = 'data_rigid_diffuser/base.yaml',\n",
    "                 limit=5028,\n",
    "                 conf=None,\n",
    "                 ckpt_model=None,\n",
    "                 cur_step=None,\n",
    "                 cur_epoch=None,\n",
    "                name='gu_null',\n",
    "                ckpt_opt=None):\n",
    "        \"\"\"Initialize experiment.\n",
    "        Args:\n",
    "            exp_cfg: Experiment configuration.\n",
    "        \"\"\"\n",
    "#         with open(config_path, 'r') as file:\n",
    "#             config = yaml.safe_load(file)\n",
    "#         conf = Struct(config)\n",
    "        \n",
    "        logging.basicConfig(filename='test.log', level=logging.INFO)\n",
    "        self._log = logging.getLogger(__name__)\n",
    "        \n",
    "        \n",
    "        #indices for, unsure if needed\n",
    "        self.CA = Data_Graph.CA\n",
    "        self.N = Data_Graph.N\n",
    "        self.C = Data_Graph.C\n",
    "\n",
    "        self.name=name\n",
    "        \n",
    "        config_path='data_rigid_diffuser/base.yaml'\n",
    "        \n",
    "        if conf is None:\n",
    "            #currently faking valid loader in create_dataset\n",
    "            conf = {'batch_size'  : 4,\n",
    "                          'topk'  : 4,\n",
    "                        'stride'  : 8,\n",
    "                            'KNN' : 30,\n",
    "                      'num_heads' : 16,\n",
    "                       'channels' : 64,\n",
    "                   'channels_div' : 8,\n",
    "                     'num_layers' : 1,\n",
    "                 'num_layers_ca'  : 2,\n",
    "               'edge_feature_dim' : 1,\n",
    "              'latent_pool_type'  : 'avg',\n",
    "                        't_size'  : 12,\n",
    "                         'max_t'  : 0.2,\n",
    "                           'mult' : 2,\n",
    "                       'zero_lin' : True,\n",
    "                      'use_tdeg1' : False,\n",
    "                       'roll': False,\n",
    "                       'circ_pe':False,\n",
    "                            'cuda': True,\n",
    "                  'learning rate' : 0.0001,\n",
    "                   'weight_decay' : 0.000001,\n",
    "                    'sc_nf_real'  : 0.05 ,\n",
    "                    'sc_3D_real'  : 1.0 ,\n",
    "                    'sc_3D_null'  : 0.05 ,\n",
    "                    'device'      : 'cuda',\n",
    "                    'num_epoch'   : 100,\n",
    "                    'log_freq'    : 1000,\n",
    "                    'ckpt_freq'   : 10000,\n",
    "                    'early_chkpt' : 2,\n",
    "                    'coord_scale' : 10.0,\n",
    "                    'nf_threshold_real': 1.99,\n",
    "                    'nf_dim': 5}\n",
    "                \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        conf_check = {'ckpt_dir' : 'GUN_checkpoints/',\n",
    "                      'eval_dir' : 'Eval_Direc/',\n",
    "                      'apa_path' : 'data_npose/h4_apa_coords.npz',\n",
    "                      'tog_path' : 'data_npose/h4_tog_coords.npz',\n",
    "                     }\n",
    "        self._conf = conf\n",
    "        \n",
    "\n",
    "        \n",
    "        self.device = conf['device']\n",
    "        self.paths_valid = conf_check['apa_path']\n",
    "        self.paths_train = conf_check['tog_path']\n",
    "        self.score_weights = {}\n",
    "        self.score_weights['nf_real'] = torch.tensor(float(conf['sc_nf_real']),device=self.device)\n",
    "        self.score_weights['3D_real'] = torch.tensor(float(conf['sc_3D_real']),device=self.device)\n",
    "        self.score_weights['3D_null'] = torch.tensor(float(conf['sc_3D_null']),device=self.device)\n",
    "\n",
    "        #needed\n",
    "        self.coord_scale = conf['coord_scale']\n",
    "        self.N_CA_dist = (Data_Graph.N_CA_dist/self.coord_scale).to('cuda')\n",
    "        self.C_CA_dist = (Data_Graph.C_CA_dist/self.coord_scale).to('cuda')\n",
    "        \n",
    "        self.num_epoch = conf['num_epoch']\n",
    "        self.log_freq = conf['log_freq']\n",
    "        self.ckpt_freq = conf['ckpt_freq']\n",
    "        self.early_ckpt = conf['early_chkpt']\n",
    "        \n",
    "        self.B = conf['batch_size']\n",
    "        self.L = 128\n",
    "        self.limit = limit\n",
    "        self.KNN = conf['KNN']\n",
    "        self.stride = conf['stride']\n",
    "        self.real_threshold = conf['nf_threshold_real']\n",
    "        self.nf_dim = conf['nf_dim'] #need to connect via creation hardcoded just here now for loss\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.fnd = FrameDiffNoise(config_path_diffuser, roll = conf['roll'], max_t = conf['max_t'])\n",
    "        self.mkg = Make_nullKNN_MP_Graphs(KNN = self.KNN, mp_stride = self.stride, n_nodes = self.L, \n",
    "                                           real_threshold = self.real_threshold,  pe_circ_encode=conf['circ_pe'])\n",
    "        \n",
    "        #set nf_dim on fiber out\n",
    "        self._model = GraphUNet_Null(fiber_start = Fiber({0:17, 1:2}),\n",
    "                                     fiber_out = Fiber({0:5,1:2}),\n",
    "                                          k = conf['topk'],\n",
    "                                batch_size  = self.B,\n",
    "                                     stride = conf['stride'],\n",
    "                                 max_degree = 3,\n",
    "                                   channels = conf['channels'],\n",
    "                                  num_heads = conf['num_heads'],\n",
    "                               channels_div = conf['channels_div'],\n",
    "                                 num_layers = conf['num_layers'],\n",
    "                              num_layers_ca = conf['num_layers_ca'],\n",
    "                           edge_feature_dim = conf['edge_feature_dim'],\n",
    "                           latent_pool_type = conf['latent_pool_type'],\n",
    "                                     t_size = conf['t_size'],\n",
    "                                       mult = conf['mult'],\n",
    "                                   zero_lin = conf['zero_lin'],\n",
    "                                  use_tdeg1 = conf['use_tdeg1'],\n",
    "                                        cuda= conf['cuda'])\n",
    "        \n",
    "        \n",
    "        num_parameters = sum(p.numel() for p in self._model.parameters())\n",
    "        self.num_parameters = num_parameters\n",
    "        self._log.info(f'Number of model parameters {num_parameters}')\n",
    "#         self._optimizer = EMA(0.980)\n",
    "#         for name, param in self._model.named_parameters():\n",
    "#             if param.requires_grad:\n",
    "#                 self._optimizer.register(name, param.data)\n",
    "\n",
    "        if ckpt_model is not None:\n",
    "            ckpt_model = {k.replace('module.', ''):v for k,v in ckpt_model.items()}\n",
    "            self._model.load_state_dict(ckpt_model, strict=True)\n",
    "        \n",
    "        \n",
    "        self._optimizer = torch.optim.Adam( self._model.parameters(),\n",
    "                                                       lr=conf['learning rate'],\n",
    "                                                       weight_decay=conf['weight_decay'])\n",
    "        if ckpt_opt is not None:\n",
    "            self._optimizer.load_state_dict(ckpt_opt)\n",
    "            optimizer_to(self._optimizer, self.device)\n",
    "        \n",
    "        \n",
    "        dt_string = datetime.now().strftime(\"%dD_%mM_%YY_%Hh_%Mm_%Ss\")\n",
    "        dt_string_short = datetime.now().strftime(\"%dD_%mM_%YY\")\n",
    "        self.ckpt_dir =  conf_check['ckpt_dir']\n",
    "        self.eval_dir = conf_check['eval_dir']\n",
    "        eval_name = f'{self.name}_{dt_string_short}'\n",
    "        if self.ckpt_dir is not None:\n",
    "            # Set-up checkpoint location\n",
    "            ckpt_dir = os.path.join(\n",
    "                 self.ckpt_dir,\n",
    "                 self.name,\n",
    "                 dt_string)\n",
    "            if not os.path.exists(ckpt_dir):\n",
    "                os.makedirs(ckpt_dir, exist_ok=True)\n",
    "            self.ckpt_dir = ckpt_dir\n",
    "            self._log.info(f'Checkpoints saved to: {ckpt_dir}')\n",
    "        else:  \n",
    "            self._log.info('Checkpoint not being saved.')\n",
    "            \n",
    "        if self.eval_dir is not None :\n",
    "            self.eval_dir = os.path.join(\n",
    "                self.eval_dir,\n",
    "                eval_name,\n",
    "                dt_string)\n",
    "            self.eval_dir = self.eval_dir\n",
    "            self._log.info(f'Evaluation saved to: {self.eval_dir}')\n",
    "        else:\n",
    "            self.eval_dir = os.devnull\n",
    "            self._log.info(f'Evaluation will not be saved.')\n",
    "    #         self._aux_data_history = deque(maxlen=100)\n",
    "    \n",
    "        if cur_epoch is None:\n",
    "            self.trained_epochs = 0\n",
    "        else:\n",
    "            self.trained_epochs = cur_epoch\n",
    "            \n",
    "        if cur_step is None:\n",
    "            self.trained_steps = 0\n",
    "        else:\n",
    "            self.trained_steps = cur_step\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def diffuser(self):\n",
    "        return self._diffuser\n",
    "\n",
    "    @property\n",
    "    def model(self):\n",
    "        return self._model\n",
    "\n",
    "    @property\n",
    "    def conf(self):\n",
    "        return self._conf\n",
    "    \n",
    "    def create_dataset(self, fake_valid=True):\n",
    "        \n",
    "        \n",
    "        \n",
    "        #grab the first 3 atoms which are N,CA,C\n",
    "        rr = np.load(self.paths_train)\n",
    "        coords_train = [rr[f] for f in rr.files][0][:self.limit,:]\n",
    "        \n",
    "        rr = np.load(self.paths_valid)\n",
    "        coords_valid = [rr[f] for f in rr.files][0][:self.limit,:]\n",
    "        \n",
    "        self.B\n",
    "        \n",
    "        device='cuda'\n",
    "        \n",
    "        prot_trainData = Data_Graph.ProteinBB_Dataset(coords_train[:self.limit], n_nodes=self.L,\n",
    "                      n_atoms=5, coord_div=10, cast_type=torch.float32)\n",
    "        train_dL = DataLoader(prot_trainData, batch_size=self.B, shuffle=True, drop_last=True)\n",
    "        \n",
    "        prot_validData = Data_Graph.ProteinBB_Dataset(coords_valid[:self.limit], n_nodes=self.L,\n",
    "                      n_atoms=5, coord_div=10, cast_type=torch.float32)\n",
    "        if fake_valid:\n",
    "            valid_dL = train_dL\n",
    "        else:\n",
    "            valid_dL = DataLoader(prot_trainData, batch_size=self.B, shuffle=True, drop_last=True)\n",
    "        \n",
    "        return train_dL, valid_dL\n",
    "    \n",
    "    def start_training(self, return_logs=False):\n",
    "\n",
    "\n",
    "        self._model = self._model.to(self.device)\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "        self._model.train()\n",
    "        (train_loader, valid_loader) = self.create_dataset()\n",
    "\n",
    "        logs = []\n",
    "        print('number of epochs', self.num_epoch)\n",
    "        for epoch in range(self.trained_epochs, self.num_epoch+self.trained_epochs):\n",
    "            print('epoch', epoch)\n",
    "            print('mem_used',torch.cuda.memory_allocated('cuda:0'))\n",
    "            epoch_log = self.train_epoch(\n",
    "                train_loader,\n",
    "                valid_loader,\n",
    "                epoch=epoch,\n",
    "                return_logs=return_logs\n",
    "            )\n",
    "            if return_logs:\n",
    "                logs.append(epoch_log)\n",
    "\n",
    "        self._log.info('Done')\n",
    "        return logs\n",
    "    \n",
    "    \n",
    "    \n",
    "    def train_epoch(self, train_loader, valid_loader,epoch=0, return_logs=False):\n",
    "        \n",
    "        log_lossses = defaultdict(list)\n",
    "        \n",
    "        global_logs = []\n",
    "        \n",
    "        log_time = time.time()\n",
    "        step_time = time.time()\n",
    "        losskeeper = []\n",
    "        for train_feats in train_loader:\n",
    "            \n",
    "            #train_feats = tree.map_structure(lambda x: x.to(device), train_feats)\n",
    "            loss, aux_data = self.update_fn(train_feats)\n",
    "            \n",
    "            losskeeper.append(float(loss))\n",
    "            \n",
    "            \n",
    "            for k,v in aux_data.items():\n",
    "                log_lossses[k].append(np.array(v))\n",
    "            \n",
    "            self.trained_steps += 1\n",
    "\n",
    "            \n",
    "            \n",
    "            # Logging to terminal\n",
    "            if self.trained_steps == 1 or self.trained_steps % self.log_freq == 0:\n",
    "                elapsed_time = time.time() - log_time\n",
    "                log_time = time.time()\n",
    "                step_per_sec = self.log_freq / elapsed_time\n",
    "                rolling_losses = tree.map_structure(np.mean, log_lossses)\n",
    "                loss_log = ' '.join([\n",
    "                    f'{k}={v[0]:.4f}'\n",
    "                    for k,v in rolling_losses.items() if 'batch' not in k\n",
    "                ])\n",
    "                \n",
    "                self._log.info(\n",
    "                    f'[{self.trained_steps}]: {loss_log}, steps/sec={step_per_sec:.5f}')\n",
    "                log_lossses = defaultdict(list)\n",
    "                \n",
    "                print(f'[{self.trained_steps}]: {loss_log}, steps/sec={step_per_sec:.5f}')\n",
    "                print(np.mean(losskeeper[-1000:]))\n",
    "\n",
    "            # Take checkpoint\n",
    "            \n",
    "            if self.ckpt_dir is not None and (\n",
    "                    (self.trained_steps % self.ckpt_freq) == 0\n",
    "                    or (self.early_ckpt and self.trained_steps == 2)\n",
    "                ):\n",
    "                ckpt_path = os.path.join(\n",
    "                    self.ckpt_dir, f'step_{self.trained_steps}.pth')\n",
    "                du.write_checkpoint(\n",
    "                    ckpt_path,\n",
    "                    copy.deepcopy(self.model.state_dict()),\n",
    "                    self._conf,\n",
    "                    copy.deepcopy(self._optimizer.state_dict()),\n",
    "                    self.trained_epochs,\n",
    "                    self.trained_steps,\n",
    "                    logger=self._log,\n",
    "                    use_torch=True\n",
    "                )\n",
    "                \n",
    "\n",
    "                # Run evaluation\n",
    "                self._log.info(f'Running evaluation of {ckpt_path}')\n",
    "                start_time = time.time()\n",
    "                eval_dir = os.path.join(self.eval_dir, f'step_{self.trained_steps}')\n",
    "                print('eval',eval_dir)\n",
    "                os.makedirs(eval_dir, exist_ok=True)\n",
    "                ckpt_metrics = self.eval_fn(valid_loader,eval_dir,epoch=epoch)\n",
    "                eval_time = time.time() - start_time\n",
    "                self._log.info(f'Finished evaluation in {eval_time:.2f}s')\n",
    "            else:\n",
    "                ckpt_metrics = None\n",
    "                eval_time = None\n",
    "\n",
    "\n",
    "            if torch.isnan(loss):                \n",
    "                raise Exception(f'NaN encountered')\n",
    "                \n",
    "            del loss\n",
    "            for k,v in aux_data.items():\n",
    "                del v\n",
    "\n",
    "\n",
    "\n",
    "    def update_fn(self, data):\n",
    "        \"\"\"Updates the state using some data and returns metrics.\"\"\"\n",
    "        self._optimizer.zero_grad()\n",
    "        loss, aux_data = self.loss_fn(data)\n",
    "        loss.backward()\n",
    "        self._optimizer.step()\n",
    "        loss_out = loss.detach().cpu()\n",
    "        #del loss\n",
    "        return loss_out , aux_data\n",
    "    \n",
    "    def eval_model(self, noised_dict, batched_t):\n",
    "        \n",
    "        def convert_pV_to_points(dict_in, L, key_in='bb_firstp', return_indiv=False):\n",
    "            \"\"\"Concatenates to xyz from Calpha+atom vectors\"\"\"\n",
    "            CA_fp  = dict_in[key_in]['CA'].reshape(self.B, L, 3).to(self.device)\n",
    "            NC_fp = CA_fp + dict_in[key_in]['N_CA'].reshape(self.B, L, 3).to(self.device)\n",
    "            CC_fp = CA_fp + dict_in[key_in]['C_CA'].reshape(self.B, L, 3).to(self.device)\n",
    "            fp =  torch.cat((NC_fp,CA_fp,CC_fp),dim=2).reshape(self.B,L,3,3)\n",
    "            if return_indiv:\n",
    "                return fp, CA_fp, NC_fp, CC_fp\n",
    "            return fp\n",
    "    \n",
    "        CA_t  = noised_dict['bb_shifted']['CA'].reshape(self.B, self.L, 3).to(self.device)\n",
    "        NC_t = CA_t + noised_dict['bb_shifted']['N_CA'].reshape(self.B, self.L, 3).to(self.device)*self.N_CA_dist\n",
    "        CC_t = CA_t + noised_dict['bb_shifted']['C_CA'].reshape(self.B, self.L, 3).to(self.device)*self.C_CA_dist\n",
    "        true =  torch.cat((NC_t,CA_t,CC_t),dim=2).reshape(self.B,self.L,3,3)\n",
    "\n",
    "        CA_n  = noised_dict['bb_noised']['CA'].reshape(self.B, self.L, 3).to(self.device)\n",
    "        NC_n = CA_n + noised_dict['bb_noised']['N_CA'].reshape(self.B, self.L, 3).to(self.device)*self.N_CA_dist\n",
    "        CC_n = CA_n + noised_dict['bb_noised']['C_CA'].reshape(self.B, self.L, 3).to(self.device)*self.C_CA_dist\n",
    "        noise_xyz =  torch.cat((NC_n,CA_n,CC_n),dim=2).reshape(self.B, self.L,3,3)\n",
    "        \n",
    "        feat_dict = self.mkg.prep_for_network(noised_dict) #prepares graphs\n",
    "        with torch.no_grad():\n",
    "            out = self._model(feat_dict, batched_t)\n",
    "            CA_p = out['1'][:,0,:].reshape(self.B, self.L, 3) + CA_n #translation of Calpha\n",
    "            Qs = out['1'][:,1,:] # rotation\n",
    "            Qs = Qs.unsqueeze(1).repeat((1,2,1))\n",
    "            Qs = torch.cat((torch.ones((self.B*self.L,2,1),device=Qs.device),Qs),dim=-1).reshape(self.B,self.L,2,4)\n",
    "            Qs = normQ(Qs)\n",
    "            Rs = Qs2Rs(Qs)\n",
    "            N_C_to_Rot = torch.cat((noised_dict['bb_noised']['N_CA'].reshape(self.B, self.L, 3).to(self.device),\n",
    "                                    noised_dict['bb_noised']['C_CA'].reshape(self.B, self.L, 3).to(self.device)),\n",
    "                                   dim=2).reshape(self.B,self.L,2,1,3)\n",
    "\n",
    "\n",
    "            rot_vecs = einsum('bnkij,bnkhj->bnki',Rs, N_C_to_Rot)\n",
    "            NC_p = CA_p + rot_vecs[:,:,0,:].to(self.device)*self.N_CA_dist\n",
    "            CC_p = CA_p + rot_vecs[:,:,1,:].reshape(self.B, self.L, 3).to(self.device)*self.C_CA_dist\n",
    "\n",
    "            pred = torch.cat((NC_p,CA_p,CC_p),dim=2).reshape(self.B,self.L,3,3)\n",
    "            \n",
    "            fp = convert_pV_to_points(noised_dict, 1, key_in='bb_firstp')\n",
    "            lp = convert_pV_to_points(noised_dict,1,  key_in='bb_lastp')                                             \n",
    "            real_mask = noised_dict['real_nodes_mask'].to(self.device)\n",
    "            score_scales = noised_dict['score_scales'].to(self.device)\n",
    "\n",
    "            nf_pred = out['0']\n",
    "            real_nodes_pred = torch.round(nf_pred).clamp(0,1)\n",
    "            real_nodes_pred_mask = (real_nodes_pred.squeeze().sum(-1)>self.real_threshold).reshape(self.B,self.L)\n",
    "            \n",
    "            lr, lr_d = FAPE_loss_real(pred, true, score_scales, real_mask,  d_clamp=10.0, d_clamp_inter=30.0,\n",
    "                           A=10.0, gamma=1.0, eps=1e-6)\n",
    "\n",
    "            ln, ln_d = FAPE_loss_null(pred, fp, lp, real_mask, score_scales,  d_clamp=10.0,\n",
    "                               d_clamp_inter=30.0, A=10.0, gamma=1.0, eps=1e-6)\n",
    "\n",
    "            ln = ln*self.score_weights['3D_null']\n",
    "            lr = lr*self.score_weights['3D_real']\n",
    "\n",
    "            structure_loss = lr + ln\n",
    "\n",
    "            #score for node feats determining whether node is real or fake\n",
    "            nf_pred = out['0']\n",
    "\n",
    "            nf_feat_dim = noised_dict['real_nodes_noise'].shape[-1]\n",
    "            nf_true = torch.ones(noised_dict['real_nodes_mask'].shape+(nf_feat_dim,) + (1,),\n",
    "                                 dtype=torch.float,device = self.device)\n",
    "\n",
    "            nf_real_mask_mult = real_mask.unsqueeze(-1).unsqueeze(-1).to(self.device)\n",
    "            nf_true = nf_true*nf_real_mask_mult\n",
    "\n",
    "            nf_pred = nf_pred.reshape(self.B,-1,nf_feat_dim)\n",
    "            pred_nf_loss = torch.sum(torch.square(nf_true.squeeze()-nf_pred),dim=-1)\n",
    "\n",
    "            ss_scales = to_cuda(noised_dict['score_scales'])[:,None,None]\n",
    "            pnfloss = (torch.sum((pred_nf_loss*ss_scales))/(self.L*self.nf_dim))*self.score_weights['nf_real']\n",
    "\n",
    "            final_loss = structure_loss + pnfloss\n",
    "            val_loss = {'pnf_loss': pnfloss.detach().cpu(),\n",
    "                        'structure_loss':structure_loss.detach().cpu(),\n",
    "                        'structure_null':ln.detach().cpu(),\n",
    "                        'structure_real':lr.detach().cpu()}\n",
    "\n",
    "        real_nodes_true_mask = noised_dict['real_nodes_mask']\n",
    "        \n",
    "        del nf_real_mask_mult\n",
    "        del ss_scales\n",
    "        del real_mask\n",
    "        del score_scales\n",
    "        del batched_t\n",
    "        del structure_loss\n",
    "        del pnfloss\n",
    "        del nf_pred\n",
    "        del nf_true\n",
    "        del N_C_to_Rot\n",
    "        del CA_n, NC_n, CC_n\n",
    "        \n",
    "        for k,v in out.items():\n",
    "            del v\n",
    "        for k,v in feat_dict.items():\n",
    "            del v\n",
    "        \n",
    "        #needs to be rolled to N-terminal = 0 for pymol output, add coord_scale\n",
    "        return true.detach().cpu(), noise_xyz.detach().cpu(), pred.detach().cpu() , real_nodes_pred_mask.detach().cpu(), real_nodes_true_mask.detach().cpu(), val_loss\n",
    "    \n",
    "    def eval_fn(self, valid_loader, eval_dir, epoch=0, input_t=None, max_cycles=10):\n",
    "        \n",
    "        train_feats = next(iter(valid_loader))\n",
    "        \n",
    "        if input_t is None:\n",
    "            #visualize_T\n",
    "            vis_t = np.array([0.01,0.05,0.1,0.2,0.3,0.5,0.8,1.0])\n",
    "            vis_t = vis_t[None,...].repeat(int(np.ceil(self.B/len(vis_t))),axis=0).flatten()[:self.B]\n",
    "        elif type(input_t) == float:\n",
    "            vis_t = np.ones((self.B,))*input_t\n",
    "        else:\n",
    "            vis_t = input_t\n",
    "\n",
    "        noised_dict = self.fnd.forward(train_feats, t_vec=vis_t)\n",
    "        batched_t = to_cuda( noised_dict['t_vec'] )\n",
    "\n",
    "        true, noise_xyz, pred, real_nodes_pred_mask, real_nodes_true_mask, val_loss = self.eval_model(noised_dict, batched_t)\n",
    "        util.pdb_writer.dump_tnp_null(true, noise_xyz, pred, vis_t, e=epoch, \n",
    "                      numOut=len(vis_t), real_mask=real_nodes_true_mask, \n",
    "                      pred_mask=real_nodes_pred_mask.detach().cpu(), outdir=eval_dir)\n",
    "        \n",
    "        log_lossses = defaultdict(list)\n",
    "        losskeeper = []\n",
    "        eval_steps = 0\n",
    "        \n",
    "        \n",
    "        for i,train_feats in enumerate(valid_loader):\n",
    "            noised_dict = self.fnd.forward(train_feats)\n",
    "            batched_t = to_cuda( noised_dict['t_vec'] )\n",
    "            true, noise_xyz, pred, real_nodes_pred_mask, real_nodes_true_mask, val_loss = self.eval_model(noised_dict, batched_t)\n",
    "            eval_steps += 1\n",
    "            lsum = 0 \n",
    "            for k,v in val_loss.items():\n",
    "                log_lossses[k].append(np.array(v))\n",
    "                lsum+=v.detach().cpu()\n",
    "            losskeeper.append(lsum)   \n",
    "            \n",
    "            del true\n",
    "            del noise_xyz\n",
    "            del pred\n",
    "            del real_nodes_pred_mask\n",
    "            for k,v in val_loss.items():\n",
    "                del v\n",
    "            del batched_t\n",
    "            if i>max_cycles:\n",
    "                break\n",
    "            \n",
    "            \n",
    "        # Logging to terminal\n",
    "        rolling_losses = tree.map_structure(np.mean, log_lossses)\n",
    "        loss_log = ' '.join([\n",
    "            f'{k}={v[0]:.4f}'\n",
    "            for k,v in rolling_losses.items() if 'batch' not in k\n",
    "        ])\n",
    "\n",
    "        log_lossses = defaultdict(list)\n",
    "        print(f'[{eval_steps}]: {loss_log}')\n",
    "        print('eval_loss',np.mean(losskeeper[-1000:]),len(losskeeper))\n",
    "        \n",
    "        for k,v in noised_dict.items():\n",
    "            del v\n",
    "    \n",
    "    \n",
    "    \n",
    "    def loss_fn(self, bb_dict, t_val=None):\n",
    "        \n",
    "        def convert_pV_to_points(dict_in, L, key_in='bb_firstp', return_indiv=False):\n",
    "            \"\"\"Concatenates to xyz from Calpha+atom vectors\"\"\"\n",
    "            CA_fp  = dict_in[key_in]['CA'].reshape(self.B, L, 3).to(self.device)\n",
    "            NC_fp = CA_fp + dict_in[key_in]['N_CA'].reshape(self.B, L, 3).to(self.device)\n",
    "            CC_fp = CA_fp + dict_in[key_in]['C_CA'].reshape(self.B, L, 3).to(self.device)\n",
    "            fp =  torch.cat((NC_fp,CA_fp,CC_fp),dim=2).reshape(self.B,L,3,3)\n",
    "            if return_indiv:\n",
    "                return fp, CA_fp, NC_fp, CC_fp\n",
    "            return fp\n",
    "        \n",
    "        if t_val is not None:\n",
    "            noised_dict = self.fnd.forward(bb_dict, t_vec=t_val)\n",
    "        else:\n",
    "            #generates with random t\n",
    "            noised_dict = self.fnd.forward(bb_dict)\n",
    "        \n",
    "        batched_t = noised_dict['t_vec'].to(self.device)\n",
    "        \n",
    "        #not converted to distance by multiplying by bond distance, seems to work better\n",
    "        true =  convert_pV_to_points(noised_dict, self.L, key_in='bb_shifted')\n",
    "        noise_xyz, CA_n, NC_n, CC_n =   convert_pV_to_points(noised_dict, self.L, key_in='bb_noised', return_indiv=True)\n",
    "                                                                                                            \n",
    "        #prepare graphs\n",
    "        feat_dict = self.mkg.prep_for_network(noised_dict, cuda=True)\n",
    "        out  = self._model(feat_dict,batched_t)\n",
    "                                                                    \n",
    "        #FAPE Loss for the prediction\n",
    "        CA_p = out['1'][:,0,:].reshape(self.B, self.L, 3) + CA_n #translation of Calpha\n",
    "        Qs = out['1'][:,1,:] # rotation , convert from x,y,z (Quat) to rotate input vectors\n",
    "        Qs = Qs.unsqueeze(1).repeat((1,2,1))\n",
    "        Qs = torch.cat((torch.ones((self.B*self.L,2,1), device=Qs.device),Qs),dim=-1).reshape(self.B, self.L, 2, 4)\n",
    "        Qs = normQ(Qs)\n",
    "        Rs = Qs2Rs(Qs)\n",
    "        N_C_to_Rot = torch.cat((noised_dict['bb_noised']['N_CA'].reshape(self.B, self.L, 3).to(self.device),\n",
    "                                noised_dict['bb_noised']['C_CA'].reshape(self.B, self.L, 3).to(self.device)),dim=2).reshape(self.B,self.L,2,1,3)\n",
    "        rot_vecs = einsum('bnkij,bnkhj->bnki',Rs, N_C_to_Rot)\n",
    "        NC_p = CA_p + rot_vecs[:,:,0,:]*self.N_CA_dist #comparable but seems better not have it for true, but have it for pred\n",
    "        CC_p = CA_p + rot_vecs[:,:,1,:]*self.C_CA_dist #maybe this helep prevent \n",
    "\n",
    "        pred = torch.cat((NC_p,CA_p,CC_p),dim=2).reshape(self.B,self.L,3,3)\n",
    "                                                                \n",
    "                                                                                                                     \n",
    "        fp = convert_pV_to_points(noised_dict, 1, key_in='bb_firstp')\n",
    "        lp = convert_pV_to_points(noised_dict,1,  key_in='bb_lastp')                                             \n",
    "        real_mask = noised_dict['real_nodes_mask'].to(self.device)\n",
    "        score_scales = noised_dict['score_scales'].to(self.device)\n",
    "\n",
    "        lr, lr_d = FAPE_loss_real(pred, true, score_scales, real_mask,  d_clamp=10.0, d_clamp_inter=30.0,\n",
    "                       A=10.0, gamma=1.0, eps=1e-6)\n",
    "\n",
    "        ln, ln_d = FAPE_loss_null(pred, fp, lp, real_mask, score_scales,  d_clamp=10.0,\n",
    "                           d_clamp_inter=30.0, A=10.0, gamma=1.0, eps=1e-6)\n",
    "        \n",
    "        ln = ln*self.score_weights['3D_null']\n",
    "        lr = lr*self.score_weights['3D_real']\n",
    "\n",
    "        structure_loss = lr + ln\n",
    "\n",
    "        #score for node feats determining whether node is real or fake\n",
    "        nf_pred = out['0']\n",
    "\n",
    "        nf_feat_dim = noised_dict['real_nodes_noise'].shape[-1]\n",
    "        nf_true = torch.ones(noised_dict['real_nodes_mask'].shape+(nf_feat_dim,) + (1,),\n",
    "                             dtype=torch.float,device = self.device)\n",
    "\n",
    "        nf_real_mask_mult = real_mask.unsqueeze(-1).unsqueeze(-1).to(self.device)\n",
    "        nf_true = nf_true*nf_real_mask_mult\n",
    "\n",
    "        nf_pred = nf_pred.reshape(self.B,-1,nf_feat_dim)\n",
    "        pred_nf_loss = torch.sum(torch.square(nf_true.squeeze()-nf_pred),dim=(-2,-1))\n",
    "\n",
    "        ss_scales = score_scales[:,None,None]\n",
    "        pnfloss = (torch.sum((pred_nf_loss*ss_scales))/(self.L*self.nf_dim))*self.score_weights['nf_real']\n",
    "\n",
    "        final_loss = structure_loss + pnfloss\n",
    "        \n",
    "        aux_loss = {'pnf_loss':pnfloss.detach().cpu(),\n",
    "                    'structure_loss':structure_loss.detach().cpu(),\n",
    "                    'structure_null':ln.detach().cpu(),\n",
    "                    'structure_real':lr.detach().cpu()}\n",
    "        \n",
    "        del nf_real_mask_mult\n",
    "        del ss_scales\n",
    "        del real_mask\n",
    "        del score_scales\n",
    "        del batched_t\n",
    "        del structure_loss\n",
    "        del pnfloss\n",
    "        del nf_pred\n",
    "        del nf_true\n",
    "        del N_C_to_Rot\n",
    "        del noise_xyz, CA_n, NC_n, CC_n\n",
    "        \n",
    "        for k,v in out.items():\n",
    "            del v\n",
    "            \n",
    "        for k,v in feat_dict.items():\n",
    "            del v\n",
    "                                                                \n",
    "        return final_loss, aux_loss\n",
    "                                                                \n",
    "                                                                \n",
    "            \n",
    "                                                                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e36e2432",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = {'batch_size'  : 8,\n",
    "                          'topk'  : 8,\n",
    "                        'stride'  : 8,\n",
    "                            'KNN' : 30,\n",
    "                      'num_heads' : 16,\n",
    "                       'channels' : 32,\n",
    "                   'channels_div' : 8,\n",
    "                     'num_layers' : 1,\n",
    "                 'num_layers_ca'  : 2,\n",
    "               'edge_feature_dim' : 1,\n",
    "              'latent_pool_type'  : 'max',\n",
    "                        't_size'  : 12,\n",
    "                         'max_t'  : 1.0,\n",
    "                           'mult' : 2,\n",
    "                       'zero_lin' : True,\n",
    "                      'use_tdeg1' : False,\n",
    "                       'roll': False,\n",
    "                       'circ_pe':False,\n",
    "                            'cuda': True,\n",
    "                  'learning rate' : 0.0005,\n",
    "                   'weight_decay' : 0.00001,\n",
    "                    'sc_nf_real'  : 0.01 ,\n",
    "                    'sc_3D_real'  : 1.0 ,\n",
    "                    'sc_3D_null'  : 0.01 ,\n",
    "                    'device'      : 'cuda',\n",
    "                    'num_epoch'   : 150,\n",
    "                    'log_freq'    : 1000,\n",
    "                    'ckpt_freq'   : 10000,\n",
    "                    'early_chkpt' : 2,\n",
    "                    'coord_scale' : 10.0,\n",
    "                    'nf_threshold_real': 1.99,\n",
    "                    'nf_dim': 5}\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c3b6d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = Experiment(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090ab830",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "number of epochs 150\n",
      "epoch 0\n",
      "mem_used 165536256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nwoodall/miniconda3/envs/se33/lib/python3.9/site-packages/dgl/backend/pytorch/tensor.py:449: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]: pnf_loss=0.2334 structure_loss=2.1985 structure_null=0.0131 structure_real=2.1854, steps/sec=231.89594\n",
      "2.431854486465454\n",
      "eval Eval_Direc/gu_null_23D_04M_2024Y/23D_04M_2024Y_00h_19m_35s/step_2\n",
      "[12]: pnf_loss=1.1253 structure_loss=4.1753 structure_null=0.0128 structure_real=4.1625\n",
      "eval_loss 4.659372 12\n",
      "epoch 1\n",
      "mem_used 246537216\n",
      "[1000]: pnf_loss=0.1374 structure_loss=1.6194 structure_null=0.0107 structure_real=1.6087, steps/sec=4.69075\n",
      "1.9961769263590536\n",
      "epoch 2\n",
      "mem_used 246729216\n",
      "epoch 3\n",
      "mem_used 247350272\n",
      "[2000]: pnf_loss=0.1808 structure_loss=1.6081 structure_null=0.0101 structure_real=1.5979, steps/sec=4.03289\n",
      "2.0152524761084853\n",
      "epoch 4\n",
      "mem_used 246572032\n",
      "[3000]: pnf_loss=0.1606 structure_loss=1.5310 structure_null=0.0099 structure_real=1.5211, steps/sec=0.96798\n",
      "1.9966975101193443\n",
      "epoch 5\n",
      "mem_used 247011328\n",
      "epoch 6\n",
      "mem_used 246680576\n",
      "[4000]: pnf_loss=0.1417 structure_loss=1.3586 structure_null=0.0088 structure_real=1.3498, steps/sec=2.03136\n",
      "1.736142652815786\n",
      "epoch 7\n",
      "mem_used 246649344\n",
      "[5000]: pnf_loss=0.2305 structure_loss=2.2914 structure_null=0.0106 structure_real=2.2808, steps/sec=0.78097\n",
      "2.6476369113322127\n",
      "epoch 8\n",
      "mem_used 246868992\n",
      "epoch 9\n",
      "mem_used 246596608\n",
      "[6000]: pnf_loss=0.2639 structure_loss=3.2988 structure_null=0.0104 structure_real=3.2884, steps/sec=1.35555\n",
      "2.6414181949763464\n",
      "epoch 10\n",
      "mem_used 246764544\n",
      "epoch 11\n",
      "mem_used 247042560\n",
      "[7000]: pnf_loss=0.5212 structure_loss=1.8376 structure_null=0.0091 structure_real=1.8285, steps/sec=5.11487\n",
      "1.7610442716142405\n",
      "epoch 12\n",
      "mem_used 246667264\n",
      "[8000]: pnf_loss=0.1548 structure_loss=1.3439 structure_null=0.0092 structure_real=1.3347, steps/sec=1.01703\n",
      "1.7219488181430718\n",
      "epoch 13\n",
      "mem_used 246818816\n",
      "epoch 14\n",
      "mem_used 246738944\n",
      "[9000]: pnf_loss=0.1402 structure_loss=1.5489 structure_null=0.0091 structure_real=1.5399, steps/sec=2.26722\n",
      "1.6654083207249641\n",
      "epoch 15\n",
      "mem_used 247010816\n",
      "[10000]: pnf_loss=0.1188 structure_loss=1.2008 structure_null=0.0082 structure_real=1.1926, steps/sec=0.81185\n",
      "1.7236165774279628\n",
      "eval Eval_Direc/gu_null_23D_04M_2024Y/23D_04M_2024Y_00h_19m_35s/step_10000\n",
      "[12]: pnf_loss=0.2707 structure_loss=2.0529 structure_null=0.0106 structure_real=2.0423\n",
      "eval_loss 3.5091884 12\n",
      "epoch 16\n",
      "mem_used 246571520\n",
      "epoch 17\n",
      "mem_used 246869504\n",
      "[11000]: pnf_loss=0.1431 structure_loss=1.3291 structure_null=0.0084 structure_real=1.3206, steps/sec=1.45476\n",
      "1.710583057300544\n",
      "epoch 18\n",
      "mem_used 247785472\n",
      "epoch 19\n",
      "mem_used 246625280\n",
      "[12000]: pnf_loss=0.1491 structure_loss=1.2997 structure_null=0.0081 structure_real=1.2916, steps/sec=6.87290\n",
      "1.6469077152364395\n",
      "epoch 20\n",
      "mem_used 246750208\n",
      "[13000]: pnf_loss=0.1474 structure_loss=1.1925 structure_null=0.0081 structure_real=1.1844, steps/sec=1.06294\n",
      "1.6852547504685143\n",
      "epoch 21\n",
      "mem_used 246595072\n",
      "epoch 22\n",
      "mem_used 246602752\n",
      "[14000]: pnf_loss=0.1519 structure_loss=1.3743 structure_null=0.0088 structure_real=1.3655, steps/sec=2.57204\n",
      "1.6501282576633536\n",
      "epoch 23\n",
      "mem_used 246658560\n",
      "[15000]: pnf_loss=0.1237 structure_loss=1.2485 structure_null=0.0093 structure_real=1.2393, steps/sec=0.84731\n",
      "1.7071688781968124\n",
      "epoch 24\n",
      "mem_used 247955456\n",
      "epoch 25\n",
      "mem_used 246590464\n",
      "[16000]: pnf_loss=0.1291 structure_loss=1.1509 structure_null=0.0087 structure_real=1.1423, steps/sec=1.57327\n",
      "1.6891532762845358\n",
      "epoch 26\n",
      "mem_used 246744576\n",
      "epoch 27\n",
      "mem_used 247477760\n",
      "[17000]: pnf_loss=0.1149 structure_loss=1.2128 structure_null=0.0080 structure_real=1.2049, steps/sec=10.59691\n",
      "1.6313440582968972\n",
      "epoch 28\n",
      "mem_used 246642688\n",
      "[18000]: pnf_loss=0.1163 structure_loss=1.2393 structure_null=0.0080 structure_real=1.2313, steps/sec=1.13570\n",
      "1.6467167995870113\n",
      "epoch 29\n",
      "mem_used 246588416\n",
      "epoch 30\n",
      "mem_used 246892032\n",
      "[19000]: pnf_loss=0.1173 structure_loss=1.1735 structure_null=0.0070 structure_real=1.1665, steps/sec=2.97773\n",
      "1.6171551555395127\n",
      "epoch 31\n",
      "mem_used 247189504\n",
      "[20000]: pnf_loss=0.0756 structure_loss=1.3874 structure_null=0.0082 structure_real=1.3792, steps/sec=0.88704\n",
      "1.6024095944892196\n",
      "eval Eval_Direc/gu_null_23D_04M_2024Y/23D_04M_2024Y_00h_19m_35s/step_20000\n",
      "[12]: pnf_loss=0.1213 structure_loss=1.2617 structure_null=0.0075 structure_real=1.2542\n",
      "eval_loss 2.788495 12\n",
      "epoch 32\n",
      "mem_used 246886400\n",
      "epoch 33\n",
      "mem_used 246610944\n",
      "[21000]: pnf_loss=0.2547 structure_loss=1.5059 structure_null=0.0081 structure_real=1.4978, steps/sec=1.71148\n",
      "1.7175783463146375\n",
      "epoch 34\n",
      "mem_used 246856704\n",
      "epoch 35\n",
      "mem_used 246585856\n",
      "[22000]: pnf_loss=0.2040 structure_loss=1.4644 structure_null=0.0090 structure_real=1.4554, steps/sec=23.75408\n",
      "1.7072729766368866\n",
      "epoch 36\n",
      "mem_used 246601216\n",
      "[23000]: pnf_loss=0.2112 structure_loss=1.2888 structure_null=0.0088 structure_real=1.2800, steps/sec=1.20460\n",
      "1.6887213691156737\n",
      "epoch 37\n",
      "mem_used 246995968\n",
      "epoch 38\n",
      "mem_used 246626816\n",
      "[24000]: pnf_loss=0.1321 structure_loss=1.4126 structure_null=0.0087 structure_real=1.4040, steps/sec=3.45695\n",
      "1.6658336038098616\n",
      "epoch 39\n",
      "mem_used 246669824\n",
      "[25000]: pnf_loss=0.1365 structure_loss=1.2412 structure_null=0.0083 structure_real=1.2329, steps/sec=0.93120\n",
      "1.625914912054858\n",
      "epoch 40\n",
      "mem_used 247581696\n",
      "epoch 41\n",
      "mem_used 246874624\n",
      "[26000]: pnf_loss=0.1039 structure_loss=1.4500 structure_null=0.0083 structure_real=1.4417, steps/sec=1.87385\n",
      "1.5839450070782313\n",
      "epoch 42\n",
      "mem_used 246710784\n",
      "[27000]: pnf_loss=0.0957 structure_loss=1.2463 structure_null=0.0093 structure_real=1.2370, steps/sec=0.75689\n",
      "1.6418797710002997\n",
      "epoch 43\n",
      "mem_used 246608896\n",
      "epoch 44\n",
      "mem_used 247817728\n",
      "[28000]: pnf_loss=0.0991 structure_loss=1.6936 structure_null=0.0087 structure_real=1.6849, steps/sec=1.28850\n",
      "1.6141641023366347\n",
      "epoch 45\n",
      "mem_used 246695936\n",
      "epoch 46\n",
      "mem_used 247064064\n",
      "[29000]: pnf_loss=0.1772 structure_loss=1.3276 structure_null=0.0089 structure_real=1.3187, steps/sec=4.19893\n",
      "1.8230038934520312\n",
      "epoch 47\n",
      "mem_used 246734336\n",
      "[30000]: pnf_loss=0.7509 structure_loss=2.2265 structure_null=0.0098 structure_real=2.2166, steps/sec=0.97854\n",
      "1.6409786341111523\n",
      "eval Eval_Direc/gu_null_23D_04M_2024Y/23D_04M_2024Y_00h_19m_35s/step_30000\n",
      "[12]: pnf_loss=0.1343 structure_loss=1.2163 structure_null=0.0076 structure_real=1.2087\n",
      "eval_loss 3.0047417 12\n",
      "epoch 48\n",
      "mem_used 246776320\n",
      "epoch 49\n",
      "mem_used 246915072\n",
      "[31000]: pnf_loss=0.1905 structure_loss=1.7086 structure_null=0.0101 structure_real=1.6985, steps/sec=2.07801\n",
      "1.7341971303287305\n",
      "epoch 50\n",
      "mem_used 247585280\n",
      "[32000]: pnf_loss=0.1462 structure_loss=1.5219 structure_null=0.0086 structure_real=1.5133, steps/sec=0.79099\n",
      "1.6854402709007263\n",
      "epoch 51\n",
      "mem_used 246753280\n",
      "epoch 52\n",
      "mem_used 246587904\n",
      "[33000]: pnf_loss=0.1758 structure_loss=1.5528 structure_null=0.0084 structure_real=1.5444, steps/sec=1.37700\n",
      "1.5844069826741551\n",
      "epoch 53\n",
      "mem_used 246537216\n",
      "epoch 54\n",
      "mem_used 246624768\n",
      "[34000]: pnf_loss=0.1570 structure_loss=1.4496 structure_null=0.0084 structure_real=1.4412, steps/sec=5.31327\n",
      "1.7017502134496516\n"
     ]
    }
   ],
   "source": [
    "exp.start_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41c427c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_file = 'GUN_checkpoints/gu_null/19D_04M_2024Y_07h_30m_59s/step_60000.pth'\n",
    "ckpt_model=torch.load(checkpoint_file)['model']\n",
    "ckpt_opt = torch.load(checkpoint_file)['optimizer']\n",
    "exp = Experiment(conf=conf,ckpt_model=ckpt_model,ckpt_opt=ckpt_opt, cur_step=60000, cur_epoch=48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87a90204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "number of epochs 150\n",
      "epoch 48\n",
      "mem_used 267491328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nwoodall/miniconda3/envs/se33/lib/python3.9/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
      "/home/nwoodall/miniconda3/envs/se33/lib/python3.9/site-packages/dgl/backend/pytorch/tensor.py:449: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[61000]: pnf_loss=0.1956 structure_loss=0.9004 structure_null=0.0228 structure_real=0.8776, steps/sec=1.64994\n",
      "0.9825538541078568\n",
      "epoch 49\n",
      "mem_used 331020288\n",
      "[62000]: pnf_loss=0.4056 structure_loss=0.9947 structure_null=0.0198 structure_real=0.9749, steps/sec=2.28232\n",
      "1.0069489349908174\n",
      "epoch 50\n",
      "mem_used 330826752\n",
      "[63000]: pnf_loss=0.1467 structure_loss=0.6151 structure_null=0.0198 structure_real=0.5953, steps/sec=3.49967\n",
      "0.9644192707391432\n",
      "epoch 51\n",
      "mem_used 330617344\n",
      "[64000]: pnf_loss=0.3153 structure_loss=0.9217 structure_null=0.0186 structure_real=0.9031, steps/sec=7.57891\n",
      "0.9955984182232852\n",
      "[65000]: pnf_loss=0.1352 structure_loss=0.6007 structure_null=0.0192 structure_real=0.5815, steps/sec=1.71850\n",
      "0.9735476514697075\n",
      "epoch 52\n",
      "mem_used 330702848\n",
      "[66000]: pnf_loss=0.1800 structure_loss=0.6139 structure_null=0.0208 structure_real=0.5932, steps/sec=1.80559\n",
      "0.9618979032024925\n",
      "epoch 53\n",
      "mem_used 329815040\n",
      "[67000]: pnf_loss=0.1154 structure_loss=0.6444 structure_null=0.0216 structure_real=0.6228, steps/sec=2.40638\n",
      "0.9672414454546842\n",
      "epoch 54\n",
      "mem_used 330760192\n",
      "[68000]: pnf_loss=0.1045 structure_loss=0.6040 structure_null=0.0206 structure_real=0.5834, steps/sec=3.78098\n",
      "0.9653996786175857\n",
      "epoch 55\n",
      "mem_used 329548800\n",
      "[69000]: pnf_loss=0.8987 structure_loss=1.1884 structure_null=0.0172 structure_real=1.1711, steps/sec=8.52154\n",
      "1.0312837759653728\n",
      "[70000]: pnf_loss=0.3845 structure_loss=0.8172 structure_null=0.0199 structure_real=0.7973, steps/sec=1.71060\n",
      "0.9292049199938774\n",
      "eval Eval_Direc/gu_null_19D_04M_2024Y/19D_04M_2024Y_16h_53m_42s/step_70000\n",
      "[12]: pnf_loss=0.1326 structure_loss=0.6944 structure_null=0.0202 structure_real=0.6742\n",
      "eval_loss 1.8463439 12\n",
      "epoch 56\n",
      "mem_used 330702336\n",
      "[71000]: pnf_loss=0.1616 structure_loss=0.6091 structure_null=0.0197 structure_real=0.5895, steps/sec=1.80481\n",
      "0.9291331049496845\n",
      "epoch 57\n",
      "mem_used 330230784\n",
      "[72000]: pnf_loss=0.4158 structure_loss=0.8196 structure_null=0.0225 structure_real=0.7971, steps/sec=2.47431\n",
      "0.9535320909609788\n",
      "epoch 58\n",
      "mem_used 329396736\n",
      "[73000]: pnf_loss=0.3905 structure_loss=0.8958 structure_null=0.0201 structure_real=0.8757, steps/sec=3.94688\n",
      "0.9493953370770742\n",
      "epoch 59\n",
      "mem_used 329929728\n",
      "[74000]: pnf_loss=0.1324 structure_loss=0.5760 structure_null=0.0190 structure_real=0.5570, steps/sec=9.80391\n",
      "0.8885508003951497\n",
      "[75000]: pnf_loss=0.1746 structure_loss=0.8471 structure_null=0.0188 structure_real=0.8283, steps/sec=1.70376\n",
      "0.9444314975738526\n",
      "epoch 60\n",
      "mem_used 331273728\n",
      "[76000]: pnf_loss=0.1487 structure_loss=0.7529 structure_null=0.0167 structure_real=0.7362, steps/sec=1.85880\n",
      "0.9388523427159505\n",
      "epoch 61\n",
      "mem_used 331624960\n",
      "[77000]: pnf_loss=0.1386 structure_loss=0.9521 structure_null=0.0210 structure_real=0.9311, steps/sec=2.59502\n",
      "0.9545329687989716\n",
      "epoch 62\n",
      "mem_used 330806784\n",
      "[78000]: pnf_loss=0.1205 structure_loss=0.6297 structure_null=0.0261 structure_real=0.6036, steps/sec=4.24716\n",
      "0.9304256479241955\n",
      "epoch 63\n",
      "mem_used 329986560\n",
      "[79000]: pnf_loss=0.1739 structure_loss=0.7080 structure_null=0.0197 structure_real=0.6883, steps/sec=11.69884\n",
      "0.888525640964508\n",
      "[80000]: pnf_loss=0.1252 structure_loss=0.6623 structure_null=0.0220 structure_real=0.6403, steps/sec=1.71509\n",
      "0.9636407897472382\n",
      "eval Eval_Direc/gu_null_19D_04M_2024Y/19D_04M_2024Y_16h_53m_42s/step_80000\n",
      "[12]: pnf_loss=0.6397 structure_loss=1.0010 structure_null=0.0237 structure_real=0.9773\n",
      "eval_loss 1.926918 12\n",
      "epoch 64\n",
      "mem_used 330018816\n",
      "[81000]: pnf_loss=0.1091 structure_loss=0.7408 structure_null=0.0194 structure_real=0.7214, steps/sec=1.91730\n",
      "0.9382553569771148\n",
      "epoch 65\n",
      "mem_used 330805248\n",
      "[82000]: pnf_loss=0.0827 structure_loss=0.8786 structure_null=0.0184 structure_real=0.8603, steps/sec=2.69908\n",
      "0.9134601748499742\n",
      "epoch 66\n",
      "mem_used 331371520\n",
      "[83000]: pnf_loss=0.2666 structure_loss=0.7730 structure_null=0.0226 structure_real=0.7504, steps/sec=4.54910\n",
      "0.8952886350652113\n",
      "epoch 67\n",
      "mem_used 329076736\n",
      "[84000]: pnf_loss=0.1717 structure_loss=0.5988 structure_null=0.0170 structure_real=0.5818, steps/sec=15.14039\n",
      "0.9338872865733937\n",
      "[85000]: pnf_loss=0.1130 structure_loss=0.6682 structure_null=0.0221 structure_real=0.6461, steps/sec=1.70911\n",
      "0.9589439025521278\n",
      "epoch 68\n",
      "mem_used 331299840\n",
      "[86000]: pnf_loss=0.1487 structure_loss=0.8000 structure_null=0.0221 structure_real=0.7779, steps/sec=1.98391\n",
      "0.9425435769003491\n",
      "epoch 69\n",
      "mem_used 331325440\n",
      "[87000]: pnf_loss=0.5008 structure_loss=0.8778 structure_null=0.0194 structure_real=0.8584, steps/sec=2.82387\n",
      "0.9554270511835962\n",
      "epoch 70\n",
      "mem_used 331295744\n",
      "[88000]: pnf_loss=0.1975 structure_loss=0.5720 structure_null=0.0166 structure_real=0.5553, steps/sec=4.93679\n",
      "0.9810638396726178\n",
      "epoch 71\n",
      "mem_used 330815488\n",
      "[89000]: pnf_loss=0.1969 structure_loss=0.7494 structure_null=0.0218 structure_real=0.7276, steps/sec=18.99120\n",
      "0.8831704721022188\n",
      "[90000]: pnf_loss=0.0862 structure_loss=0.8837 structure_null=0.0149 structure_real=0.8688, steps/sec=1.71582\n",
      "0.9073652747273445\n",
      "eval Eval_Direc/gu_null_19D_04M_2024Y/19D_04M_2024Y_16h_53m_42s/step_90000\n",
      "[12]: pnf_loss=0.1335 structure_loss=0.5953 structure_null=0.0211 structure_real=0.5742\n",
      "eval_loss 1.5250119 12\n",
      "epoch 72\n",
      "mem_used 329273856\n",
      "[91000]: pnf_loss=0.2087 structure_loss=0.9226 structure_null=0.0183 structure_real=0.9043, steps/sec=2.04760\n",
      "0.9577787560768999\n",
      "epoch 73\n",
      "mem_used 331134464\n",
      "[92000]: pnf_loss=0.1319 structure_loss=0.6237 structure_null=0.0209 structure_real=0.6029, steps/sec=2.96070\n",
      "0.9350250671220862\n",
      "epoch 74\n",
      "mem_used 329384960\n",
      "[93000]: pnf_loss=0.1534 structure_loss=0.6478 structure_null=0.0179 structure_real=0.6299, steps/sec=5.36137\n",
      "0.9790117761249062\n",
      "epoch 75\n",
      "mem_used 329659392\n",
      "[94000]: pnf_loss=0.9691 structure_loss=1.4603 structure_null=0.0218 structure_real=1.4384, steps/sec=28.02743\n",
      "0.9659770863955138\n",
      "[95000]: pnf_loss=0.1764 structure_loss=0.7352 structure_null=0.0189 structure_real=0.7163, steps/sec=1.70660\n",
      "0.9639277455806732\n",
      "epoch 76\n",
      "mem_used 330438144\n",
      "[96000]: pnf_loss=0.0848 structure_loss=0.6333 structure_null=0.0196 structure_real=0.6137, steps/sec=2.12377\n",
      "0.9772210099655597\n",
      "epoch 77\n",
      "mem_used 331329536\n",
      "[97000]: pnf_loss=0.1439 structure_loss=0.6216 structure_null=0.0217 structure_real=0.5999, steps/sec=3.12211\n",
      "0.9387593383762911\n",
      "epoch 78\n",
      "mem_used 331236352\n",
      "[98000]: pnf_loss=0.1420 structure_loss=0.5841 structure_null=0.0175 structure_real=0.5666, steps/sec=5.86325\n",
      "0.9181666674285099\n",
      "epoch 79\n",
      "mem_used 329569792\n",
      "[99000]: pnf_loss=0.2726 structure_loss=0.7134 structure_null=0.0199 structure_real=0.6935, steps/sec=51.40801\n",
      "1.06620340275042\n",
      "[100000]: pnf_loss=0.1256 structure_loss=0.5692 structure_null=0.0219 structure_real=0.5473, steps/sec=1.70355\n",
      "0.9470884717702865\n",
      "eval Eval_Direc/gu_null_19D_04M_2024Y/19D_04M_2024Y_16h_53m_42s/step_100000\n",
      "[12]: pnf_loss=0.1069 structure_loss=0.6872 structure_null=0.0164 structure_real=0.6707\n",
      "eval_loss 1.5283586 12\n",
      "epoch 80\n",
      "mem_used 331346944\n",
      "[101000]: pnf_loss=0.0642 structure_loss=0.7274 structure_null=0.0187 structure_real=0.7086, steps/sec=2.20081\n",
      "0.9258057243891598\n",
      "epoch 81\n",
      "mem_used 330805760\n",
      "[102000]: pnf_loss=0.1423 structure_loss=0.8522 structure_null=0.0183 structure_real=0.8339, steps/sec=3.28600\n",
      "0.9524099457011287\n",
      "epoch 82\n",
      "mem_used 331291136\n",
      "[103000]: pnf_loss=0.1178 structure_loss=0.7889 structure_null=0.0169 structure_real=0.7720, steps/sec=6.49875\n",
      "0.9587212345982326\n",
      "epoch 83\n",
      "mem_used 329218560\n",
      "[104000]: pnf_loss=0.4043 structure_loss=0.7440 structure_null=0.0208 structure_real=0.7231, steps/sec=331.69682\n",
      "0.9127493023872375\n",
      "[105000]: pnf_loss=0.1036 structure_loss=0.8074 structure_null=0.0180 structure_real=0.7894, steps/sec=1.71672\n",
      "0.9299057607650757\n",
      "epoch 84\n",
      "mem_used 331265024\n",
      "[106000]: pnf_loss=0.1018 structure_loss=0.7377 structure_null=0.0204 structure_real=0.7174, steps/sec=2.29653\n",
      "0.9117481328427472\n",
      "epoch 85\n",
      "mem_used 331277824\n",
      "[107000]: pnf_loss=0.2308 structure_loss=1.0193 structure_null=0.0186 structure_real=1.0007, steps/sec=3.48232\n",
      "0.9898269515173498\n",
      "epoch 86\n",
      "mem_used 329218560\n",
      "[108000]: pnf_loss=0.1181 structure_loss=0.6570 structure_null=0.0198 structure_real=0.6373, steps/sec=7.24334\n",
      "0.9147964687938364\n",
      "[109000]: pnf_loss=0.0503 structure_loss=1.1802 structure_null=0.0206 structure_real=1.1596, steps/sec=1.71150\n",
      "0.9215808463096619\n",
      "epoch 87\n",
      "mem_used 329216000\n",
      "[110000]: pnf_loss=0.0814 structure_loss=0.9680 structure_null=0.0165 structure_real=0.9515, steps/sec=1.74672\n",
      "0.9330658314782009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval Eval_Direc/gu_null_19D_04M_2024Y/19D_04M_2024Y_16h_53m_42s/step_110000\n",
      "[12]: pnf_loss=0.5565 structure_loss=1.2600 structure_null=0.0180 structure_real=1.2420\n",
      "eval_loss 1.6500773 12\n",
      "epoch 88\n",
      "mem_used 330872832\n",
      "[111000]: pnf_loss=0.7722 structure_loss=1.3276 structure_null=0.0192 structure_real=1.3084, steps/sec=2.37682\n",
      "0.9163335034416782\n",
      "epoch 89\n",
      "mem_used 329273856\n",
      "[112000]: pnf_loss=0.1243 structure_loss=0.5997 structure_null=0.0159 structure_real=0.5838, steps/sec=3.66559\n",
      "0.9357024462125214\n",
      "epoch 90\n",
      "mem_used 330151424\n",
      "[113000]: pnf_loss=0.1254 structure_loss=0.6721 structure_null=0.0162 structure_real=0.6559, steps/sec=8.27949\n",
      "0.8692646434584868\n",
      "[114000]: pnf_loss=0.1724 structure_loss=0.5755 structure_null=0.0189 structure_real=0.5566, steps/sec=1.71032\n",
      "0.9595632363557816\n",
      "epoch 91\n",
      "mem_used 331344896\n",
      "[115000]: pnf_loss=0.1708 structure_loss=0.7164 structure_null=0.0147 structure_real=0.7017, steps/sec=1.79486\n",
      "0.9437804973288758\n",
      "epoch 92\n",
      "mem_used 330775040\n",
      "[116000]: pnf_loss=0.0894 structure_loss=0.8579 structure_null=0.0198 structure_real=0.8380, steps/sec=2.47699\n",
      "0.8971318916950611\n",
      "epoch 93\n",
      "mem_used 330238464\n",
      "[117000]: pnf_loss=0.1354 structure_loss=0.5762 structure_null=0.0154 structure_real=0.5607, steps/sec=3.95752\n",
      "0.8962376615096783\n",
      "epoch 94\n",
      "mem_used 329567744\n",
      "[118000]: pnf_loss=0.1277 structure_loss=0.5906 structure_null=0.0177 structure_real=0.5729, steps/sec=9.72352\n",
      "0.9417685521452615\n",
      "[119000]: pnf_loss=0.1119 structure_loss=0.8117 structure_null=0.0183 structure_real=0.7934, steps/sec=1.71443\n",
      "0.9226345108747482\n",
      "epoch 95\n",
      "mem_used 330344960\n",
      "[120000]: pnf_loss=0.1838 structure_loss=0.7032 structure_null=0.0169 structure_real=0.6862, steps/sec=1.87194\n",
      "0.916969514242601\n",
      "eval Eval_Direc/gu_null_19D_04M_2024Y/19D_04M_2024Y_16h_53m_42s/step_120000\n",
      "[12]: pnf_loss=0.0837 structure_loss=0.6134 structure_null=0.0164 structure_real=0.5970\n",
      "eval_loss 1.6820774 12\n",
      "epoch 96\n",
      "mem_used 330893312\n",
      "[121000]: pnf_loss=0.1032 structure_loss=0.5937 structure_null=0.0162 structure_real=0.5775, steps/sec=2.58420\n",
      "0.9338558575654604\n",
      "epoch 97\n",
      "mem_used 329205760\n",
      "[122000]: pnf_loss=0.5132 structure_loss=1.2845 structure_null=0.0186 structure_real=1.2659, steps/sec=4.18539\n",
      "0.9007567706213536\n",
      "epoch 98\n",
      "mem_used 331042816\n",
      "[123000]: pnf_loss=0.1349 structure_loss=0.7251 structure_null=0.0213 structure_real=0.7038, steps/sec=11.44050\n",
      "0.9703744820753734\n",
      "[124000]: pnf_loss=0.1879 structure_loss=0.7181 structure_null=0.0183 structure_real=0.6998, steps/sec=1.70904\n",
      "0.9158601760864258\n",
      "epoch 99\n",
      "mem_used 330776576\n",
      "[125000]: pnf_loss=0.1830 structure_loss=0.7452 structure_null=0.0153 structure_real=0.7299, steps/sec=1.92498\n",
      "0.9430643380822954\n",
      "epoch 100\n",
      "mem_used 330744320\n",
      "[126000]: pnf_loss=0.1179 structure_loss=0.6654 structure_null=0.0169 structure_real=0.6485, steps/sec=2.69654\n",
      "0.9286732632409102\n",
      "epoch 101\n",
      "mem_used 330757120\n",
      "[127000]: pnf_loss=0.1829 structure_loss=0.8949 structure_null=0.0176 structure_real=0.8773, steps/sec=4.53184\n",
      "0.8849850834831399\n",
      "epoch 102\n",
      "mem_used 330734592\n",
      "[128000]: pnf_loss=0.1404 structure_loss=0.6254 structure_null=0.0183 structure_real=0.6071, steps/sec=13.90958\n",
      "0.9750595126972824\n",
      "[129000]: pnf_loss=0.7999 structure_loss=1.3296 structure_null=0.0168 structure_real=1.3129, steps/sec=1.70792\n",
      "0.918226825594902\n",
      "epoch 103\n",
      "mem_used 331365376\n",
      "[130000]: pnf_loss=0.0466 structure_loss=0.7820 structure_null=0.0165 structure_real=0.7655, steps/sec=1.97071\n",
      "0.9258398038803497\n",
      "eval Eval_Direc/gu_null_19D_04M_2024Y/19D_04M_2024Y_16h_53m_42s/step_130000\n",
      "[12]: pnf_loss=0.1226 structure_loss=0.6123 structure_null=0.0168 structure_real=0.5955\n",
      "eval_loss 1.6527716 12\n",
      "epoch 104\n",
      "mem_used 330860544\n",
      "[131000]: pnf_loss=0.1392 structure_loss=0.6554 structure_null=0.0164 structure_real=0.6390, steps/sec=2.85350\n",
      "0.9255517325118968\n",
      "epoch 105\n",
      "mem_used 330214912\n",
      "[132000]: pnf_loss=0.1672 structure_loss=0.6628 structure_null=0.0170 structure_real=0.6458, steps/sec=4.89545\n",
      "0.9303427092370145\n",
      "epoch 106\n",
      "mem_used 331385344\n",
      "[133000]: pnf_loss=0.1178 structure_loss=0.7518 structure_null=0.0185 structure_real=0.7333, steps/sec=18.18541\n",
      "0.8572839352678745\n",
      "[134000]: pnf_loss=0.1047 structure_loss=0.7122 structure_null=0.0181 structure_real=0.6941, steps/sec=1.71532\n",
      "0.9408589125275612\n",
      "epoch 107\n",
      "mem_used 329371136\n",
      "[135000]: pnf_loss=0.2060 structure_loss=0.7179 structure_null=0.0217 structure_real=0.6962, steps/sec=2.04165\n",
      "0.9286809312257573\n",
      "epoch 108\n",
      "mem_used 330851328\n",
      "[136000]: pnf_loss=0.0964 structure_loss=0.8159 structure_null=0.0204 structure_real=0.7955, steps/sec=2.94071\n",
      "0.912841932013117\n",
      "epoch 109\n",
      "mem_used 329546752\n",
      "[137000]: pnf_loss=0.0735 structure_loss=0.7183 structure_null=0.0157 structure_real=0.7027, steps/sec=5.28515\n",
      "0.970132148856349\n",
      "epoch 110\n",
      "mem_used 331085824\n",
      "[138000]: pnf_loss=0.1150 structure_loss=0.5861 structure_null=0.0196 structure_real=0.5665, steps/sec=25.87788\n",
      "0.9245766618035056\n",
      "[139000]: pnf_loss=0.0616 structure_loss=0.7423 structure_null=0.0169 structure_real=0.7254, steps/sec=1.72132\n",
      "0.9122146116495132\n",
      "epoch 111\n",
      "mem_used 329717248\n",
      "[140000]: pnf_loss=0.8114 structure_loss=1.1276 structure_null=0.0211 structure_real=1.1065, steps/sec=2.13009\n",
      "0.9157541844399809\n",
      "eval Eval_Direc/gu_null_19D_04M_2024Y/19D_04M_2024Y_16h_53m_42s/step_140000\n",
      "[12]: pnf_loss=0.0928 structure_loss=0.7908 structure_null=0.0201 structure_real=0.7707\n",
      "eval_loss 1.5267926 12\n",
      "epoch 112\n",
      "mem_used 332306432\n",
      "[141000]: pnf_loss=0.1593 structure_loss=0.6044 structure_null=0.0160 structure_real=0.5884, steps/sec=3.13841\n",
      "0.9853606341757636\n",
      "epoch 113\n",
      "mem_used 332107264\n",
      "[142000]: pnf_loss=0.1698 structure_loss=0.5760 structure_null=0.0166 structure_real=0.5595, steps/sec=5.77919\n",
      "0.8888608746609445\n",
      "epoch 114\n",
      "mem_used 332029440\n",
      "[143000]: pnf_loss=0.1331 structure_loss=0.5704 structure_null=0.0184 structure_real=0.5519, steps/sec=45.06966\n",
      "0.9675242195003911\n",
      "[144000]: pnf_loss=0.1154 structure_loss=0.5706 structure_null=0.0144 structure_real=0.5562, steps/sec=1.71641\n",
      "0.935654280424118\n",
      "epoch 115\n",
      "mem_used 329830912\n",
      "[145000]: pnf_loss=0.1134 structure_loss=0.6629 structure_null=0.0190 structure_real=0.6439, steps/sec=2.21385\n",
      "0.9521968834836718\n",
      "epoch 116\n",
      "mem_used 331015168\n",
      "[146000]: pnf_loss=0.1155 structure_loss=0.6977 structure_null=0.0177 structure_real=0.6800, steps/sec=3.26241\n",
      "0.9340496803740509\n",
      "epoch 117\n",
      "mem_used 330193920\n",
      "[147000]: pnf_loss=0.1159 structure_loss=0.6299 structure_null=0.0206 structure_real=0.6093, steps/sec=6.35180\n",
      "0.9172292695509807\n",
      "epoch 118\n",
      "mem_used 330064896\n",
      "[148000]: pnf_loss=0.1623 structure_loss=0.8334 structure_null=0.0202 structure_real=0.8133, steps/sec=171.35848\n",
      "1.029397004842758\n",
      "[149000]: pnf_loss=0.2185 structure_loss=0.7562 structure_null=0.0230 structure_real=0.7332, steps/sec=1.70800\n",
      "0.9030008864998817\n",
      "epoch 119\n",
      "mem_used 331343360\n",
      "[150000]: pnf_loss=0.1686 structure_loss=0.6838 structure_null=0.0170 structure_real=0.6668, steps/sec=2.26679\n",
      "0.9006424490832395\n",
      "eval Eval_Direc/gu_null_19D_04M_2024Y/19D_04M_2024Y_16h_53m_42s/step_150000\n",
      "[12]: pnf_loss=0.1725 structure_loss=0.7201 structure_null=0.0201 structure_real=0.7000\n",
      "eval_loss 1.6926055 12\n",
      "epoch 120\n",
      "mem_used 329936384\n",
      "[151000]: pnf_loss=0.1591 structure_loss=0.6560 structure_null=0.0165 structure_real=0.6396, steps/sec=3.42807\n",
      "0.9319358100333521\n",
      "epoch 121\n",
      "mem_used 331043328\n",
      "[152000]: pnf_loss=0.1606 structure_loss=0.7349 structure_null=0.0141 structure_real=0.7208, steps/sec=7.13095\n",
      "0.9358008758293535\n",
      "[153000]: pnf_loss=0.8644 structure_loss=1.0283 structure_null=0.0174 structure_real=1.0109, steps/sec=1.71180\n",
      "0.9354375455975532\n",
      "epoch 122\n",
      "mem_used 330260480\n",
      "[154000]: pnf_loss=0.1134 structure_loss=0.5931 structure_null=0.0161 structure_real=0.5770, steps/sec=1.73525\n",
      "0.9344650162820175\n",
      "epoch 123\n",
      "mem_used 332139520\n",
      "[155000]: pnf_loss=0.0863 structure_loss=0.6852 structure_null=0.0169 structure_real=0.6682, steps/sec=2.35242\n",
      "0.9487564096779659\n",
      "epoch 124\n",
      "mem_used 330073600\n",
      "[156000]: pnf_loss=0.1040 structure_loss=0.7096 structure_null=0.0188 structure_real=0.6908, steps/sec=3.64879\n",
      "0.9275947590159555\n",
      "epoch 125\n",
      "mem_used 330241536\n",
      "[157000]: pnf_loss=0.1140 structure_loss=0.6616 structure_null=0.0166 structure_real=0.6450, steps/sec=8.06273\n",
      "0.9595486273132794\n",
      "[158000]: pnf_loss=0.1382 structure_loss=0.7488 structure_null=0.0177 structure_real=0.7311, steps/sec=1.70492\n",
      "0.9454600065946579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 126\n",
      "mem_used 329558528\n",
      "[159000]: pnf_loss=0.1510 structure_loss=0.5768 structure_null=0.0179 structure_real=0.5590, steps/sec=1.78695\n",
      "0.9196542344003353\n",
      "epoch 127\n",
      "mem_used 330198528\n",
      "[160000]: pnf_loss=0.1318 structure_loss=0.6955 structure_null=0.0144 structure_real=0.6811, steps/sec=2.45132\n",
      "0.8882737097302331\n",
      "eval Eval_Direc/gu_null_19D_04M_2024Y/19D_04M_2024Y_16h_53m_42s/step_160000\n",
      "[12]: pnf_loss=0.1197 structure_loss=0.6033 structure_null=0.0162 structure_real=0.5871\n",
      "eval_loss 1.4699801 12\n",
      "epoch 128\n",
      "mem_used 331072512\n",
      "[161000]: pnf_loss=0.1643 structure_loss=0.6837 structure_null=0.0165 structure_real=0.6672, steps/sec=3.87203\n",
      "0.90690370852297\n",
      "epoch 129\n",
      "mem_used 331176448\n",
      "[162000]: pnf_loss=0.1912 structure_loss=0.5634 structure_null=0.0168 structure_real=0.5466, steps/sec=9.31148\n",
      "0.8945066622697591\n",
      "[163000]: pnf_loss=0.1235 structure_loss=0.8972 structure_null=0.0160 structure_real=0.8813, steps/sec=1.70355\n",
      "0.910186319053173\n",
      "epoch 130\n",
      "mem_used 329777664\n",
      "[164000]: pnf_loss=0.1632 structure_loss=0.8023 structure_null=0.0176 structure_real=0.7847, steps/sec=1.84595\n",
      "0.9394392414304145\n",
      "epoch 131\n",
      "mem_used 329953280\n",
      "[165000]: pnf_loss=0.1380 structure_loss=0.6337 structure_null=0.0199 structure_real=0.6138, steps/sec=2.56359\n",
      "0.9121784581197216\n",
      "epoch 132\n",
      "mem_used 330164736\n",
      "[166000]: pnf_loss=0.1186 structure_loss=0.6078 structure_null=0.0166 structure_real=0.5913, steps/sec=4.14372\n",
      "0.9042542950042243\n",
      "epoch 133\n",
      "mem_used 330487296\n",
      "[167000]: pnf_loss=0.2379 structure_loss=0.9399 structure_null=0.0175 structure_real=0.9224, steps/sec=10.97180\n",
      "0.9245588206475781\n",
      "[168000]: pnf_loss=0.4163 structure_loss=0.8474 structure_null=0.0202 structure_real=0.8272, steps/sec=1.70683\n",
      "0.9665101541280746\n",
      "epoch 134\n",
      "mem_used 329292800\n",
      "[169000]: pnf_loss=0.1897 structure_loss=0.6936 structure_null=0.0178 structure_real=0.6758, steps/sec=1.90728\n",
      "0.9326492864597082\n",
      "epoch 135\n",
      "mem_used 331579392\n",
      "[170000]: pnf_loss=0.1241 structure_loss=0.5689 structure_null=0.0171 structure_real=0.5517, steps/sec=2.65733\n",
      "0.9101925032001204\n",
      "eval Eval_Direc/gu_null_19D_04M_2024Y/19D_04M_2024Y_16h_53m_42s/step_170000\n",
      "[12]: pnf_loss=0.1672 structure_loss=0.8074 structure_null=0.0196 structure_real=0.7878\n",
      "eval_loss 1.9146799 12\n",
      "epoch 136\n",
      "mem_used 331077120\n",
      "[171000]: pnf_loss=0.1140 structure_loss=0.6254 structure_null=0.0134 structure_real=0.6120, steps/sec=4.48767\n",
      "0.910177991570284\n",
      "epoch 137\n",
      "mem_used 330125312\n",
      "[172000]: pnf_loss=0.1509 structure_loss=0.6116 structure_null=0.0165 structure_real=0.5950, steps/sec=13.47211\n",
      "0.9125173495510431\n",
      "[173000]: pnf_loss=0.1398 structure_loss=0.5913 structure_null=0.0183 structure_real=0.5731, steps/sec=1.71788\n",
      "0.9004328867793083\n",
      "epoch 138\n",
      "mem_used 330681856\n",
      "[174000]: pnf_loss=0.2389 structure_loss=0.8450 structure_null=0.0202 structure_real=0.8248, steps/sec=1.96752\n",
      "0.9598974743793751\n",
      "epoch 139\n",
      "mem_used 329649152\n",
      "[175000]: pnf_loss=0.0773 structure_loss=0.8645 structure_null=0.0161 structure_real=0.8483, steps/sec=2.78624\n",
      "0.8926905195242429\n",
      "epoch 140\n",
      "mem_used 331412480\n",
      "[176000]: pnf_loss=0.1466 structure_loss=0.5773 structure_null=0.0123 structure_real=0.5649, steps/sec=4.78568\n",
      "0.9653286267532392\n",
      "epoch 141\n",
      "mem_used 329349632\n",
      "[177000]: pnf_loss=0.2901 structure_loss=0.9752 structure_null=0.0184 structure_real=0.9568, steps/sec=17.07574\n",
      "0.9171385397814741\n",
      "[178000]: pnf_loss=0.0726 structure_loss=0.7288 structure_null=0.0181 structure_real=0.7108, steps/sec=1.70457\n",
      "0.926595508992672\n",
      "epoch 142\n",
      "mem_used 330670080\n",
      "[179000]: pnf_loss=0.1341 structure_loss=0.5688 structure_null=0.0210 structure_real=0.5478, steps/sec=2.02964\n",
      "0.9042838854348009\n",
      "epoch 143\n",
      "mem_used 330705920\n",
      "[180000]: pnf_loss=0.0952 structure_loss=0.6814 structure_null=0.0175 structure_real=0.6639, steps/sec=2.96827\n",
      "0.9265116461321838\n",
      "eval Eval_Direc/gu_null_19D_04M_2024Y/19D_04M_2024Y_16h_53m_42s/step_180000\n",
      "[12]: pnf_loss=0.1487 structure_loss=0.6094 structure_null=0.0182 structure_real=0.5912\n",
      "eval_loss 1.579311 12\n",
      "epoch 144\n",
      "mem_used 331782144\n",
      "[181000]: pnf_loss=0.1220 structure_loss=0.5754 structure_null=0.0186 structure_real=0.5568, steps/sec=5.20758\n",
      "0.8933897547242118\n",
      "epoch 145\n",
      "mem_used 330982912\n",
      "[182000]: pnf_loss=0.0585 structure_loss=0.7913 structure_null=0.0163 structure_real=0.7751, steps/sec=23.92783\n",
      "0.9595939340725751\n",
      "[183000]: pnf_loss=0.1509 structure_loss=0.5613 structure_null=0.0132 structure_real=0.5481, steps/sec=1.72117\n",
      "0.9271876654624939\n",
      "epoch 146\n",
      "mem_used 331035136\n",
      "[184000]: pnf_loss=0.1323 structure_loss=0.7434 structure_null=0.0144 structure_real=0.7290, steps/sec=2.11919\n",
      "0.8922119818682752\n",
      "epoch 147\n",
      "mem_used 329334784\n",
      "[185000]: pnf_loss=0.0954 structure_loss=0.6494 structure_null=0.0176 structure_real=0.6318, steps/sec=3.07231\n",
      "0.9068062774154827\n",
      "epoch 148\n",
      "mem_used 330212352\n",
      "[186000]: pnf_loss=0.1465 structure_loss=0.5757 structure_null=0.0157 structure_real=0.5600, steps/sec=5.66358\n",
      "1.091222499012947\n",
      "epoch 149\n",
      "mem_used 330117632\n",
      "[187000]: pnf_loss=0.2260 structure_loss=0.7882 structure_null=0.0184 structure_real=0.7698, steps/sec=39.80637\n",
      "1.0344271479650986\n",
      "[188000]: pnf_loss=0.1569 structure_loss=0.6746 structure_null=0.0178 structure_real=0.6568, steps/sec=1.71467\n",
      "0.9267907098531724\n",
      "epoch 150\n",
      "mem_used 330238464\n",
      "[189000]: pnf_loss=0.2732 structure_loss=0.7712 structure_null=0.0169 structure_real=0.7543, steps/sec=2.16781\n",
      "0.8997198980273181\n",
      "epoch 151\n",
      "mem_used 331499520\n",
      "[190000]: pnf_loss=0.1067 structure_loss=0.7467 structure_null=0.0230 structure_real=0.7237, steps/sec=3.21383\n",
      "0.9142057477639167\n",
      "eval Eval_Direc/gu_null_19D_04M_2024Y/19D_04M_2024Y_16h_53m_42s/step_190000\n",
      "[12]: pnf_loss=0.1771 structure_loss=0.7405 structure_null=0.0149 structure_real=0.7256\n",
      "eval_loss 1.4928846 12\n",
      "epoch 152\n",
      "mem_used 331742720\n",
      "[191000]: pnf_loss=0.1194 structure_loss=0.6512 structure_null=0.0208 structure_real=0.6303, steps/sec=6.25987\n",
      "0.9339587346595877\n",
      "epoch 153\n",
      "mem_used 329357312\n",
      "[192000]: pnf_loss=0.1073 structure_loss=0.7043 structure_null=0.0174 structure_real=0.6869, steps/sec=113.12141\n",
      "1.1311262766520183\n",
      "[193000]: pnf_loss=0.1277 structure_loss=0.7322 structure_null=0.0174 structure_real=0.7148, steps/sec=1.70639\n",
      "0.9714822704792023\n",
      "epoch 154\n",
      "mem_used 331006976\n",
      "[194000]: pnf_loss=0.1255 structure_loss=0.7009 structure_null=0.0143 structure_real=0.6866, steps/sec=2.26225\n",
      "0.9171956144568159\n",
      "epoch 155\n",
      "mem_used 330778112\n",
      "[195000]: pnf_loss=0.0986 structure_loss=0.6189 structure_null=0.0146 structure_real=0.6043, steps/sec=3.44629\n",
      "0.9291149482517661\n",
      "epoch 156\n",
      "mem_used 331839488\n",
      "[196000]: pnf_loss=0.1492 structure_loss=0.5935 structure_null=0.0141 structure_real=0.5794, steps/sec=7.11370\n",
      "0.8980517160208499\n",
      "[197000]: pnf_loss=0.1531 structure_loss=0.6946 structure_null=0.0139 structure_real=0.6807, steps/sec=1.72386\n",
      "0.9191250346302986\n",
      "epoch 157\n",
      "mem_used 330075136\n",
      "[198000]: pnf_loss=0.1368 structure_loss=0.7550 structure_null=0.0173 structure_real=0.7376, steps/sec=1.74862\n",
      "0.9181328700306205\n",
      "epoch 158\n",
      "mem_used 330814976\n",
      "[199000]: pnf_loss=0.0806 structure_loss=0.7102 structure_null=0.0168 structure_real=0.6934, steps/sec=2.35723\n",
      "0.941973593300336\n",
      "epoch 159\n",
      "mem_used 329628672\n",
      "[200000]: pnf_loss=0.1705 structure_loss=0.6352 structure_null=0.0138 structure_real=0.6214, steps/sec=3.64257\n",
      "0.9024747703342558\n",
      "eval Eval_Direc/gu_null_19D_04M_2024Y/19D_04M_2024Y_16h_53m_42s/step_200000\n",
      "[12]: pnf_loss=0.0903 structure_loss=0.5994 structure_null=0.0150 structure_real=0.5844\n",
      "eval_loss 1.4207178 12\n",
      "epoch 160\n",
      "mem_used 330628608\n",
      "[201000]: pnf_loss=0.0847 structure_loss=0.6627 structure_null=0.0154 structure_real=0.6473, steps/sec=8.08847\n",
      "0.9110970121842844\n",
      "[202000]: pnf_loss=0.1345 structure_loss=0.5914 structure_null=0.0187 structure_real=0.5727, steps/sec=1.71307\n",
      "0.9077575593590737\n",
      "epoch 161\n",
      "mem_used 332144640\n",
      "[203000]: pnf_loss=0.1125 structure_loss=0.5826 structure_null=0.0172 structure_real=0.5655, steps/sec=1.78561\n",
      "0.9324544694923385\n",
      "epoch 162\n",
      "mem_used 331366912\n",
      "[204000]: pnf_loss=0.1377 structure_loss=0.5549 structure_null=0.0147 structure_real=0.5402, steps/sec=2.45084\n",
      "0.9578671487647923\n",
      "epoch 163\n",
      "mem_used 331068416\n",
      "[205000]: pnf_loss=0.1506 structure_loss=0.6097 structure_null=0.0184 structure_real=0.5913, steps/sec=3.85080\n",
      "0.8802832327531964\n",
      "epoch 164\n",
      "mem_used 330856448\n",
      "[206000]: pnf_loss=0.1464 structure_loss=0.7389 structure_null=0.0147 structure_real=0.7243, steps/sec=9.02195\n",
      "0.9622415002990277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[207000]: pnf_loss=0.0847 structure_loss=0.6782 structure_null=0.0151 structure_real=0.6630, steps/sec=1.71437\n",
      "0.8789839829802513\n",
      "epoch 165\n",
      "mem_used 330194432\n",
      "[208000]: pnf_loss=0.1064 structure_loss=0.7946 structure_null=0.0155 structure_real=0.7791, steps/sec=1.83494\n",
      "0.8694860761070866\n",
      "epoch 166\n",
      "mem_used 330590720\n",
      "[209000]: pnf_loss=0.1795 structure_loss=0.5682 structure_null=0.0174 structure_real=0.5508, steps/sec=2.52612\n",
      "0.968441602738038\n",
      "epoch 167\n",
      "mem_used 330554368\n",
      "[210000]: pnf_loss=0.1129 structure_loss=0.5694 structure_null=0.0157 structure_real=0.5537, steps/sec=4.08247\n",
      "0.8986959042880747\n",
      "eval Eval_Direc/gu_null_19D_04M_2024Y/19D_04M_2024Y_16h_53m_42s/step_210000\n",
      "[12]: pnf_loss=0.0643 structure_loss=0.6868 structure_null=0.0138 structure_real=0.6730\n",
      "eval_loss 1.7079157 12\n",
      "epoch 168\n",
      "mem_used 330467328\n",
      "[211000]: pnf_loss=0.0699 structure_loss=0.7755 structure_null=0.0183 structure_real=0.7572, steps/sec=10.65605\n",
      "0.8895366594195366\n",
      "[212000]: pnf_loss=0.1704 structure_loss=0.9285 structure_null=0.0138 structure_real=0.9146, steps/sec=1.70619\n",
      "0.890863720536232\n",
      "epoch 169\n",
      "mem_used 331041792\n",
      "[213000]: pnf_loss=0.1187 structure_loss=0.9096 structure_null=0.0172 structure_real=0.8923, steps/sec=1.88974\n",
      "0.9059325086978055\n",
      "epoch 170\n",
      "mem_used 330964480\n",
      "[214000]: pnf_loss=0.1061 structure_loss=0.5568 structure_null=0.0136 structure_real=0.5432, steps/sec=2.64830\n",
      "0.902583381893465\n",
      "epoch 171\n",
      "mem_used 330766336\n",
      "[215000]: pnf_loss=0.0986 structure_loss=0.5878 structure_null=0.0153 structure_real=0.5725, steps/sec=4.37599\n",
      "0.9232500797678695\n",
      "epoch 172\n",
      "mem_used 329589248\n",
      "[216000]: pnf_loss=0.1276 structure_loss=0.7289 structure_null=0.0140 structure_real=0.7149, steps/sec=12.90401\n",
      "0.8852352554147894\n",
      "[217000]: pnf_loss=0.1047 structure_loss=0.6149 structure_null=0.0128 structure_real=0.6022, steps/sec=1.70310\n",
      "0.9669217395186425\n",
      "epoch 173\n",
      "mem_used 329948160\n",
      "[218000]: pnf_loss=0.0439 structure_loss=0.6816 structure_null=0.0209 structure_real=0.6606, steps/sec=1.94730\n",
      "0.895022761957986\n",
      "epoch 174\n",
      "mem_used 330384384\n",
      "[219000]: pnf_loss=0.1581 structure_loss=0.7363 structure_null=0.0149 structure_real=0.7215, steps/sec=2.76267\n",
      "0.9057668908514251\n",
      "epoch 175\n",
      "mem_used 331427840\n",
      "[220000]: pnf_loss=0.1328 structure_loss=0.7964 structure_null=0.0133 structure_real=0.7831, steps/sec=4.73706\n",
      "0.8867248309946456\n",
      "eval Eval_Direc/gu_null_19D_04M_2024Y/19D_04M_2024Y_16h_53m_42s/step_220000\n",
      "[12]: pnf_loss=0.1085 structure_loss=0.7759 structure_null=0.0162 structure_real=0.7598\n",
      "eval_loss 1.6874825 12\n",
      "epoch 176\n",
      "mem_used 331313152\n",
      "[221000]: pnf_loss=0.1006 structure_loss=0.6259 structure_null=0.0125 structure_real=0.6134, steps/sec=16.34770\n",
      "0.8580749309979953\n",
      "[222000]: pnf_loss=0.1924 structure_loss=0.7535 structure_null=0.0153 structure_real=0.7382, steps/sec=1.72039\n",
      "0.8745989010930061\n",
      "epoch 177\n",
      "mem_used 330479104\n",
      "[223000]: pnf_loss=0.2788 structure_loss=0.7255 structure_null=0.0182 structure_real=0.7073, steps/sec=2.00855\n",
      "0.9136619684687753\n",
      "epoch 178\n",
      "mem_used 331351552\n",
      "[224000]: pnf_loss=0.1445 structure_loss=0.5635 structure_null=0.0173 structure_real=0.5462, steps/sec=2.88581\n",
      "0.8905950595766811\n",
      "epoch 179\n",
      "mem_used 330642432\n",
      "[225000]: pnf_loss=0.1338 structure_loss=0.5692 structure_null=0.0129 structure_real=0.5563, steps/sec=5.11329\n",
      "0.8695474995507134\n",
      "epoch 180\n",
      "mem_used 331040256\n",
      "[226000]: pnf_loss=0.0885 structure_loss=0.7491 structure_null=0.0181 structure_real=0.7309, steps/sec=22.37874\n",
      "0.9826409220695496\n",
      "[227000]: pnf_loss=0.3673 structure_loss=0.7823 structure_null=0.0171 structure_real=0.7653, steps/sec=1.71151\n",
      "0.8706333832740784\n",
      "epoch 181\n",
      "mem_used 330870272\n",
      "[228000]: pnf_loss=0.9452 structure_loss=1.4295 structure_null=0.0187 structure_real=1.4108, steps/sec=2.09656\n",
      "0.8776599569780632\n",
      "epoch 182\n",
      "mem_used 331404288\n",
      "[229000]: pnf_loss=0.1799 structure_loss=0.5667 structure_null=0.0185 structure_real=0.5483, steps/sec=3.03004\n",
      "0.8861179841795002\n",
      "epoch 183\n",
      "mem_used 331041280\n",
      "[230000]: pnf_loss=0.0909 structure_loss=0.5948 structure_null=0.0169 structure_real=0.5779, steps/sec=5.51860\n",
      "0.9853441756279742\n",
      "eval Eval_Direc/gu_null_19D_04M_2024Y/19D_04M_2024Y_16h_53m_42s/step_230000\n",
      "[12]: pnf_loss=0.1058 structure_loss=0.6123 structure_null=0.0146 structure_real=0.5977\n",
      "eval_loss 1.5075488 12\n",
      "epoch 184\n",
      "mem_used 331436032\n",
      "[231000]: pnf_loss=0.1013 structure_loss=0.6032 structure_null=0.0134 structure_real=0.5898, steps/sec=36.41586\n",
      "0.8911135581632456\n",
      "[232000]: pnf_loss=0.0743 structure_loss=0.6951 structure_null=0.0147 structure_real=0.6804, steps/sec=1.70828\n",
      "0.8898943865299225\n",
      "epoch 185\n",
      "mem_used 332458496\n",
      "[233000]: pnf_loss=0.0852 structure_loss=0.8118 structure_null=0.0159 structure_real=0.7959, steps/sec=2.15593\n",
      "0.9196685624182902\n",
      "epoch 186\n",
      "mem_used 331068416\n",
      "[234000]: pnf_loss=0.1142 structure_loss=0.6266 structure_null=0.0141 structure_real=0.6125, steps/sec=3.20647\n",
      "0.8895524049296362\n",
      "epoch 187\n",
      "mem_used 330809856\n",
      "[235000]: pnf_loss=0.2111 structure_loss=0.7488 structure_null=0.0162 structure_real=0.7326, steps/sec=6.13194\n",
      "0.8709501736000557\n",
      "epoch 188\n",
      "mem_used 330312192\n",
      "[236000]: pnf_loss=0.1303 structure_loss=0.5592 structure_null=0.0131 structure_real=0.5461, steps/sec=85.29047\n",
      "0.8651096135377884\n",
      "[237000]: pnf_loss=0.6958 structure_loss=1.0932 structure_null=0.0159 structure_real=1.0773, steps/sec=1.70711\n",
      "0.906579659640789\n",
      "epoch 189\n",
      "mem_used 330013696\n",
      "[238000]: pnf_loss=0.1056 structure_loss=0.5894 structure_null=0.0172 structure_real=0.5721, steps/sec=2.25001\n",
      "0.8711673855625317\n",
      "epoch 190\n",
      "mem_used 329274368\n",
      "[239000]: pnf_loss=0.0782 structure_loss=0.6713 structure_null=0.0157 structure_real=0.6556, steps/sec=3.39605\n",
      "0.8712904695936814\n",
      "epoch 191\n",
      "mem_used 331043840\n",
      "[240000]: pnf_loss=0.0713 structure_loss=0.5968 structure_null=0.0143 structure_real=0.5825, steps/sec=6.84063\n",
      "0.9099337197690603\n",
      "eval Eval_Direc/gu_null_19D_04M_2024Y/19D_04M_2024Y_16h_53m_42s/step_240000\n",
      "[12]: pnf_loss=0.1191 structure_loss=0.6335 structure_null=0.0133 structure_real=0.6202\n",
      "eval_loss 1.8178004 12\n",
      "[241000]: pnf_loss=0.1339 structure_loss=0.7056 structure_null=0.0111 structure_real=0.6946, steps/sec=1.69717\n",
      "0.8834137030243874\n",
      "epoch 192\n",
      "mem_used 331215360\n",
      "[242000]: pnf_loss=0.1214 structure_loss=0.6165 structure_null=0.0151 structure_real=0.6015, steps/sec=1.72148\n",
      "0.8703997419365952\n",
      "epoch 193\n",
      "mem_used 331417600\n",
      "[243000]: pnf_loss=0.0772 structure_loss=0.6947 structure_null=0.0184 structure_real=0.6763, steps/sec=2.31476\n",
      "0.8961405143445852\n",
      "epoch 194\n",
      "mem_used 330864640\n",
      "[244000]: pnf_loss=0.1103 structure_loss=0.6234 structure_null=0.0171 structure_real=0.6063, steps/sec=3.57458\n",
      "0.90139132750583\n",
      "epoch 195\n",
      "mem_used 329994752\n",
      "[245000]: pnf_loss=0.2319 structure_loss=0.7209 structure_null=0.0160 structure_real=0.7049, steps/sec=7.73193\n",
      "0.890039451251742\n",
      "[246000]: pnf_loss=0.0936 structure_loss=0.5651 structure_null=0.0158 structure_real=0.5493, steps/sec=1.71906\n",
      "0.8588274312019348\n",
      "epoch 196\n",
      "mem_used 330600448\n",
      "[247000]: pnf_loss=0.1493 structure_loss=0.7280 structure_null=0.0199 structure_real=0.7081, steps/sec=1.79146\n",
      "0.8880020219260726\n",
      "epoch 197\n",
      "mem_used 330523648\n",
      "[248000]: pnf_loss=0.1125 structure_loss=0.7017 structure_null=0.0171 structure_real=0.6846, steps/sec=2.42672\n",
      "0.8820123975988493\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp.start_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff6bb6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab2f3e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_file = 'GUN_checkpoints/gu_null/03D_04M_2024Y_09h_16m_56s/step_80000.pth'\n",
    "# checkpoint_file = 'GUN_checkpoints/gu_null/04D_04M_2024Y_01h_02m_24s/step_190000.pth'\n",
    "# ckpt_model=torch.load(checkpoint_file)['model']\n",
    "# ckpt_opt = torch.load(checkpoint_file)['optimizer']\n",
    "# exp = Experiment(conf=conf,ckpt_model=ckpt_model,ckpt_opt=ckpt_opt, cur_step=80000, cur_epoch=63)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19108401",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exp.start_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1816e0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = {'batch_size'  : 4,\n",
    "                          'topk'  : 4,\n",
    "                        'stride'  : 8,\n",
    "                            'KNN' : 30,\n",
    "                      'num_heads' : 16,\n",
    "                       'channels' : 64,\n",
    "                   'channels_div' : 8,\n",
    "                     'num_layers' : 1,\n",
    "                 'num_layers_ca'  : 2,\n",
    "               'edge_feature_dim' : 1,\n",
    "              'latent_pool_type'  : 'avg',\n",
    "                        't_size'  : 12,\n",
    "                         'max_t'  : 1.0,\n",
    "                           'mult' : 2,\n",
    "                       'zero_lin' : True,\n",
    "                      'use_tdeg1' : False,\n",
    "                       'roll': False,\n",
    "                       'circ_pe':False,\n",
    "                            'cuda': True,\n",
    "                  'learning rate' : 0.00005,\n",
    "                   'weight_decay' : 0.000001,\n",
    "                    'sc_nf_real'  : 0.05 ,\n",
    "                    'sc_3D_real'  : 1.0 ,\n",
    "                    'sc_3D_null'  : 0.05 ,\n",
    "                    'device'      : 'cuda',\n",
    "                    'num_epoch'   : 200,\n",
    "                    'log_freq'    : 1000,\n",
    "                    'ckpt_freq'   : 10000,\n",
    "                    'early_chkpt' : 2,\n",
    "                    'coord_scale' : 10.0,\n",
    "                    'nf_threshold_real': 1.99,\n",
    "                    'nf_dim': 5}\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "645170a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_file = 'GUN_checkpoints/gu_null/03D_04M_2024Y_09h_16m_56s/step_80000.pth'\n",
    "# checkpoint_file = 'GUN_checkpoints/gu_null/04D_04M_2024Y_01h_02m_24s/step_190000.pth'\n",
    "checkpoint_file = 'GUN_checkpoints/gu_null/17D_04M_2024Y_08h_12m_52s/step_80000.pth'\n",
    "ckpt_model=torch.load(checkpoint_file)['model']\n",
    "ckpt_opt = torch.load(checkpoint_file)['optimizer']\n",
    "exp = Experiment(conf=conf,ckpt_model=ckpt_model,ckpt_opt=ckpt_opt, cur_step=80000, cur_epoch=63)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d11ad81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "number of epochs 200\n",
      "epoch 63\n",
      "mem_used 267491328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nwoodall/miniconda3/envs/se33/lib/python3.9/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
      "/home/nwoodall/miniconda3/envs/se33/lib/python3.9/site-packages/dgl/backend/pytorch/tensor.py:449: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[81000]: pnf_loss=0.1720 structure_loss=0.6261 structure_null=0.0159 structure_real=0.6102, steps/sec=1.66463\n",
      "0.8913424137234688\n",
      "epoch 64\n",
      "mem_used 330800640\n",
      "[82000]: pnf_loss=0.1188 structure_loss=0.5737 structure_null=0.0211 structure_real=0.5526, steps/sec=2.26389\n",
      "0.890775143939746\n",
      "epoch 65\n",
      "mem_used 330141184\n",
      "[83000]: pnf_loss=0.0991 structure_loss=0.9243 structure_null=0.0158 structure_real=0.9085, steps/sec=3.44608\n",
      "0.8681614997455612\n",
      "epoch 66\n",
      "mem_used 329175552\n",
      "[84000]: pnf_loss=0.0962 structure_loss=0.7531 structure_null=0.0153 structure_real=0.7378, steps/sec=7.43667\n",
      "0.8489423753392749\n",
      "[85000]: pnf_loss=0.3431 structure_loss=0.7611 structure_null=0.0119 structure_real=0.7492, steps/sec=1.70967\n",
      "0.8719044039845467\n",
      "epoch 67\n",
      "mem_used 329757184\n",
      "[86000]: pnf_loss=0.1736 structure_loss=0.8050 structure_null=0.0160 structure_real=0.7890, steps/sec=1.75615\n",
      "0.8593068655748917\n",
      "epoch 68\n",
      "mem_used 330156544\n",
      "[87000]: pnf_loss=0.0644 structure_loss=0.5708 structure_null=0.0159 structure_real=0.5549, steps/sec=2.38443\n",
      "0.8333057655321134\n",
      "epoch 69\n",
      "mem_used 328947712\n",
      "[88000]: pnf_loss=0.1085 structure_loss=0.7254 structure_null=0.0159 structure_real=0.7094, steps/sec=3.72749\n",
      "0.8982896854263206\n",
      "epoch 70\n",
      "mem_used 331291136\n",
      "[89000]: pnf_loss=0.2070 structure_loss=0.7009 structure_null=0.0171 structure_real=0.6838, steps/sec=8.44501\n",
      "0.8993444760047381\n",
      "[90000]: pnf_loss=0.0635 structure_loss=0.5917 structure_null=0.0126 structure_real=0.5791, steps/sec=1.70454\n",
      "0.8366420676112175\n",
      "eval Eval_Direc/gu_null_18D_04M_2024Y/18D_04M_2024Y_10h_29m_24s/step_90000\n",
      "[12]: pnf_loss=0.1410 structure_loss=0.5766 structure_null=0.0178 structure_real=0.5588\n",
      "eval_loss 1.485583 12\n",
      "epoch 71\n",
      "mem_used 329253888\n",
      "[91000]: pnf_loss=0.2118 structure_loss=1.3414 structure_null=0.0177 structure_real=1.3237, steps/sec=1.80453\n",
      "0.8587982833890592\n",
      "epoch 72\n",
      "mem_used 329918976\n",
      "[92000]: pnf_loss=0.0555 structure_loss=0.6217 structure_null=0.0175 structure_real=0.6041, steps/sec=2.50730\n",
      "0.8545182004592651\n",
      "epoch 73\n",
      "mem_used 331131904\n",
      "[93000]: pnf_loss=1.0063 structure_loss=1.1747 structure_null=0.0154 structure_real=1.1594, steps/sec=3.96878\n",
      "0.8381521452304929\n",
      "epoch 74\n",
      "mem_used 329730560\n",
      "[94000]: pnf_loss=0.0745 structure_loss=0.7808 structure_null=0.0162 structure_real=0.7647, steps/sec=9.81889\n",
      "0.8376156667064381\n",
      "[95000]: pnf_loss=0.1361 structure_loss=0.7195 structure_null=0.0143 structure_real=0.7053, steps/sec=1.70811\n",
      "0.8493686896562577\n",
      "epoch 75\n",
      "mem_used 329165824\n",
      "[96000]: pnf_loss=0.0859 structure_loss=0.6119 structure_null=0.0097 structure_real=0.6022, steps/sec=1.85414\n",
      "0.8386250892450716\n",
      "epoch 76\n",
      "mem_used 330950656\n",
      "[97000]: pnf_loss=0.1867 structure_loss=0.7698 structure_null=0.0141 structure_real=0.7557, steps/sec=2.58981\n",
      "0.8480237287346256\n",
      "epoch 77\n",
      "mem_used 328737792\n",
      "[98000]: pnf_loss=0.0939 structure_loss=0.6570 structure_null=0.0154 structure_real=0.6416, steps/sec=4.26664\n",
      "0.8473347335015956\n",
      "epoch 78\n",
      "mem_used 329139712\n",
      "[99000]: pnf_loss=0.3587 structure_loss=0.8368 structure_null=0.0143 structure_real=0.8225, steps/sec=11.80405\n",
      "0.8490666175710744\n",
      "[100000]: pnf_loss=0.1906 structure_loss=0.8908 structure_null=0.0147 structure_real=0.8762, steps/sec=1.70868\n",
      "0.8596336904168129\n",
      "eval Eval_Direc/gu_null_18D_04M_2024Y/18D_04M_2024Y_10h_29m_24s/step_100000\n",
      "[12]: pnf_loss=0.0637 structure_loss=0.6085 structure_null=0.0154 structure_real=0.5931\n",
      "eval_loss 1.452613 12\n",
      "epoch 79\n",
      "mem_used 330300928\n",
      "[101000]: pnf_loss=0.0905 structure_loss=0.6559 structure_null=0.0141 structure_real=0.6418, steps/sec=1.91955\n",
      "0.9806519986407177\n",
      "epoch 80\n",
      "mem_used 329984000\n",
      "[102000]: pnf_loss=0.1540 structure_loss=0.5752 structure_null=0.0145 structure_real=0.5607, steps/sec=2.71361\n",
      "0.8771379827130615\n",
      "epoch 81\n",
      "mem_used 330441216\n",
      "[103000]: pnf_loss=0.0813 structure_loss=0.5723 structure_null=0.0160 structure_real=0.5563, steps/sec=4.55889\n",
      "0.8876867984386689\n",
      "epoch 82\n",
      "mem_used 331919872\n",
      "[104000]: pnf_loss=0.1405 structure_loss=0.8264 structure_null=0.0158 structure_real=0.8106, steps/sec=14.54130\n",
      "0.8594518252927014\n",
      "[105000]: pnf_loss=0.0414 structure_loss=0.5908 structure_null=0.0115 structure_real=0.5793, steps/sec=1.70329\n",
      "0.8410958732962608\n",
      "epoch 83\n",
      "mem_used 329391104\n",
      "[106000]: pnf_loss=0.1377 structure_loss=0.8408 structure_null=0.0146 structure_real=0.8262, steps/sec=1.97706\n",
      "0.8154927820660347\n",
      "epoch 84\n",
      "mem_used 330945024\n",
      "[107000]: pnf_loss=0.0417 structure_loss=0.6524 structure_null=0.0156 structure_real=0.6368, steps/sec=2.81604\n",
      "0.864016941827328\n",
      "epoch 85\n",
      "mem_used 329654272\n",
      "[108000]: pnf_loss=0.2040 structure_loss=0.6776 structure_null=0.0149 structure_real=0.6627, steps/sec=4.91860\n",
      "0.8569244897434477\n",
      "epoch 86\n",
      "mem_used 329382912\n",
      "[109000]: pnf_loss=0.1729 structure_loss=0.6793 structure_null=0.0142 structure_real=0.6652, steps/sec=19.36568\n",
      "0.8570061814918947\n",
      "[110000]: pnf_loss=0.1189 structure_loss=0.6135 structure_null=0.0162 structure_real=0.5973, steps/sec=1.70997\n",
      "0.8582891087532043\n",
      "eval Eval_Direc/gu_null_18D_04M_2024Y/18D_04M_2024Y_10h_29m_24s/step_110000\n",
      "[12]: pnf_loss=0.0624 structure_loss=0.6658 structure_null=0.0126 structure_real=0.6532\n",
      "eval_loss 1.555009 12\n",
      "epoch 87\n",
      "mem_used 329777664\n",
      "[111000]: pnf_loss=0.0710 structure_loss=0.7378 structure_null=0.0201 structure_real=0.7177, steps/sec=2.05307\n",
      "0.8546006289812235\n",
      "epoch 88\n",
      "mem_used 329269248\n",
      "[112000]: pnf_loss=0.0946 structure_loss=0.5556 structure_null=0.0142 structure_real=0.5414, steps/sec=2.92884\n",
      "0.8942014216340106\n",
      "epoch 89\n",
      "mem_used 329323520\n",
      "[113000]: pnf_loss=0.0813 structure_loss=0.7334 structure_null=0.0168 structure_real=0.7166, steps/sec=5.12524\n",
      "0.9278808007450223\n",
      "epoch 90\n",
      "mem_used 330658816\n",
      "[114000]: pnf_loss=0.0640 structure_loss=0.5785 structure_null=0.0177 structure_real=0.5608, steps/sec=28.11958\n",
      "0.8834860911134814\n",
      "[115000]: pnf_loss=0.0764 structure_loss=0.5731 structure_null=0.0146 structure_real=0.5585, steps/sec=1.70246\n",
      "0.8618601128458977\n",
      "epoch 91\n",
      "mem_used 329701376\n",
      "[116000]: pnf_loss=0.0345 structure_loss=0.6769 structure_null=0.0142 structure_real=0.6627, steps/sec=2.11731\n",
      "0.8405396073908356\n",
      "epoch 92\n",
      "mem_used 331037696\n",
      "[117000]: pnf_loss=0.0791 structure_loss=0.5693 structure_null=0.0142 structure_real=0.5551, steps/sec=3.13134\n",
      "0.867032573580524\n",
      "epoch 93\n",
      "mem_used 329494016\n",
      "[118000]: pnf_loss=0.0846 structure_loss=0.6095 structure_null=0.0129 structure_real=0.5966, steps/sec=5.89362\n",
      "0.9681370702283135\n",
      "epoch 94\n",
      "mem_used 329637888\n",
      "[119000]: pnf_loss=0.0673 structure_loss=0.6342 structure_null=0.0146 structure_real=0.6196, steps/sec=51.51792\n",
      "0.9726887146631876\n",
      "[120000]: pnf_loss=0.0954 structure_loss=0.5880 structure_null=0.0155 structure_real=0.5725, steps/sec=1.70011\n",
      "0.9353151745200157\n",
      "eval Eval_Direc/gu_null_18D_04M_2024Y/18D_04M_2024Y_10h_29m_24s/step_120000\n",
      "[12]: pnf_loss=0.1538 structure_loss=0.6494 structure_null=0.0211 structure_real=0.6283\n",
      "eval_loss 1.5684681 12\n",
      "epoch 95\n",
      "mem_used 329967104\n",
      "[121000]: pnf_loss=0.1268 structure_loss=0.6724 structure_null=0.0183 structure_real=0.6540, steps/sec=2.20627\n",
      "0.8802562016494495\n",
      "epoch 96\n",
      "mem_used 330043392\n",
      "[122000]: pnf_loss=0.0864 structure_loss=0.6484 structure_null=0.0156 structure_real=0.6328, steps/sec=3.28563\n",
      "0.8702040321565088\n",
      "epoch 97\n",
      "mem_used 329934848\n",
      "[123000]: pnf_loss=0.1304 structure_loss=0.6542 structure_null=0.0166 structure_real=0.6376, steps/sec=6.50019\n",
      "0.8307181897054192\n",
      "epoch 98\n",
      "mem_used 330512896\n",
      "[124000]: pnf_loss=0.0783 structure_loss=0.7858 structure_null=0.0159 structure_real=0.7699, steps/sec=323.94677\n",
      "0.7901390075683594\n",
      "[125000]: pnf_loss=0.0868 structure_loss=0.5851 structure_null=0.0211 structure_real=0.5640, steps/sec=1.72279\n",
      "0.8709188383221627\n",
      "epoch 99\n",
      "mem_used 329761792\n",
      "[126000]: pnf_loss=0.1393 structure_loss=0.6249 structure_null=0.0190 structure_real=0.6059, steps/sec=2.28914\n",
      "0.8855608149486429\n",
      "epoch 100\n",
      "mem_used 330437120\n",
      "[127000]: pnf_loss=0.0964 structure_loss=0.6007 structure_null=0.0152 structure_real=0.5855, steps/sec=3.47946\n",
      "0.843878172444229\n",
      "epoch 101\n",
      "mem_used 329996288\n",
      "[128000]: pnf_loss=0.2077 structure_loss=0.7115 structure_null=0.0134 structure_real=0.6982, steps/sec=7.29685\n",
      "0.8376902978644412\n",
      "[129000]: pnf_loss=0.0495 structure_loss=0.5819 structure_null=0.0186 structure_real=0.5633, steps/sec=1.71486\n",
      "0.8764489697217941\n",
      "epoch 102\n",
      "mem_used 329244672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[130000]: pnf_loss=0.0941 structure_loss=0.6123 structure_null=0.0188 structure_real=0.5935, steps/sec=1.74802\n",
      "0.8764637076012798\n",
      "eval Eval_Direc/gu_null_18D_04M_2024Y/18D_04M_2024Y_10h_29m_24s/step_130000\n",
      "[12]: pnf_loss=0.0584 structure_loss=0.6519 structure_null=0.0151 structure_real=0.6367\n",
      "eval_loss 1.48955 12\n",
      "epoch 103\n",
      "mem_used 330154496\n",
      "[131000]: pnf_loss=0.1217 structure_loss=0.8063 structure_null=0.0164 structure_real=0.7899, steps/sec=2.37256\n",
      "0.8600767249034511\n",
      "epoch 104\n",
      "mem_used 328772608\n",
      "[132000]: pnf_loss=0.1737 structure_loss=0.6267 structure_null=0.0158 structure_real=0.6109, steps/sec=3.73011\n",
      "0.8397645705719489\n",
      "epoch 105\n",
      "mem_used 328729088\n",
      "[133000]: pnf_loss=0.2980 structure_loss=0.8885 structure_null=0.0143 structure_real=0.8741, steps/sec=8.25837\n",
      "1.0077410299801133\n",
      "[134000]: pnf_loss=0.0800 structure_loss=0.8284 structure_null=0.0168 structure_real=0.8117, steps/sec=1.71734\n",
      "0.8470541269779205\n",
      "epoch 106\n",
      "mem_used 329354752\n",
      "[135000]: pnf_loss=0.1305 structure_loss=0.7406 structure_null=0.0156 structure_real=0.7251, steps/sec=1.80563\n",
      "0.8503663007904029\n",
      "epoch 107\n",
      "mem_used 329576448\n",
      "[136000]: pnf_loss=0.2881 structure_loss=0.8958 structure_null=0.0146 structure_real=0.8812, steps/sec=2.45063\n",
      "0.8443962767978624\n",
      "epoch 108\n",
      "mem_used 329668096\n",
      "[137000]: pnf_loss=0.1948 structure_loss=0.6912 structure_null=0.0143 structure_real=0.6769, steps/sec=3.14613\n",
      "0.8212057187639433\n",
      "epoch 109\n",
      "mem_used 331173888\n",
      "[138000]: pnf_loss=0.0712 structure_loss=0.6571 structure_null=0.0148 structure_real=0.6423, steps/sec=7.69499\n",
      "0.8436869007817814\n",
      "[139000]: pnf_loss=0.0964 structure_loss=0.6217 structure_null=0.0151 structure_real=0.6066, steps/sec=1.37504\n",
      "0.8229782452583313\n",
      "epoch 110\n",
      "mem_used 330616320\n",
      "[140000]: pnf_loss=0.0818 structure_loss=0.8678 structure_null=0.0115 structure_real=0.8562, steps/sec=1.49250\n",
      "0.8235417871081738\n",
      "eval Eval_Direc/gu_null_18D_04M_2024Y/18D_04M_2024Y_10h_29m_24s/step_140000\n",
      "[12]: pnf_loss=0.0664 structure_loss=0.5850 structure_null=0.0137 structure_real=0.5713\n",
      "eval_loss 1.4575027 12\n",
      "epoch 111\n",
      "mem_used 331758592\n",
      "[141000]: pnf_loss=0.0989 structure_loss=0.5712 structure_null=0.0146 structure_real=0.5566, steps/sec=2.07485\n",
      "0.8236470105777304\n",
      "epoch 112\n",
      "mem_used 329200128\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 251\u001b[0m, in \u001b[0;36mExperiment.start_training\u001b[0;34m(self, return_logs)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m, epoch)\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmem_used\u001b[39m\u001b[38;5;124m'\u001b[39m,torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmemory_allocated(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m--> 251\u001b[0m epoch_log \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_logs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_logs\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_logs:\n\u001b[1;32m    258\u001b[0m     logs\u001b[38;5;241m.\u001b[39mappend(epoch_log)\n",
      "Cell \u001b[0;32mIn[7], line 277\u001b[0m, in \u001b[0;36mExperiment.train_epoch\u001b[0;34m(self, train_loader, valid_loader, epoch, return_logs)\u001b[0m\n\u001b[1;32m    273\u001b[0m losskeeper \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_feats \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m    275\u001b[0m     \n\u001b[1;32m    276\u001b[0m     \u001b[38;5;66;03m#train_feats = tree.map_structure(lambda x: x.to(device), train_feats)\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m     loss, aux_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_feats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m     losskeeper\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(loss))\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m aux_data\u001b[38;5;241m.\u001b[39mitems():\n",
      "Cell \u001b[0;32mIn[7], line 355\u001b[0m, in \u001b[0;36mExperiment.update_fn\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    353\u001b[0m loss, aux_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(data)\n\u001b[1;32m    354\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 355\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    356\u001b[0m loss_out \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m    357\u001b[0m \u001b[38;5;66;03m#del loss\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/se33/lib/python3.9/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/se33/lib/python3.9/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/miniconda3/envs/se33/lib/python3.9/site-packages/torch/optim/adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    130\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    133\u001b[0m         group,\n\u001b[1;32m    134\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    139\u001b[0m         state_steps)\n\u001b[0;32m--> 141\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniconda3/envs/se33/lib/python3.9/site-packages/torch/optim/adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 281\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/se33/lib/python3.9/site-packages/torch/optim/adam.py:490\u001b[0m, in \u001b[0;36m_multi_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    489\u001b[0m     bias_correction1 \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m _get_value(step) \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m device_state_steps]\n\u001b[0;32m--> 490\u001b[0m     bias_correction2 \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m _get_value(step) \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m device_state_steps]\n\u001b[1;32m    492\u001b[0m     step_size \u001b[38;5;241m=\u001b[39m _stack_if_compiling([(lr \u001b[38;5;241m/\u001b[39m bc) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m bc \u001b[38;5;129;01min\u001b[39;00m bias_correction1])\n\u001b[1;32m    494\u001b[0m     bias_correction2_sqrt \u001b[38;5;241m=\u001b[39m [_dispatch_sqrt(bc) \u001b[38;5;28;01mfor\u001b[39;00m bc \u001b[38;5;129;01min\u001b[39;00m bias_correction2]\n",
      "File \u001b[0;32m~/miniconda3/envs/se33/lib/python3.9/site-packages/torch/optim/adam.py:490\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    489\u001b[0m     bias_correction1 \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m _get_value(step) \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m device_state_steps]\n\u001b[0;32m--> 490\u001b[0m     bias_correction2 \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m device_state_steps]\n\u001b[1;32m    492\u001b[0m     step_size \u001b[38;5;241m=\u001b[39m _stack_if_compiling([(lr \u001b[38;5;241m/\u001b[39m bc) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m bc \u001b[38;5;129;01min\u001b[39;00m bias_correction1])\n\u001b[1;32m    494\u001b[0m     bias_correction2_sqrt \u001b[38;5;241m=\u001b[39m [_dispatch_sqrt(bc) \u001b[38;5;28;01mfor\u001b[39;00m bc \u001b[38;5;129;01min\u001b[39;00m bias_correction2]\n",
      "File \u001b[0;32m~/miniconda3/envs/se33/lib/python3.9/site-packages/torch/optim/optimizer.py:44\u001b[0m, in \u001b[0;36m_get_value\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "exp.start_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0305f1ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7970fc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = Experiment(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2cf6e82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "number of epochs 200\n",
      "epoch 0\n",
      "mem_used 44206080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nwoodall/miniconda3/envs/se33/lib/python3.9/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
      "/home/nwoodall/miniconda3/envs/se33/lib/python3.9/site-packages/dgl/backend/pytorch/tensor.py:449: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]: pnf_loss=0.7902 structure_loss=1.2504 structure_null=0.0318 structure_real=1.2185, steps/sec=137.56294\n",
      "2.040560007095337\n",
      "eval Eval_Direc/gu_null_17D_04M_2024Y/17D_04M_2024Y_08h_12m_52s/step_2\n",
      "[12]: pnf_loss=0.2440 structure_loss=1.0732 structure_null=0.0317 structure_real=1.0414\n",
      "eval_loss 2.5491464 12\n",
      "[1000]: pnf_loss=0.3434 structure_loss=1.0767 structure_null=0.0314 structure_real=1.0454, steps/sec=2.12608\n",
      "1.3665101749300956\n",
      "epoch 1\n",
      "mem_used 195171328\n",
      "[2000]: pnf_loss=0.2056 structure_loss=0.7446 structure_null=0.0267 structure_real=0.7179, steps/sec=2.88742\n",
      "1.2791705283149575\n",
      "epoch 2\n",
      "mem_used 196280320\n",
      "[3000]: pnf_loss=0.1922 structure_loss=0.7652 structure_null=0.0257 structure_real=0.7395, steps/sec=4.42737\n",
      "1.2312087887599144\n",
      "epoch 3\n",
      "mem_used 196801024\n",
      "[4000]: pnf_loss=0.1622 structure_loss=0.7723 structure_null=0.0288 structure_real=0.7436, steps/sec=9.51446\n",
      "1.2101461189803078\n",
      "[5000]: pnf_loss=0.2156 structure_loss=0.8310 structure_null=0.0269 structure_real=0.8041, steps/sec=2.20545\n",
      "1.141635810673237\n",
      "epoch 4\n",
      "mem_used 197178368\n",
      "[6000]: pnf_loss=0.1962 structure_loss=0.8019 structure_null=0.0254 structure_real=0.7764, steps/sec=2.27320\n",
      "1.151725446552406\n",
      "epoch 5\n",
      "mem_used 197050880\n",
      "[7000]: pnf_loss=0.4011 structure_loss=0.9644 structure_null=0.0270 structure_real=0.9374, steps/sec=3.08048\n",
      "1.118201465539999\n",
      "epoch 6\n",
      "mem_used 197751296\n",
      "[8000]: pnf_loss=0.1963 structure_loss=0.7595 structure_null=0.0247 structure_real=0.7348, steps/sec=4.80950\n",
      "1.131790168680999\n",
      "epoch 7\n",
      "mem_used 197099008\n",
      "[9000]: pnf_loss=2.2604 structure_loss=2.4919 structure_null=0.0263 structure_real=2.4655, steps/sec=10.96620\n",
      "1.1206043908252052\n",
      "[10000]: pnf_loss=0.1588 structure_loss=1.0365 structure_null=0.0211 structure_real=1.0154, steps/sec=2.20646\n",
      "1.1415702176690101\n",
      "eval Eval_Direc/gu_null_17D_04M_2024Y/17D_04M_2024Y_08h_12m_52s/step_10000\n",
      "[12]: pnf_loss=0.2075 structure_loss=0.9366 structure_null=0.0239 structure_real=0.9127\n",
      "eval_loss 2.0871828 12\n",
      "epoch 8\n",
      "mem_used 196485120\n",
      "[11000]: pnf_loss=0.1340 structure_loss=0.7675 structure_null=0.0223 structure_real=0.7453, steps/sec=2.33992\n",
      "1.0587354383857575\n",
      "epoch 9\n",
      "mem_used 196181504\n",
      "[12000]: pnf_loss=0.1650 structure_loss=0.6997 structure_null=0.0197 structure_real=0.6800, steps/sec=3.21688\n",
      "1.0826916593998628\n",
      "epoch 10\n",
      "mem_used 197699584\n",
      "[13000]: pnf_loss=0.2698 structure_loss=0.6657 structure_null=0.0222 structure_real=0.6435, steps/sec=5.12935\n",
      "0.9921378479447476\n",
      "epoch 11\n",
      "mem_used 197275648\n",
      "[14000]: pnf_loss=0.7468 structure_loss=1.0294 structure_null=0.0206 structure_real=1.0088, steps/sec=12.76269\n",
      "1.0757415246412245\n",
      "[15000]: pnf_loss=1.0489 structure_loss=1.3857 structure_null=0.0242 structure_real=1.3615, steps/sec=2.20964\n",
      "1.0239952442646028\n",
      "epoch 12\n",
      "mem_used 196724224\n",
      "[16000]: pnf_loss=0.1609 structure_loss=0.8780 structure_null=0.0198 structure_real=0.8581, steps/sec=2.40902\n",
      "1.0321610037025926\n",
      "epoch 13\n",
      "mem_used 196101120\n",
      "[17000]: pnf_loss=0.1800 structure_loss=0.6154 structure_null=0.0244 structure_real=0.5910, steps/sec=3.34868\n",
      "0.985274649534674\n",
      "epoch 14\n",
      "mem_used 195210752\n",
      "[18000]: pnf_loss=0.5887 structure_loss=0.9650 structure_null=0.0200 structure_real=0.9451, steps/sec=5.49419\n",
      "0.9850558889149433\n",
      "epoch 15\n",
      "mem_used 195831808\n",
      "[19000]: pnf_loss=0.3370 structure_loss=0.9211 structure_null=0.0192 structure_real=0.9019, steps/sec=15.22686\n",
      "1.0603140428148468\n",
      "[20000]: pnf_loss=0.5549 structure_loss=1.6289 structure_null=0.0209 structure_real=1.6080, steps/sec=2.21579\n",
      "0.9678353663682938\n",
      "eval Eval_Direc/gu_null_17D_04M_2024Y/17D_04M_2024Y_08h_12m_52s/step_20000\n",
      "[12]: pnf_loss=0.2716 structure_loss=1.0295 structure_null=0.0188 structure_real=1.0107\n",
      "eval_loss 1.5865964 12\n",
      "epoch 16\n",
      "mem_used 197058560\n",
      "[21000]: pnf_loss=0.1340 structure_loss=0.6454 structure_null=0.0191 structure_real=0.6263, steps/sec=2.48876\n",
      "0.9524314941855164\n",
      "epoch 17\n",
      "mem_used 197014528\n",
      "[22000]: pnf_loss=0.1579 structure_loss=0.5914 structure_null=0.0184 structure_real=0.5730, steps/sec=3.48343\n",
      "0.9358820517543001\n",
      "epoch 18\n",
      "mem_used 195258880\n",
      "[23000]: pnf_loss=0.1412 structure_loss=0.6988 structure_null=0.0192 structure_real=0.6796, steps/sec=5.90270\n",
      "0.9759204672938362\n",
      "epoch 19\n",
      "mem_used 196116992\n",
      "[24000]: pnf_loss=0.1332 structure_loss=0.6659 structure_null=0.0200 structure_real=0.6459, steps/sec=18.90867\n",
      "0.8954872539919666\n",
      "[25000]: pnf_loss=0.1720 structure_loss=0.6455 structure_null=0.0215 structure_real=0.6240, steps/sec=2.21168\n",
      "0.9671144386529923\n",
      "epoch 20\n",
      "mem_used 195672064\n",
      "[26000]: pnf_loss=0.1072 structure_loss=0.6543 structure_null=0.0226 structure_real=0.6316, steps/sec=2.57217\n",
      "0.927836639728657\n",
      "epoch 21\n",
      "mem_used 195343360\n",
      "[27000]: pnf_loss=0.1591 structure_loss=0.5792 structure_null=0.0205 structure_real=0.5587, steps/sec=3.69873\n",
      "0.983019865172024\n",
      "epoch 22\n",
      "mem_used 195830272\n",
      "[28000]: pnf_loss=0.1299 structure_loss=0.6199 structure_null=0.0202 structure_real=0.5996, steps/sec=6.42472\n",
      "1.0654701290792123\n",
      "epoch 23\n",
      "mem_used 196604928\n",
      "[29000]: pnf_loss=0.0933 structure_loss=0.6465 structure_null=0.0174 structure_real=0.6291, steps/sec=24.66556\n",
      "0.9352562280183427\n",
      "[30000]: pnf_loss=0.2659 structure_loss=0.8154 structure_null=0.0224 structure_real=0.7930, steps/sec=2.21287\n",
      "0.9414287675023079\n",
      "eval Eval_Direc/gu_null_17D_04M_2024Y/17D_04M_2024Y_08h_12m_52s/step_30000\n",
      "[12]: pnf_loss=0.0854 structure_loss=0.6969 structure_null=0.0204 structure_real=0.6765\n",
      "eval_loss 1.5962886 12\n",
      "epoch 24\n",
      "mem_used 198128640\n",
      "[31000]: pnf_loss=0.1067 structure_loss=0.8392 structure_null=0.0203 structure_real=0.8190, steps/sec=2.66129\n",
      "0.971462968736887\n",
      "epoch 25\n",
      "mem_used 196510208\n",
      "[32000]: pnf_loss=0.1834 structure_loss=0.7092 structure_null=0.0189 structure_real=0.6903, steps/sec=3.85296\n",
      "0.9145793900282486\n",
      "epoch 26\n",
      "mem_used 195468288\n",
      "[33000]: pnf_loss=0.2243 structure_loss=0.7332 structure_null=0.0201 structure_real=0.7131, steps/sec=6.97605\n",
      "0.9258908545071224\n",
      "epoch 27\n",
      "mem_used 196083712\n",
      "[34000]: pnf_loss=0.1860 structure_loss=0.7667 structure_null=0.0193 structure_real=0.7475, steps/sec=36.02729\n",
      "0.9324822093619675\n",
      "[35000]: pnf_loss=0.1281 structure_loss=0.7126 structure_null=0.0210 structure_real=0.6916, steps/sec=2.21687\n",
      "0.8950762818455696\n",
      "epoch 28\n",
      "mem_used 195983872\n",
      "[36000]: pnf_loss=0.1297 structure_loss=0.6447 structure_null=0.0204 structure_real=0.6243, steps/sec=2.76149\n",
      "0.9359313977594993\n",
      "epoch 29\n",
      "mem_used 195894272\n",
      "[37000]: pnf_loss=0.1319 structure_loss=0.5654 structure_null=0.0190 structure_real=0.5464, steps/sec=4.04163\n",
      "0.9073192541516459\n",
      "epoch 30\n",
      "mem_used 197498880\n",
      "[38000]: pnf_loss=0.0921 structure_loss=0.5999 structure_null=0.0180 structure_real=0.5819, steps/sec=7.60584\n",
      "0.9290822639547546\n",
      "epoch 31\n",
      "mem_used 197212672\n",
      "[39000]: pnf_loss=0.1412 structure_loss=0.6122 structure_null=0.0185 structure_real=0.5938, steps/sec=66.72809\n",
      "1.1286324407115127\n",
      "[40000]: pnf_loss=0.1764 structure_loss=0.5925 structure_null=0.0195 structure_real=0.5731, steps/sec=2.21078\n",
      "0.9434221465587616\n",
      "eval Eval_Direc/gu_null_17D_04M_2024Y/17D_04M_2024Y_08h_12m_52s/step_40000\n",
      "[12]: pnf_loss=0.0865 structure_loss=0.6863 structure_null=0.0159 structure_real=0.6705\n",
      "eval_loss 1.8573751 12\n",
      "epoch 32\n",
      "mem_used 196464640\n",
      "[41000]: pnf_loss=0.3078 structure_loss=0.7609 structure_null=0.0167 structure_real=0.7442, steps/sec=2.85744\n",
      "0.9134869844335871\n",
      "epoch 33\n",
      "mem_used 195885568\n",
      "[42000]: pnf_loss=0.0920 structure_loss=0.6892 structure_null=0.0167 structure_real=0.6725, steps/sec=4.27140\n",
      "0.9132257121146758\n",
      "epoch 34\n",
      "mem_used 196351488\n",
      "[43000]: pnf_loss=0.0563 structure_loss=0.6839 structure_null=0.0194 structure_real=0.6645, steps/sec=8.50500\n",
      "0.9475574818731264\n",
      "epoch 35\n",
      "mem_used 197655040\n",
      "[44000]: pnf_loss=0.0557 structure_loss=0.8627 structure_null=0.0158 structure_real=0.8469, steps/sec=442.98226\n",
      "0.9252865314483643\n",
      "[45000]: pnf_loss=0.1163 structure_loss=0.6658 structure_null=0.0211 structure_real=0.6447, steps/sec=2.21774\n",
      "0.9197920432090759\n",
      "epoch 36\n",
      "mem_used 198132224\n",
      "[46000]: pnf_loss=0.0893 structure_loss=0.7691 structure_null=0.0188 structure_real=0.7503, steps/sec=2.96356\n",
      "0.924434908730461\n",
      "epoch 37\n",
      "mem_used 195140608\n",
      "[47000]: pnf_loss=0.1133 structure_loss=0.5766 structure_null=0.0201 structure_real=0.5564, steps/sec=4.49290\n",
      "0.9394820361661814\n",
      "epoch 38\n",
      "mem_used 195436032\n",
      "[48000]: pnf_loss=0.1101 structure_loss=0.6901 structure_null=0.0178 structure_real=0.6723, steps/sec=9.41286\n",
      "1.0335572532608979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[49000]: pnf_loss=0.1569 structure_loss=0.6224 structure_null=0.0164 structure_real=0.6060, steps/sec=2.20709\n",
      "0.9052397536635399\n",
      "epoch 39\n",
      "mem_used 197422080\n",
      "[50000]: pnf_loss=0.1143 structure_loss=0.6028 structure_null=0.0189 structure_real=0.5839, steps/sec=2.25806\n",
      "0.9249303359214213\n",
      "eval Eval_Direc/gu_null_17D_04M_2024Y/17D_04M_2024Y_08h_12m_52s/step_50000\n",
      "[12]: pnf_loss=0.1252 structure_loss=0.7132 structure_null=0.0187 structure_real=0.6945\n",
      "eval_loss 1.4671744 12\n",
      "epoch 40\n",
      "mem_used 196446720\n",
      "[51000]: pnf_loss=0.0580 structure_loss=0.7400 structure_null=0.0220 structure_real=0.7180, steps/sec=3.06474\n",
      "0.930435782422622\n",
      "epoch 41\n",
      "mem_used 196203520\n",
      "[52000]: pnf_loss=0.1457 structure_loss=0.6598 structure_null=0.0200 structure_real=0.6399, steps/sec=4.78882\n",
      "0.8926469082451278\n",
      "epoch 42\n",
      "mem_used 197757440\n",
      "[53000]: pnf_loss=0.1288 structure_loss=0.5704 structure_null=0.0195 structure_real=0.5509, steps/sec=10.77557\n",
      "0.9105219681864803\n",
      "[54000]: pnf_loss=0.1031 structure_loss=0.6778 structure_null=0.0203 structure_real=0.6575, steps/sec=2.21324\n",
      "0.8923358563780784\n",
      "epoch 43\n",
      "mem_used 197269504\n",
      "[55000]: pnf_loss=0.1167 structure_loss=0.8707 structure_null=0.0150 structure_real=0.8557, steps/sec=2.33400\n",
      "0.8919223910639233\n",
      "epoch 44\n",
      "mem_used 196056576\n",
      "[56000]: pnf_loss=0.1524 structure_loss=0.8291 structure_null=0.0173 structure_real=0.8118, steps/sec=3.21027\n",
      "0.8612186054273837\n",
      "epoch 45\n",
      "mem_used 196088320\n",
      "[57000]: pnf_loss=0.1243 structure_loss=0.6438 structure_null=0.0167 structure_real=0.6271, steps/sec=5.09803\n",
      "0.8928927686022616\n",
      "epoch 46\n",
      "mem_used 195487232\n",
      "[58000]: pnf_loss=0.1571 structure_loss=0.7664 structure_null=0.0184 structure_real=0.7479, steps/sec=12.41914\n",
      "0.9539925224995345\n",
      "[59000]: pnf_loss=0.1047 structure_loss=0.6223 structure_null=0.0150 structure_real=0.6073, steps/sec=2.22131\n",
      "0.9191928842067718\n",
      "epoch 47\n",
      "mem_used 197717504\n",
      "[60000]: pnf_loss=0.1352 structure_loss=0.9508 structure_null=0.0176 structure_real=0.9332, steps/sec=2.40247\n",
      "0.8766273790798539\n",
      "eval Eval_Direc/gu_null_17D_04M_2024Y/17D_04M_2024Y_08h_12m_52s/step_60000\n",
      "[12]: pnf_loss=0.0955 structure_loss=0.6013 structure_null=0.0121 structure_real=0.5892\n",
      "eval_loss 1.3811599 12\n",
      "epoch 48\n",
      "mem_used 195919360\n",
      "[61000]: pnf_loss=0.1322 structure_loss=0.8056 structure_null=0.0187 structure_real=0.7869, steps/sec=3.32661\n",
      "0.9217327409479992\n",
      "epoch 49\n",
      "mem_used 196050432\n",
      "[62000]: pnf_loss=0.1049 structure_loss=0.6071 structure_null=0.0175 structure_real=0.5897, steps/sec=5.43272\n",
      "0.907817361102936\n",
      "epoch 50\n",
      "mem_used 196185088\n",
      "[63000]: pnf_loss=0.1087 structure_loss=0.5770 structure_null=0.0158 structure_real=0.5612, steps/sec=14.62014\n",
      "1.0107243967056274\n",
      "[64000]: pnf_loss=0.1182 structure_loss=0.6533 structure_null=0.0157 structure_real=0.6376, steps/sec=2.21910\n",
      "0.8876893683671951\n",
      "epoch 51\n",
      "mem_used 197037568\n",
      "[65000]: pnf_loss=0.4079 structure_loss=0.7898 structure_null=0.0175 structure_real=0.7723, steps/sec=2.47195\n",
      "0.8619986785886536\n",
      "epoch 52\n",
      "mem_used 195344384\n",
      "[66000]: pnf_loss=0.6004 structure_loss=1.2444 structure_null=0.0156 structure_real=1.2287, steps/sec=3.48061\n",
      "0.9120241475742568\n",
      "epoch 53\n",
      "mem_used 196370944\n",
      "[67000]: pnf_loss=0.0755 structure_loss=0.6831 structure_null=0.0150 structure_real=0.6681, steps/sec=5.84653\n",
      "0.8882895673485104\n",
      "epoch 54\n",
      "mem_used 197604352\n",
      "[68000]: pnf_loss=0.0528 structure_loss=0.6104 structure_null=0.0154 structure_real=0.5950, steps/sec=18.09170\n",
      "0.9050341976470635\n",
      "[69000]: pnf_loss=0.1559 structure_loss=0.7486 structure_null=0.0135 structure_real=0.7352, steps/sec=2.21751\n",
      "0.8841043709516525\n",
      "epoch 55\n",
      "mem_used 195095040\n",
      "[70000]: pnf_loss=0.0535 structure_loss=0.7835 structure_null=0.0166 structure_real=0.7668, steps/sec=2.56363\n",
      "0.869587730603411\n",
      "eval Eval_Direc/gu_null_17D_04M_2024Y/17D_04M_2024Y_08h_12m_52s/step_70000\n",
      "[12]: pnf_loss=0.0987 structure_loss=1.0109 structure_null=0.0146 structure_real=0.9963\n",
      "eval_loss 1.59961 12\n",
      "epoch 56\n",
      "mem_used 196050432\n",
      "[71000]: pnf_loss=0.0986 structure_loss=0.5999 structure_null=0.0173 structure_real=0.5825, steps/sec=3.64194\n",
      "0.8732021321591578\n",
      "epoch 57\n",
      "mem_used 195004416\n",
      "[72000]: pnf_loss=0.0398 structure_loss=0.6555 structure_null=0.0129 structure_real=0.6425, steps/sec=6.30918\n",
      "0.9048911888375242\n",
      "epoch 58\n",
      "mem_used 197688320\n",
      "[73000]: pnf_loss=0.1628 structure_loss=0.7893 structure_null=0.0172 structure_real=0.7721, steps/sec=23.65366\n",
      "0.9020869922130665\n",
      "[74000]: pnf_loss=0.1868 structure_loss=0.5838 structure_null=0.0205 structure_real=0.5633, steps/sec=2.21563\n",
      "0.9085302708745002\n",
      "epoch 59\n",
      "mem_used 196646912\n",
      "[75000]: pnf_loss=0.1031 structure_loss=0.7034 structure_null=0.0183 structure_real=0.6851, steps/sec=2.64841\n",
      "0.8608728034806793\n",
      "epoch 60\n",
      "mem_used 197918208\n",
      "[76000]: pnf_loss=0.1823 structure_loss=0.6054 structure_null=0.0156 structure_real=0.5898, steps/sec=3.82290\n",
      "0.9289899282414338\n",
      "epoch 61\n",
      "mem_used 196420096\n",
      "[77000]: pnf_loss=0.0802 structure_loss=0.5741 structure_null=0.0166 structure_real=0.5576, steps/sec=6.86518\n",
      "0.8982080182423902\n",
      "epoch 62\n",
      "mem_used 196050432\n",
      "[78000]: pnf_loss=0.0910 structure_loss=0.5725 structure_null=0.0193 structure_real=0.5532, steps/sec=33.55549\n",
      "0.8364964515873881\n",
      "[79000]: pnf_loss=0.1665 structure_loss=0.8721 structure_null=0.0128 structure_real=0.8593, steps/sec=2.18024\n",
      "0.8534677616357803\n",
      "epoch 63\n",
      "mem_used 195777024\n",
      "[80000]: pnf_loss=0.1341 structure_loss=0.7768 structure_null=0.0150 structure_real=0.7619, steps/sec=0.74636\n",
      "0.8552265861273106\n",
      "eval Eval_Direc/gu_null_17D_04M_2024Y/17D_04M_2024Y_08h_12m_52s/step_80000\n",
      "[12]: pnf_loss=0.0804 structure_loss=0.6494 structure_null=0.0119 structure_real=0.6375\n",
      "eval_loss 1.4917135 12\n",
      "epoch 64\n",
      "mem_used 196030976\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 251\u001b[0m, in \u001b[0;36mExperiment.start_training\u001b[0;34m(self, return_logs)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m, epoch)\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmem_used\u001b[39m\u001b[38;5;124m'\u001b[39m,torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmemory_allocated(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m--> 251\u001b[0m epoch_log \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_logs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_logs\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_logs:\n\u001b[1;32m    258\u001b[0m     logs\u001b[38;5;241m.\u001b[39mappend(epoch_log)\n",
      "Cell \u001b[0;32mIn[7], line 277\u001b[0m, in \u001b[0;36mExperiment.train_epoch\u001b[0;34m(self, train_loader, valid_loader, epoch, return_logs)\u001b[0m\n\u001b[1;32m    273\u001b[0m losskeeper \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_feats \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m    275\u001b[0m     \n\u001b[1;32m    276\u001b[0m     \u001b[38;5;66;03m#train_feats = tree.map_structure(lambda x: x.to(device), train_feats)\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m     loss, aux_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_feats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m     losskeeper\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(loss))\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m aux_data\u001b[38;5;241m.\u001b[39mitems():\n",
      "Cell \u001b[0;32mIn[7], line 354\u001b[0m, in \u001b[0;36mExperiment.update_fn\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    353\u001b[0m loss, aux_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(data)\n\u001b[0;32m--> 354\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    356\u001b[0m loss_out \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\n",
      "File \u001b[0;32m~/miniconda3/envs/se33/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/se33/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "exp.start_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f599909a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a8e64ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = Experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ce2c47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "number of epochs 100\n",
      "epoch 0\n",
      "mem_used 44206080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nwoodall/miniconda3/envs/se33/lib/python3.9/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
      "/home/nwoodall/miniconda3/envs/se33/lib/python3.9/site-packages/dgl/backend/pytorch/tensor.py:449: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]: pnf_loss=2.8846 structure_loss=1.8591 structure_null=0.0316 structure_real=1.8275, steps/sec=142.13944\n",
      "4.74365758895874\n",
      "eval Eval_Direc/gu_null_01D_04M_2024Y/01D_04M_2024Y_00h_20m_09s/step_2\n",
      "[12]: pnf_loss=0.7785 structure_loss=1.2240 structure_null=0.0314 structure_real=1.1926\n",
      "eval_loss 4.446444 12\n",
      "[1000]: pnf_loss=2.8060 structure_loss=1.8154 structure_null=0.0316 structure_real=1.7837, steps/sec=2.15333\n",
      "1.6399432535767555\n",
      "epoch 1\n",
      "mem_used 196537856\n",
      "[2000]: pnf_loss=0.0629 structure_loss=1.1616 structure_null=0.0309 structure_real=1.1307, steps/sec=2.94169\n",
      "1.4086577666880786\n",
      "epoch 2\n",
      "mem_used 196421120\n",
      "[3000]: pnf_loss=0.0293 structure_loss=2.6763 structure_null=0.0252 structure_real=2.6511, steps/sec=4.57495\n",
      "1.4042774274516008\n",
      "epoch 3\n",
      "mem_used 196994560\n",
      "[4000]: pnf_loss=0.0047 structure_loss=1.7067 structure_null=0.0253 structure_real=1.6813, steps/sec=9.59559\n",
      "1.2987553669896188\n",
      "[5000]: pnf_loss=0.1094 structure_loss=1.1695 structure_null=0.0251 structure_real=1.1444, steps/sec=2.26084\n",
      "1.400515117108822\n",
      "epoch 4\n",
      "mem_used 197085184\n",
      "[6000]: pnf_loss=0.1011 structure_loss=1.2626 structure_null=0.0266 structure_real=1.2361, steps/sec=2.33408\n",
      "1.3363610553275411\n",
      "epoch 5\n",
      "mem_used 195948544\n",
      "[7000]: pnf_loss=0.1252 structure_loss=1.2613 structure_null=0.0246 structure_real=1.2367, steps/sec=3.17979\n",
      "1.2888272195429236\n",
      "epoch 6\n",
      "mem_used 196795392\n",
      "[8000]: pnf_loss=0.0305 structure_loss=1.0395 structure_null=0.0232 structure_real=1.0163, steps/sec=4.93187\n",
      "1.4735249670051591\n",
      "epoch 7\n",
      "mem_used 196991488\n",
      "[9000]: pnf_loss=0.1029 structure_loss=1.3839 structure_null=0.0221 structure_real=1.3617, steps/sec=11.37138\n",
      "1.3032759822423186\n",
      "[10000]: pnf_loss=0.0493 structure_loss=1.0640 structure_null=0.0221 structure_real=1.0419, steps/sec=2.25414\n",
      "1.3457162082791327\n",
      "eval Eval_Direc/gu_null_01D_04M_2024Y/01D_04M_2024Y_00h_20m_09s/step_10000\n",
      "[12]: pnf_loss=0.0280 structure_loss=1.1060 structure_null=0.0204 structure_real=1.0855\n",
      "eval_loss 2.2553747 12\n",
      "epoch 8\n",
      "mem_used 196092928\n",
      "[11000]: pnf_loss=0.0612 structure_loss=1.1743 structure_null=0.0212 structure_real=1.1531, steps/sec=2.40220\n",
      "1.3047160409643488\n",
      "epoch 9\n",
      "mem_used 197145600\n",
      "[12000]: pnf_loss=0.0458 structure_loss=1.1022 structure_null=0.0218 structure_real=1.0804, steps/sec=3.31150\n",
      "1.3097495987599246\n",
      "epoch 10\n",
      "mem_used 195415552\n",
      "[13000]: pnf_loss=0.0553 structure_loss=1.1878 structure_null=0.0221 structure_real=1.1657, steps/sec=5.34172\n",
      "1.2945949142755464\n",
      "epoch 11\n",
      "mem_used 196546048\n",
      "[14000]: pnf_loss=0.0469 structure_loss=0.8593 structure_null=0.0207 structure_real=0.8387, steps/sec=13.19844\n",
      "1.3488123930258558\n",
      "[15000]: pnf_loss=0.0744 structure_loss=1.0564 structure_null=0.0218 structure_real=1.0345, steps/sec=2.27616\n",
      "1.2885296308994294\n",
      "epoch 12\n",
      "mem_used 196451328\n",
      "[16000]: pnf_loss=0.0200 structure_loss=1.2201 structure_null=0.0175 structure_real=1.2026, steps/sec=2.50224\n",
      "1.2794683761732026\n",
      "epoch 13\n",
      "mem_used 195422208\n",
      "[17000]: pnf_loss=0.0189 structure_loss=1.2334 structure_null=0.0186 structure_real=1.2148, steps/sec=3.44386\n",
      "1.288150546040629\n",
      "epoch 14\n",
      "mem_used 197566464\n",
      "[18000]: pnf_loss=0.0777 structure_loss=0.9929 structure_null=0.0191 structure_real=0.9739, steps/sec=5.67492\n",
      "1.254150642091362\n",
      "epoch 15\n",
      "mem_used 197412864\n",
      "[19000]: pnf_loss=0.0413 structure_loss=1.0357 structure_null=0.0143 structure_real=1.0214, steps/sec=15.69827\n",
      "1.2285896938422631\n",
      "[20000]: pnf_loss=0.3984 structure_loss=2.1973 structure_null=0.0207 structure_real=2.1765, steps/sec=2.27567\n",
      "1.2635746412277222\n",
      "eval Eval_Direc/gu_null_01D_04M_2024Y/01D_04M_2024Y_00h_20m_09s/step_20000\n",
      "[12]: pnf_loss=0.0359 structure_loss=0.9654 structure_null=0.0196 structure_real=0.9458\n",
      "eval_loss 2.1004534 12\n",
      "epoch 16\n",
      "mem_used 197069824\n",
      "[21000]: pnf_loss=0.0436 structure_loss=0.8450 structure_null=0.0197 structure_real=0.8253, steps/sec=2.55805\n",
      "1.2082669059032793\n",
      "epoch 17\n",
      "mem_used 197408256\n",
      "[22000]: pnf_loss=0.0701 structure_loss=1.2225 structure_null=0.0182 structure_real=1.2043, steps/sec=3.60559\n",
      "1.207883100691008\n",
      "epoch 18\n",
      "mem_used 197753856\n",
      "[23000]: pnf_loss=0.0005 structure_loss=1.3749 structure_null=0.0180 structure_real=1.3570, steps/sec=5.72473\n",
      "1.1652391796762294\n",
      "epoch 19\n",
      "mem_used 196413952\n",
      "[24000]: pnf_loss=0.0403 structure_loss=0.8647 structure_null=0.0158 structure_real=0.8488, steps/sec=19.52414\n",
      "1.1746276943092673\n",
      "[25000]: pnf_loss=0.0406 structure_loss=0.9115 structure_null=0.0209 structure_real=0.8907, steps/sec=2.27857\n",
      "1.188285204052925\n",
      "epoch 20\n",
      "mem_used 196105728\n",
      "[26000]: pnf_loss=0.0242 structure_loss=1.4147 structure_null=0.0190 structure_real=1.3957, steps/sec=2.64835\n",
      "1.2220646045928778\n",
      "epoch 21\n",
      "mem_used 197413376\n",
      "[27000]: pnf_loss=0.0799 structure_loss=1.0703 structure_null=0.0180 structure_real=1.0523, steps/sec=3.76810\n",
      "1.3223466602328602\n",
      "epoch 22\n",
      "mem_used 195393024\n",
      "[28000]: pnf_loss=0.0899 structure_loss=1.1074 structure_null=0.0214 structure_real=1.0860, steps/sec=6.58135\n",
      "1.1720262517818827\n",
      "epoch 23\n",
      "mem_used 195579392\n",
      "[29000]: pnf_loss=0.0382 structure_loss=1.1629 structure_null=0.0200 structure_real=1.1429, steps/sec=25.38116\n",
      "1.088647461339329\n",
      "[30000]: pnf_loss=0.0381 structure_loss=1.0369 structure_null=0.0173 structure_real=1.0195, steps/sec=2.27509\n",
      "1.1985441460609436\n",
      "eval Eval_Direc/gu_null_01D_04M_2024Y/01D_04M_2024Y_00h_20m_09s/step_30000\n",
      "[12]: pnf_loss=0.1444 structure_loss=1.2130 structure_null=0.0175 structure_real=1.1954\n",
      "eval_loss 2.1023357 12\n",
      "epoch 24\n",
      "mem_used 197424128\n",
      "[31000]: pnf_loss=0.0689 structure_loss=0.9273 structure_null=0.0183 structure_real=0.9089, steps/sec=2.74166\n",
      "1.179455725977627\n",
      "epoch 25\n",
      "mem_used 195600384\n",
      "[32000]: pnf_loss=0.0321 structure_loss=0.9349 structure_null=0.0162 structure_real=0.9186, steps/sec=3.94262\n",
      "1.422202869705532\n",
      "epoch 26\n",
      "mem_used 196409344\n",
      "[33000]: pnf_loss=0.0314 structure_loss=0.9847 structure_null=0.0176 structure_real=0.9671, steps/sec=7.17119\n",
      "1.1801798538591877\n",
      "epoch 27\n",
      "mem_used 197649408\n",
      "[34000]: pnf_loss=0.0309 structure_loss=0.9817 structure_null=0.0173 structure_real=0.9644, steps/sec=37.54654\n",
      "1.2205915998239987\n",
      "[35000]: pnf_loss=0.0808 structure_loss=1.4810 structure_null=0.0193 structure_real=1.4616, steps/sec=2.28278\n",
      "1.181704049706459\n",
      "epoch 28\n",
      "mem_used 196203520\n",
      "[36000]: pnf_loss=0.0618 structure_loss=0.9198 structure_null=0.0192 structure_real=0.9006, steps/sec=2.84159\n",
      "1.1601125459321102\n",
      "epoch 29\n",
      "mem_used 196657152\n",
      "[37000]: pnf_loss=0.0443 structure_loss=0.9206 structure_null=0.0170 structure_real=0.9036, steps/sec=4.13250\n",
      "1.2403950696671466\n",
      "epoch 30\n",
      "mem_used 195990016\n",
      "[38000]: pnf_loss=0.0199 structure_loss=0.9611 structure_null=0.0173 structure_real=0.9439, steps/sec=7.82128\n",
      "1.2368809210843053\n",
      "epoch 31\n",
      "mem_used 198001152\n",
      "[39000]: pnf_loss=0.0415 structure_loss=0.9007 structure_null=0.0148 structure_real=0.8859, steps/sec=69.11113\n",
      "1.1121240077596721\n",
      "[40000]: pnf_loss=0.0449 structure_loss=0.8844 structure_null=0.0149 structure_real=0.8694, steps/sec=2.27242\n",
      "1.180408309519291\n",
      "eval Eval_Direc/gu_null_01D_04M_2024Y/01D_04M_2024Y_00h_20m_09s/step_40000\n",
      "[12]: pnf_loss=0.0386 structure_loss=0.9961 structure_null=0.0188 structure_real=0.9773\n",
      "eval_loss 2.3787205 12\n",
      "epoch 32\n",
      "mem_used 196363264\n",
      "[41000]: pnf_loss=0.0518 structure_loss=0.9216 structure_null=0.0158 structure_real=0.9059, steps/sec=2.93016\n",
      "1.1809872166090405\n",
      "epoch 33\n",
      "mem_used 195609088\n",
      "[42000]: pnf_loss=0.0284 structure_loss=1.0341 structure_null=0.0150 structure_real=1.0191, steps/sec=4.37495\n",
      "1.1510393353096553\n",
      "epoch 34\n",
      "mem_used 196516864\n",
      "[43000]: pnf_loss=0.0410 structure_loss=1.1812 structure_null=0.0165 structure_real=1.1646, steps/sec=8.70419\n",
      "1.1163665190452838\n",
      "epoch 35\n",
      "mem_used 196083200\n",
      "[44000]: pnf_loss=0.0339 structure_loss=1.0086 structure_null=0.0157 structure_real=0.9929, steps/sec=445.92489\n",
      "0.9646596670150757\n",
      "[45000]: pnf_loss=0.0214 structure_loss=1.4359 structure_null=0.0176 structure_real=1.4184, steps/sec=2.26770\n",
      "1.129015704870224\n",
      "epoch 36\n",
      "mem_used 197114880\n",
      "[46000]: pnf_loss=0.0137 structure_loss=1.0186 structure_null=0.0161 structure_real=1.0025, steps/sec=3.04470\n",
      "1.1137830864458798\n",
      "epoch 37\n",
      "mem_used 196418048\n",
      "[47000]: pnf_loss=0.0849 structure_loss=1.5000 structure_null=0.0176 structure_real=1.4824, steps/sec=4.64329\n",
      "1.1279716000051945\n",
      "epoch 38\n",
      "mem_used 197701632\n",
      "[48000]: pnf_loss=0.0283 structure_loss=0.9108 structure_null=0.0158 structure_real=0.8950, steps/sec=9.72475\n",
      "1.0805276082112238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[49000]: pnf_loss=0.0003 structure_loss=3.2116 structure_null=0.0191 structure_real=3.1925, steps/sec=2.27050\n",
      "1.1563784728050233\n",
      "epoch 39\n",
      "mem_used 196229120\n",
      "[50000]: pnf_loss=0.0196 structure_loss=0.8673 structure_null=0.0151 structure_real=0.8522, steps/sec=2.32234\n",
      "1.1142327558542835\n",
      "eval Eval_Direc/gu_null_01D_04M_2024Y/01D_04M_2024Y_00h_20m_09s/step_50000\n",
      "[12]: pnf_loss=0.0979 structure_loss=1.0699 structure_null=0.0177 structure_real=1.0522\n",
      "eval_loss 2.1719978 12\n",
      "epoch 40\n",
      "mem_used 196002816\n",
      "[51000]: pnf_loss=0.0988 structure_loss=2.3887 structure_null=0.0194 structure_real=2.3694, steps/sec=3.16707\n",
      "1.1755057214034927\n",
      "epoch 41\n",
      "mem_used 196826112\n",
      "[52000]: pnf_loss=0.0554 structure_loss=0.9170 structure_null=0.0144 structure_real=0.9026, steps/sec=4.91259\n",
      "1.1106835808403805\n",
      "epoch 42\n",
      "mem_used 197424128\n",
      "[53000]: pnf_loss=0.0132 structure_loss=0.9085 structure_null=0.0138 structure_real=0.8947, steps/sec=11.04416\n",
      "1.1679852165064766\n",
      "[54000]: pnf_loss=0.0646 structure_loss=1.2299 structure_null=0.0157 structure_real=1.2142, steps/sec=2.27226\n",
      "1.1101143036484717\n",
      "epoch 43\n",
      "mem_used 196847104\n",
      "[55000]: pnf_loss=0.0457 structure_loss=0.9373 structure_null=0.0162 structure_real=0.9211, steps/sec=2.39952\n",
      "1.1367774526613152\n",
      "epoch 44\n",
      "mem_used 195593216\n",
      "[56000]: pnf_loss=0.0193 structure_loss=0.9125 structure_null=0.0151 structure_real=0.8974, steps/sec=3.30398\n",
      "1.1405613381910875\n",
      "epoch 45\n",
      "mem_used 195588608\n",
      "[57000]: pnf_loss=0.0538 structure_loss=0.9186 structure_null=0.0164 structure_real=0.9021, steps/sec=5.21976\n",
      "1.111681913644418\n",
      "epoch 46\n",
      "mem_used 196080128\n",
      "[58000]: pnf_loss=0.2060 structure_loss=1.3727 structure_null=0.0165 structure_real=1.3562, steps/sec=12.69872\n",
      "1.110244729545679\n",
      "[59000]: pnf_loss=0.0112 structure_loss=0.9656 structure_null=0.0170 structure_real=0.9486, steps/sec=2.28983\n",
      "1.2148071895241737\n",
      "epoch 47\n",
      "mem_used 197352448\n",
      "[60000]: pnf_loss=0.0000 structure_loss=1.2272 structure_null=0.0184 structure_real=1.2088, steps/sec=2.46067\n",
      "1.1040367475679462\n",
      "eval Eval_Direc/gu_null_01D_04M_2024Y/01D_04M_2024Y_00h_20m_09s/step_60000\n",
      "[12]: pnf_loss=0.0863 structure_loss=1.5474 structure_null=0.0179 structure_real=1.5294\n",
      "eval_loss 2.407183 12\n",
      "epoch 48\n",
      "mem_used 196335104\n",
      "[61000]: pnf_loss=0.0495 structure_loss=0.8470 structure_null=0.0175 structure_real=0.8296, steps/sec=3.43398\n",
      "1.109151857714337\n",
      "epoch 49\n",
      "mem_used 196193280\n",
      "[62000]: pnf_loss=0.0463 structure_loss=0.9191 structure_null=0.0173 structure_real=0.9018, steps/sec=5.63273\n",
      "1.1518280885436318\n",
      "epoch 50\n",
      "mem_used 196652032\n",
      "[63000]: pnf_loss=0.0008 structure_loss=1.1475 structure_null=0.0184 structure_real=1.1291, steps/sec=15.20528\n",
      "1.0870420396327973\n",
      "[64000]: pnf_loss=0.0030 structure_loss=1.1060 structure_null=0.0154 structure_real=1.0906, steps/sec=2.28372\n",
      "1.0896770619750022\n",
      "epoch 51\n",
      "mem_used 196373504\n",
      "[65000]: pnf_loss=0.0412 structure_loss=0.8047 structure_null=0.0138 structure_real=0.7909, steps/sec=2.54750\n",
      "1.102661209837987\n",
      "epoch 52\n",
      "mem_used 197193728\n",
      "[66000]: pnf_loss=0.0537 structure_loss=0.8909 structure_null=0.0164 structure_real=0.8745, steps/sec=3.57982\n",
      "1.155308594025156\n",
      "epoch 53\n",
      "mem_used 196796416\n",
      "[67000]: pnf_loss=0.0226 structure_loss=0.9772 structure_null=0.0143 structure_real=0.9629, steps/sec=6.01779\n",
      "1.1164923893744838\n",
      "epoch 54\n",
      "mem_used 195937280\n",
      "[68000]: pnf_loss=0.0604 structure_loss=1.5290 structure_null=0.0172 structure_real=1.5118, steps/sec=18.62613\n",
      "1.1276044088308927\n",
      "[69000]: pnf_loss=0.0152 structure_loss=0.9545 structure_null=0.0148 structure_real=0.9397, steps/sec=2.27215\n",
      "1.0974933931827544\n",
      "epoch 55\n",
      "mem_used 197254144\n",
      "[70000]: pnf_loss=0.0177 structure_loss=1.0001 structure_null=0.0164 structure_real=0.9837, steps/sec=2.63053\n",
      "1.1626544487958699\n",
      "eval Eval_Direc/gu_null_01D_04M_2024Y/01D_04M_2024Y_00h_20m_09s/step_70000\n",
      "[12]: pnf_loss=0.0644 structure_loss=0.9492 structure_null=0.0159 structure_real=0.9333\n",
      "eval_loss 2.200461 12\n",
      "epoch 56\n",
      "mem_used 196402688\n",
      "[71000]: pnf_loss=0.0049 structure_loss=1.1783 structure_null=0.0180 structure_real=1.1603, steps/sec=3.72597\n",
      "1.107975263834784\n",
      "epoch 57\n",
      "mem_used 196723200\n",
      "[72000]: pnf_loss=0.0693 structure_loss=0.9208 structure_null=0.0162 structure_real=0.9046, steps/sec=6.49716\n",
      "1.0932710544336215\n",
      "epoch 58\n",
      "mem_used 196111872\n",
      "[73000]: pnf_loss=0.0098 structure_loss=0.9402 structure_null=0.0122 structure_real=0.9280, steps/sec=24.00337\n",
      "1.119505007850363\n",
      "[74000]: pnf_loss=0.0040 structure_loss=0.9653 structure_null=0.0153 structure_real=0.9500, steps/sec=2.28061\n",
      "1.1110178750753403\n",
      "epoch 59\n",
      "mem_used 196229120\n",
      "[75000]: pnf_loss=0.0209 structure_loss=0.8461 structure_null=0.0152 structure_real=0.8309, steps/sec=2.71925\n",
      "1.1262738528787164\n",
      "epoch 60\n",
      "mem_used 198227456\n",
      "[76000]: pnf_loss=0.0343 structure_loss=0.8646 structure_null=0.0152 structure_real=0.8495, steps/sec=3.93048\n",
      "1.097976532681235\n",
      "epoch 61\n",
      "mem_used 198216192\n",
      "[77000]: pnf_loss=0.0947 structure_loss=1.0206 structure_null=0.0176 structure_real=1.0030, steps/sec=7.07147\n",
      "1.0600356199423964\n",
      "epoch 62\n",
      "mem_used 196172800\n",
      "[78000]: pnf_loss=0.0061 structure_loss=1.0966 structure_null=0.0155 structure_real=1.0811, steps/sec=34.56646\n",
      "1.0686228175957997\n",
      "[79000]: pnf_loss=0.0050 structure_loss=0.9767 structure_null=0.0148 structure_real=0.9619, steps/sec=2.28091\n",
      "1.1074479328989983\n",
      "epoch 63\n",
      "mem_used 196955136\n",
      "[80000]: pnf_loss=0.0373 structure_loss=0.8894 structure_null=0.0161 structure_real=0.8733, steps/sec=2.80773\n",
      "1.0851945159461795\n",
      "eval Eval_Direc/gu_null_01D_04M_2024Y/01D_04M_2024Y_00h_20m_09s/step_80000\n",
      "[12]: pnf_loss=0.1082 structure_loss=1.0734 structure_null=0.0168 structure_real=1.0566\n",
      "eval_loss 1.9992489 12\n",
      "epoch 64\n",
      "mem_used 196838912\n",
      "[81000]: pnf_loss=0.0500 structure_loss=0.9315 structure_null=0.0139 structure_real=0.9176, steps/sec=4.14368\n",
      "1.1440736075890237\n",
      "epoch 65\n",
      "mem_used 198277120\n",
      "[82000]: pnf_loss=0.0131 structure_loss=0.9909 structure_null=0.0142 structure_real=0.9767, steps/sec=7.74418\n",
      "1.0585208571563334\n",
      "epoch 66\n",
      "mem_used 196080128\n",
      "[83000]: pnf_loss=0.0071 structure_loss=1.5888 structure_null=0.0167 structure_real=1.5721, steps/sec=60.20155\n",
      "1.1466400419410907\n",
      "[84000]: pnf_loss=0.1622 structure_loss=1.3798 structure_null=0.0151 structure_real=1.3646, steps/sec=2.28425\n",
      "1.1154895223379135\n",
      "epoch 67\n",
      "mem_used 196417024\n",
      "[85000]: pnf_loss=0.0330 structure_loss=0.8680 structure_null=0.0160 structure_real=0.8519, steps/sec=2.92150\n",
      "1.06564020767102\n",
      "epoch 68\n",
      "mem_used 198434304\n",
      "[86000]: pnf_loss=0.0993 structure_loss=1.4903 structure_null=0.0159 structure_real=1.4744, steps/sec=4.34673\n",
      "1.1083814715611116\n",
      "epoch 69\n",
      "mem_used 196096000\n",
      "[87000]: pnf_loss=0.0111 structure_loss=0.9864 structure_null=0.0137 structure_real=0.9728, steps/sec=8.49670\n",
      "1.08522753679797\n",
      "epoch 70\n",
      "mem_used 196360704\n",
      "[88000]: pnf_loss=0.0547 structure_loss=0.9530 structure_null=0.0157 structure_real=0.9373, steps/sec=226.23770\n",
      "1.0181895434856414\n",
      "[89000]: pnf_loss=0.0163 structure_loss=0.9075 structure_null=0.0158 structure_real=0.8917, steps/sec=2.27504\n",
      "1.098803404390812\n",
      "epoch 71\n",
      "mem_used 197319168\n",
      "[90000]: pnf_loss=0.0315 structure_loss=1.7692 structure_null=0.0166 structure_real=1.7526, steps/sec=3.03167\n",
      "1.1029758406666963\n",
      "eval Eval_Direc/gu_null_01D_04M_2024Y/01D_04M_2024Y_00h_20m_09s/step_90000\n",
      "[12]: pnf_loss=0.0246 structure_loss=0.9948 structure_null=0.0158 structure_real=0.9789\n",
      "eval_loss 2.1689208 12\n",
      "epoch 72\n",
      "mem_used 197096960\n",
      "[91000]: pnf_loss=0.0485 structure_loss=0.9732 structure_null=0.0151 structure_real=0.9581, steps/sec=4.58763\n",
      "1.1545571711274885\n",
      "epoch 73\n",
      "mem_used 196373504\n",
      "[92000]: pnf_loss=0.0271 structure_loss=1.1277 structure_null=0.0156 structure_real=1.1120, steps/sec=9.47239\n",
      "1.0478955517254116\n",
      "[93000]: pnf_loss=0.0565 structure_loss=1.0338 structure_null=0.0153 structure_real=1.0185, steps/sec=2.27460\n",
      "1.0811973004341124\n",
      "epoch 74\n",
      "mem_used 196132352\n",
      "[94000]: pnf_loss=0.0425 structure_loss=0.8738 structure_null=0.0172 structure_real=0.8566, steps/sec=2.32650\n",
      "1.0647664663747956\n",
      "epoch 75\n",
      "mem_used 196374016\n",
      "[95000]: pnf_loss=0.0303 structure_loss=0.9726 structure_null=0.0145 structure_real=0.9581, steps/sec=3.14809\n",
      "1.0765283273006307\n",
      "epoch 76\n",
      "mem_used 196406272\n",
      "[96000]: pnf_loss=0.0181 structure_loss=0.9582 structure_null=0.0162 structure_real=0.9420, steps/sec=4.84168\n",
      "1.0811389237642288\n",
      "epoch 77\n",
      "mem_used 197090816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[97000]: pnf_loss=0.1815 structure_loss=1.4976 structure_null=0.0171 structure_real=1.4805, steps/sec=10.73258\n",
      "1.062409329470865\n",
      "[98000]: pnf_loss=0.0159 structure_loss=0.9219 structure_null=0.0188 structure_real=0.9030, steps/sec=2.27601\n",
      "1.0728412869572639\n",
      "epoch 78\n",
      "mem_used 196739072\n",
      "[99000]: pnf_loss=0.0418 structure_loss=0.9321 structure_null=0.0173 structure_real=0.9149, steps/sec=2.39268\n",
      "1.0509303537054882\n",
      "epoch 79\n",
      "mem_used 197154304\n",
      "[100000]: pnf_loss=0.0275 structure_loss=0.8685 structure_null=0.0136 structure_real=0.8549, steps/sec=3.26025\n",
      "1.0556270132961028\n",
      "eval Eval_Direc/gu_null_01D_04M_2024Y/01D_04M_2024Y_00h_20m_09s/step_100000\n",
      "[12]: pnf_loss=0.0516 structure_loss=0.9482 structure_null=0.0152 structure_real=0.9330\n",
      "eval_loss 2.1908886 12\n",
      "epoch 80\n",
      "mem_used 195325440\n",
      "[101000]: pnf_loss=0.0072 structure_loss=0.8929 structure_null=0.0147 structure_real=0.8782, steps/sec=5.18464\n",
      "1.0273501550609416\n",
      "epoch 81\n",
      "mem_used 197421056\n",
      "[102000]: pnf_loss=0.0561 structure_loss=0.8540 structure_null=0.0176 structure_real=0.8364, steps/sec=12.42966\n",
      "0.999033210381784\n",
      "[103000]: pnf_loss=0.0026 structure_loss=0.9580 structure_null=0.0145 structure_real=0.9435, steps/sec=2.27971\n",
      "1.0412738659381866\n",
      "epoch 82\n",
      "mem_used 196399616\n",
      "[104000]: pnf_loss=0.0138 structure_loss=0.9265 structure_null=0.0138 structure_real=0.9127, steps/sec=2.46420\n",
      "1.0371985439347913\n",
      "epoch 83\n",
      "mem_used 196225024\n",
      "[105000]: pnf_loss=0.0254 structure_loss=1.0325 structure_null=0.0162 structure_real=1.0163, steps/sec=3.42099\n",
      "1.0429713643185226\n",
      "epoch 84\n",
      "mem_used 195685376\n",
      "[106000]: pnf_loss=0.0840 structure_loss=2.3886 structure_null=0.0176 structure_real=2.3710, steps/sec=5.55524\n",
      "1.0433645542096166\n",
      "epoch 85\n",
      "mem_used 195900416\n",
      "[107000]: pnf_loss=0.0320 structure_loss=1.0646 structure_null=0.0172 structure_real=1.0474, steps/sec=14.80416\n",
      "1.0344849574950434\n",
      "[108000]: pnf_loss=0.0387 structure_loss=0.8555 structure_null=0.0149 structure_real=0.8406, steps/sec=2.27864\n",
      "1.0275144590735434\n",
      "epoch 86\n",
      "mem_used 195593216\n",
      "[109000]: pnf_loss=0.0467 structure_loss=0.8086 structure_null=0.0145 structure_real=0.7942, steps/sec=2.53777\n",
      "1.0493575383002616\n",
      "epoch 87\n",
      "mem_used 196417536\n",
      "[110000]: pnf_loss=0.0193 structure_loss=0.8329 structure_null=0.0151 structure_real=0.8177, steps/sec=3.56463\n",
      "1.0896564113926404\n",
      "eval Eval_Direc/gu_null_01D_04M_2024Y/01D_04M_2024Y_00h_20m_09s/step_110000\n",
      "[12]: pnf_loss=0.0601 structure_loss=0.9125 structure_null=0.0145 structure_real=0.8980\n",
      "eval_loss 1.8810018 12\n",
      "epoch 88\n",
      "mem_used 196839936\n",
      "[111000]: pnf_loss=0.0417 structure_loss=0.8674 structure_null=0.0140 structure_real=0.8535, steps/sec=5.92162\n",
      "1.0154179205807548\n",
      "epoch 89\n",
      "mem_used 195908096\n",
      "[112000]: pnf_loss=0.0700 structure_loss=0.9175 structure_null=0.0145 structure_real=0.9030, steps/sec=17.90729\n",
      "0.9995543121352909\n",
      "[113000]: pnf_loss=0.0033 structure_loss=1.2033 structure_null=0.0178 structure_real=1.1854, steps/sec=2.28234\n",
      "1.03431552618742\n",
      "epoch 90\n",
      "mem_used 196102144\n",
      "[114000]: pnf_loss=0.0495 structure_loss=0.8626 structure_null=0.0138 structure_real=0.8488, steps/sec=2.61937\n",
      "0.9927676118653396\n",
      "epoch 91\n",
      "mem_used 197423616\n",
      "[115000]: pnf_loss=0.0124 structure_loss=0.8286 structure_null=0.0144 structure_real=0.8142, steps/sec=3.72735\n",
      "1.0277807115536142\n",
      "epoch 92\n",
      "mem_used 196840960\n",
      "[116000]: pnf_loss=0.0012 structure_loss=0.9379 structure_null=0.0135 structure_real=0.9243, steps/sec=6.42039\n",
      "1.0802857132440202\n",
      "epoch 93\n",
      "mem_used 196222464\n",
      "[117000]: pnf_loss=0.0149 structure_loss=0.8512 structure_null=0.0121 structure_real=0.8391, steps/sec=22.54763\n",
      "1.046389715840118\n",
      "[118000]: pnf_loss=0.0144 structure_loss=0.9434 structure_null=0.0115 structure_real=0.9318, steps/sec=2.28156\n",
      "0.9822931187748909\n",
      "epoch 94\n",
      "mem_used 196303872\n",
      "[119000]: pnf_loss=0.0140 structure_loss=0.8729 structure_null=0.0100 structure_real=0.8629, steps/sec=2.70350\n",
      "0.9901081788709781\n",
      "epoch 95\n",
      "mem_used 196365312\n",
      "[120000]: pnf_loss=0.0059 structure_loss=0.8309 structure_null=0.0141 structure_real=0.8167, steps/sec=3.89926\n",
      "0.9602810209632939\n",
      "eval Eval_Direc/gu_null_01D_04M_2024Y/01D_04M_2024Y_00h_20m_09s/step_120000\n",
      "[12]: pnf_loss=0.0031 structure_loss=0.8862 structure_null=0.0158 structure_real=0.8704\n",
      "eval_loss 1.7391372 12\n",
      "epoch 96\n",
      "mem_used 195385856\n",
      "[121000]: pnf_loss=0.0092 structure_loss=0.9209 structure_null=0.0160 structure_real=0.9049, steps/sec=6.93306\n",
      "0.9701720238095377\n",
      "epoch 97\n",
      "mem_used 197438464\n",
      "[122000]: pnf_loss=0.0281 structure_loss=0.8376 structure_null=0.0147 structure_real=0.8229, steps/sec=32.04199\n",
      "0.9204977247076975\n",
      "[123000]: pnf_loss=0.0284 structure_loss=0.8223 structure_null=0.0174 structure_real=0.8049, steps/sec=2.27403\n",
      "0.9596472699642181\n",
      "epoch 98\n",
      "mem_used 195896832\n",
      "[124000]: pnf_loss=0.0508 structure_loss=0.8253 structure_null=0.0187 structure_real=0.8065, steps/sec=2.79783\n",
      "0.9392562174679899\n",
      "epoch 99\n",
      "mem_used 195831808\n",
      "[125000]: pnf_loss=0.0134 structure_loss=0.8781 structure_null=0.0156 structure_real=0.8625, steps/sec=4.09870\n",
      "0.9539122423106935\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp.start_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6518214f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f538488",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(self, train_loader, valid_loader, device, return_logs=False):\n",
    "        log_lossses = defaultdict(list)\n",
    "        global_logs = []\n",
    "        log_time = time.time()\n",
    "        step_time = time.time()\n",
    "        losskeeper = []\n",
    "        for train_feats in train_loader:\n",
    "            train_feats = tree.map_structure(lambda x: x.to(device), train_feats)\n",
    "            loss, aux_data = self.update_fn(train_feats)\n",
    "            losskeeper.append(float(loss.detach().cpu()))\n",
    "            if return_logs:\n",
    "                global_logs.append(loss)\n",
    "            for k,v in aux_data.items():\n",
    "                log_lossses[k].append(du.move_to_np(v))\n",
    "            self.trained_steps += 1\n",
    "\n",
    "            # Logging to terminal\n",
    "            if self.trained_steps == 1 or self.trained_steps % self._exp_conf.log_freq == 0:\n",
    "                elapsed_time = time.time() - log_time\n",
    "                log_time = time.time()\n",
    "                step_per_sec = self._exp_conf.log_freq / elapsed_time\n",
    "                rolling_losses = tree.map_structure(np.mean, log_lossses)\n",
    "                loss_log = ' '.join([\n",
    "                    f'{k}={v[0]:.4f}'\n",
    "                    for k,v in rolling_losses.items() if 'batch' not in k\n",
    "                ])\n",
    "                self._log.info(\n",
    "                    f'[{self.trained_steps}]: {loss_log}, steps/sec={step_per_sec:.5f}')\n",
    "                log_lossses = defaultdict(list)\n",
    "                \n",
    "                print(f'[{self.trained_steps}]: {loss_log}, steps/sec={step_per_sec:.5f}')\n",
    "                print(np.mean(losskeeper[-1000:]))\n",
    "\n",
    "            # Take checkpoint\n",
    "            if self._exp_conf.ckpt_dir is not None and (\n",
    "                    (self.trained_steps % self._exp_conf.ckpt_freq) == 0\n",
    "                    or (self._exp_conf.early_ckpt and self.trained_steps == 2)\n",
    "                ):\n",
    "                ckpt_path = os.path.join(\n",
    "                    self._exp_conf.ckpt_dir, f'step_{self.trained_steps}.pth')\n",
    "                du.write_checkpoint(\n",
    "                    ckpt_path,\n",
    "                    copy.deepcopy(self.model.state_dict()),\n",
    "                    self._conf,\n",
    "                    copy.deepcopy(self._optimizer.state_dict()),\n",
    "                    self.trained_epochs,\n",
    "                    self.trained_steps,\n",
    "                    logger=self._log,\n",
    "                    use_torch=True\n",
    "                )\n",
    "                \n",
    "\n",
    "                # Run evaluation\n",
    "                self._log.info(f'Running evaluation of {ckpt_path}')\n",
    "                start_time = time.time()\n",
    "                self._exp_conf.eval_dir = 'output/'\n",
    "                eval_dir = os.path.join(\n",
    "                    self._exp_conf.eval_dir, f'step_{self.trained_steps}')\n",
    "                print(eval_dir)\n",
    "                os.makedirs(eval_dir, exist_ok=True)\n",
    "                ckpt_metrics = self.eval_fn(\n",
    "                    eval_dir, valid_loader, device,\n",
    "                    noise_scale=self._exp_conf.noise_scale\n",
    "                )\n",
    "                eval_time = time.time() - start_time\n",
    "                self._log.info(f'Finished evaluation in {eval_time:.2f}s')\n",
    "            else:\n",
    "                ckpt_metrics = None\n",
    "                eval_time = None\n",
    "\n",
    "\n",
    "            if torch.isnan(loss):\n",
    "                if self._use_wandb:\n",
    "                    wandb.alert(\n",
    "                        title=\"Encountered NaN loss\",\n",
    "                        text=f\"Loss NaN after {self.trained_epochs} epochs, {self.trained_steps} steps\"\n",
    "                    )\n",
    "                raise Exception(f'NaN encountered')\n",
    "\n",
    "        if return_logs:\n",
    "            return global_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79ced54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b717659a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48458147",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def model_step_null(noised_dict, batched_t, graph_maker, graph_unet, train=True):\n",
    "    #prep coordinates for output display from and comparison via FAPE\n",
    "    \n",
    "    CA_t  = noised_dict['bb_shifted']['CA'].reshape(B, L, 3).to(device)\n",
    "    NC_t = CA_t + noised_dict['bb_shifted']['N_CA'].reshape(B, L, 3).to(device)#not mult by bond distance, seems to help?\n",
    "    CC_t = CA_t + noised_dict['bb_shifted']['C_CA'].reshape(B, L, 3).to(device)#not mult \n",
    "    true =  torch.cat((NC_t,CA_t,CC_t),dim=2).reshape(B,L,3,3)\n",
    "\n",
    "    CA_n  = noised_dict['bb_noised']['CA'].reshape(B, L, 3).to(device)\n",
    "    NC_n = CA_n + noised_dict['bb_noised']['N_CA'].reshape(B, L, 3).to(device)\n",
    "    CC_n = CA_n + noised_dict['bb_noised']['C_CA'].reshape(B, L, 3).to(device)\n",
    "    noise_xyz =  torch.cat((NC_n,CA_n,CC_n),dim=2).reshape(B,L,3,3)\n",
    "\n",
    "    #prepare graphs\n",
    "    feat_dict = graph_maker.prep_for_network(noised_dict, cuda=True)\n",
    "    out =graph_unet(feat_dict,batched_t)\n",
    "    \n",
    "    \n",
    "    #FAPE Loss for the prediction\n",
    "    CA_p = out['1'][:,0,:].reshape(B, L, 3)+CA_n #translation of Calpha\n",
    "    Qs = out['1'][:,1,:] # rotation , convert from x,y,z (Quat) to rotate input vectors\n",
    "    Qs = Qs.unsqueeze(1).repeat((1,2,1))\n",
    "    Qs = torch.cat((torch.ones((B*L,2,1),device=Qs.device),Qs),dim=-1).reshape(B,L,2,4)\n",
    "    Qs = normQ(Qs)\n",
    "    Rs = Qs2Rs(Qs)\n",
    "    N_C_to_Rot = torch.cat((noised_dict['bb_noised']['N_CA'].reshape(B, L, 3).to(device),\n",
    "                            noised_dict['bb_noised']['C_CA'].reshape(B, L, 3).to(device)),dim=2).reshape(B,L,2,1,3)\n",
    "    rot_vecs = einsum('bnkij,bnkhj->bnki',Rs, N_C_to_Rot)\n",
    "    NC_p = CA_p + rot_vecs[:,:,0,:]*N_CA_dist #comparable but seems better not have it for true, but have it for pred\n",
    "    CC_p = CA_p + rot_vecs[:,:,1,:]*C_CA_dist #maybe this helep prevent \n",
    "\n",
    "    pred = torch.cat((NC_p,CA_p,CC_p),dim=2).reshape(B,L,3,3)\n",
    "    \n",
    "    #divide loss by real and null nodes\n",
    "    \n",
    "    fp, lp  = convert_pV_to_points(noised_dict)\n",
    "\n",
    "    real_mask = noised_dict['real_nodes_mask'].to('cuda')\n",
    "    score_scales = noised_dict['score_scales'].to('cuda')\n",
    "\n",
    "    lr, lr_d = FAPE_loss_real(pred, true, score_scales, real_mask,  d_clamp=10.0, d_clamp_inter=30.0,\n",
    "                   A=10.0, gamma=1.0, eps=1e-6)\n",
    "    \n",
    "    ln, ln_d = FAPE_loss_null(pred, fp, lp, real_mask, score_scales,  d_clamp=10.0,\n",
    "                       d_clamp_inter=30.0, A=10.0, gamma=1.0, eps=1e-6)\n",
    "    \n",
    "    structure_loss = lr*score_weights['3D_real']+ln*score_weights['3D_null']\n",
    "    \n",
    "    #score for node feats determining whether node is real or fake\n",
    "    nf_pred = out['0']\n",
    "\n",
    "    nf_feat_dim = noised_dict['real_nodes_noise'].shape[-1]\n",
    "    nf_true = torch.ones(noised_dict['real_nodes_mask'].shape+(nf_feat_dim,) + (1,),\n",
    "                         dtype=torch.float,device=device)\n",
    "\n",
    "    nf_real_mask_mult = real_mask.unsqueeze(-1).unsqueeze(-1).to(device)\n",
    "    nf_true = nf_true*nf_real_mask_mult\n",
    "\n",
    "    nf_pred = nf_pred.reshape(B,-1,nf_feat_dim)\n",
    "    pred_nf_loss = torch.sum(torch.abs(nf_true.squeeze()-nf_pred),dim=-1) #absolute value loss\n",
    "    pred_nf_loss = pred_nf_loss.to(device)\n",
    "    \n",
    "    ss_scales = to_cuda(noised_dict['score_scales'])[:,None,None]\n",
    "    pnfloss = (torch.sum((pred_nf_loss*ss_scales/L)))*score_weights['nf_real']\n",
    "    \n",
    "    final_loss = structure_loss + pnfloss\n",
    "    \n",
    "    \n",
    "    return final_loss, pnfloss.detach().cpu(), structure_loss.detach().cpu(), lr.detach().cpu(), ln.detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2999b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(self, train_loader, valid_loader, device, return_logs=False):\n",
    "        log_lossses = defaultdict(list)\n",
    "        global_logs = []\n",
    "        log_time = time.time()\n",
    "        step_time = time.time()\n",
    "        losskeeper = []\n",
    "        for train_feats in train_loader:\n",
    "            train_feats = tree.map_structure(lambda x: x.to(device), train_feats)\n",
    "            loss, aux_data = self.update_fn(train_feats)\n",
    "            losskeeper.append(float(loss.detach().cpu()))\n",
    "            if return_logs:\n",
    "                global_logs.append(loss)\n",
    "            for k,v in aux_data.items():\n",
    "                log_lossses[k].append(du.move_to_np(v))\n",
    "            self.trained_steps += 1\n",
    "\n",
    "            # Logging to terminal\n",
    "            if self.trained_steps == 1 or self.trained_steps % self._exp_conf.log_freq == 0:\n",
    "                elapsed_time = time.time() - log_time\n",
    "                log_time = time.time()\n",
    "                step_per_sec = self._exp_conf.log_freq / elapsed_time\n",
    "                rolling_losses = tree.map_structure(np.mean, log_lossses)\n",
    "                loss_log = ' '.join([\n",
    "                    f'{k}={v[0]:.4f}'\n",
    "                    for k,v in rolling_losses.items() if 'batch' not in k\n",
    "                ])\n",
    "                self._log.info(\n",
    "                    f'[{self.trained_steps}]: {loss_log}, steps/sec={step_per_sec:.5f}')\n",
    "                log_lossses = defaultdict(list)\n",
    "                \n",
    "                print(f'[{self.trained_steps}]: {loss_log}, steps/sec={step_per_sec:.5f}')\n",
    "                print(np.mean(losskeeper[-1000:]))\n",
    "\n",
    "            # Take checkpoint\n",
    "            if self._exp_conf.ckpt_dir is not None and (\n",
    "                    (self.trained_steps % self._exp_conf.ckpt_freq) == 0\n",
    "                    or (self._exp_conf.early_ckpt and self.trained_steps == 2)\n",
    "                ):\n",
    "                ckpt_path = os.path.join(\n",
    "                    self._exp_conf.ckpt_dir, f'step_{self.trained_steps}.pth')\n",
    "                du.write_checkpoint(\n",
    "                    ckpt_path,\n",
    "                    copy.deepcopy(self.model.state_dict()),\n",
    "                    self._conf,\n",
    "                    copy.deepcopy(self._optimizer.state_dict()),\n",
    "                    self.trained_epochs,\n",
    "                    self.trained_steps,\n",
    "                    logger=self._log,\n",
    "                    use_torch=True\n",
    "                )\n",
    "                \n",
    "\n",
    "                # Run evaluation\n",
    "                self._log.info(f'Running evaluation of {ckpt_path}')\n",
    "                start_time = time.time()\n",
    "                self._exp_conf.eval_dir = 'output/'\n",
    "                eval_dir = os.path.join(\n",
    "                    self._exp_conf.eval_dir, f'step_{self.trained_steps}')\n",
    "                print(eval_dir)\n",
    "                os.makedirs(eval_dir, exist_ok=True)\n",
    "                ckpt_metrics = self.eval_fn(\n",
    "                    eval_dir, valid_loader, device,\n",
    "                    noise_scale=self._exp_conf.noise_scale\n",
    "                )\n",
    "                eval_time = time.time() - start_time\n",
    "                self._log.info(f'Finished evaluation in {eval_time:.2f}s')\n",
    "            else:\n",
    "                ckpt_metrics = None\n",
    "                eval_time = None\n",
    "\n",
    "\n",
    "            if torch.isnan(loss):\n",
    "                if self._use_wandb:\n",
    "                    wandb.alert(\n",
    "                        title=\"Encountered NaN loss\",\n",
    "                        text=f\"Loss NaN after {self.trained_epochs} epochs, {self.trained_steps} steps\"\n",
    "                    )\n",
    "                raise Exception(f'NaN encountered')\n",
    "\n",
    "        if return_logs:\n",
    "            return global_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75573c13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62506e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    \n",
    "# class EMA(nn.Module):\n",
    "#     def __init__(self, mu):\n",
    "#         super(EMA, self).__init__()\n",
    "#         self.mu = mu\n",
    "#         self.shadow = {}\n",
    "\n",
    "#     def register(self, name, val):\n",
    "#         self.shadow[name] = val.clone()\n",
    "\n",
    "#     def forward(self, name, x):\n",
    "#         assert name in self.shadow\n",
    "#         new_average = (1.0 - self.mu) * x + self.mu * self.shadow[name]\n",
    "#         self.shadow[name] = new_average.clone()\n",
    "#         return new_average\n",
    "\n",
    "class Experiment:\n",
    "\n",
    "    def __init__(self,*,config_path= 'config/base_static_4H_norot.yaml', simple=True, limit=5028):\n",
    "        \"\"\"Initialize experiment.\n",
    "        Currently does not incorporate psi pred (just taken from starting points)\n",
    "        Args:\n",
    "            exp_cfg: Experiment configuration.\n",
    "        \"\"\"\n",
    "        with open(config_path, 'r') as file:\n",
    "            config = yaml.safe_load(file)\n",
    "        conf = Struct(config)\n",
    "        \n",
    "        logging.basicConfig(filename='test.log', level=logging.INFO)\n",
    "        self._log = logging.getLogger(__name__)\n",
    "        \n",
    "        self.simple = simple\n",
    "        self.limit = limit\n",
    "        \n",
    "        self._use_wandb = False\n",
    "        \n",
    "        self.rot=False\n",
    "        self.bbloss=False\n",
    "        self.device='cuda'\n",
    "        \n",
    "        # Configs\n",
    "        self._conf = conf\n",
    "        self._exp_conf = conf.experiment\n",
    "        self._diff_conf = conf.diffuser\n",
    "        self._model_conf = conf.model\n",
    "        self._data_conf = conf.data\n",
    "        self.ipa_conf = conf.model.ipa\n",
    "        \n",
    "        \n",
    "        self.B=self._exp_conf.batch_size\n",
    "        L=65 #HARDDDCODE\n",
    "        \n",
    "        self._diffuser = se3_diffuser.SE3Diffuser(self._diff_conf)\n",
    "        self._model = ScoreNetwork(\n",
    "            self._model_conf, self._diffuser,B=self.B,L=L)\n",
    "        num_parameters = sum(p.numel() for p in self._model.parameters())\n",
    "        self._exp_conf.num_parameters = num_parameters\n",
    "        self._log.info(f'Number of model parameters {num_parameters}')\n",
    "#         self._optimizer = EMA(0.980)\n",
    "#         for name, param in self._model.named_parameters():\n",
    "#             if param.requires_grad:\n",
    "#                 self._optimizer.register(name, param.data)\n",
    "        \n",
    "        \n",
    "        self._optimizer = torch.optim.Adam(\n",
    "            self._model.score_model.parameters(), lr=self._exp_conf.learning_rate)\n",
    "        dt_string = datetime.now().strftime(\"%dD_%mM_%YY_%Hh_%Mm_%Ss\")\n",
    "        dt_string_short = datetime.now().strftime(\"%dD_%mM_%YY\")\n",
    "        self._exp_conf.ckpt_dir = 'log/'\n",
    "        if self._exp_conf.ckpt_dir is not None:\n",
    "            # Set-up checkpoint location\n",
    "            ckpt_dir = os.path.join(\n",
    "                self._exp_conf.ckpt_dir,\n",
    "                self._exp_conf.name,\n",
    "                dt_string)\n",
    "            if not os.path.exists(ckpt_dir):\n",
    "                os.makedirs(ckpt_dir, exist_ok=True)\n",
    "            self._exp_conf.ckpt_dir = ckpt_dir\n",
    "            self._log.info(f'Checkpoints saved to: {ckpt_dir}')\n",
    "        else:  \n",
    "            self._log.info('Checkpoint not being saved.')\n",
    "            \n",
    "        if self._exp_conf.eval_dir is not None :\n",
    "            eval_dir = os.path.join(\n",
    "                self._exp_conf.eval_dir,\n",
    "                self._exp_conf.name,\n",
    "                dt_string)\n",
    "            self._exp_conf.eval_dir = eval_dir\n",
    "            self._log.info(f'Evaluation saved to: {eval_dir}')\n",
    "        else:\n",
    "            self._exp_conf.eval_dir = os.devnull\n",
    "            self._log.info(f'Evaluation will not be saved.')\n",
    "    #         self._aux_data_history = deque(maxlen=100)\n",
    "\n",
    "        # Warm starting\n",
    "        ckpt_model = None\n",
    "        ckpt_opt = None\n",
    "        self.trained_epochs = 0\n",
    "        self.trained_steps = 0\n",
    "\n",
    "        # Initialize experiment objects\n",
    "\n",
    "        if ckpt_model is not None:\n",
    "            ckpt_model = {k.replace('module.', ''):v for k,v in ckpt_model.items()}\n",
    "            self._model.load_state_dict(ckpt_model, strict=True)\n",
    "\n",
    "        num_parameters = sum(p.numel() for p in self._model.parameters())\n",
    "        self._exp_conf.num_parameters = num_parameters\n",
    "        self._log.info(f'Number of model parameters {num_parameters}')\n",
    "        self._optimizer = torch.optim.Adam(\n",
    "            self._model.parameters(), lr=self._exp_conf.learning_rate)\n",
    "        if ckpt_opt is not None:\n",
    "            self._optimizer.load_state_dict(ckpt_opt)\n",
    "\n",
    "        dt_string = datetime.now().strftime(\"%dD_%mM_%YY_%Hh_%Mm_%Ss\")\n",
    "        if self._exp_conf.ckpt_dir is not None:\n",
    "            # Set-up checkpoint location\n",
    "            ckpt_dir = os.path.join(\n",
    "                self._exp_conf.ckpt_dir,\n",
    "                self._exp_conf.name,\n",
    "                dt_string)\n",
    "            if not os.path.exists(ckpt_dir):\n",
    "                os.makedirs(ckpt_dir, exist_ok=True)\n",
    "            self._exp_conf.ckpt_dir = ckpt_dir\n",
    "            self._log.info(f'Checkpoints saved to: {ckpt_dir}')\n",
    "        else:  \n",
    "            self._log.info('Checkpoint not being saved.')\n",
    "        if self._exp_conf.eval_dir is not None :\n",
    "            eval_dir = os.path.join(\n",
    "                self._exp_conf.eval_dir,\n",
    "                self._exp_conf.name,\n",
    "                dt_string)\n",
    "            self._exp_conf.eval_dir = eval_dir\n",
    "            self._log.info(f'Evaluation saved to: {eval_dir}')\n",
    "        else:\n",
    "            self._exp_conf.eval_dir = os.devnull\n",
    "            self._log.info(f'Evaluation will not be saved.')\n",
    "        self._aux_data_history = deque(maxlen=100)\n",
    "        \n",
    "    @property\n",
    "    def diffuser(self):\n",
    "        return self._diffuser\n",
    "\n",
    "    @property\n",
    "    def model(self):\n",
    "        return self._model\n",
    "\n",
    "    @property\n",
    "    def conf(self):\n",
    "        return self._conf\n",
    "    \n",
    "    def create_dataset(self):\n",
    "        self._exp_conf.eval_batch_size=self.B\n",
    "        #valid limit set\n",
    "        num_workers = self._exp_conf.num_loader_workers\n",
    "        train_dataset = pdb_data_loader.PdbDataset(\n",
    "                    data_conf=self._data_conf,\n",
    "                    diffuser=self._diffuser,\n",
    "                    is_training=True,\n",
    "                    simple = self.simple\n",
    "                )\n",
    "        valid_dataset = pdb_data_loader.PdbDataset(\n",
    "                    data_conf=self._data_conf,\n",
    "                    diffuser=self._diffuser,\n",
    "                    is_training=False,\n",
    "                    simple = self.simple\n",
    "                )\n",
    "\n",
    "        train_dataset.csv = train_dataset.csv.iloc[:self.limit]\n",
    "        valid_dataset.csv = valid_dataset.csv.iloc[:256] #valid\n",
    "\n",
    "        valid_sampler=None\n",
    "        train_sampler = pdb_data_loader.TrainSampler(\n",
    "                data_conf=self._data_conf,\n",
    "                dataset=train_dataset,\n",
    "                batch_size=self.B,\n",
    "                sample_mode=self._exp_conf.sample_mode,\n",
    "            )\n",
    "\n",
    "\n",
    "        train_loader = du.create_data_loader(\n",
    "            train_dataset,\n",
    "            sampler=train_sampler,\n",
    "            np_collate=False,\n",
    "            length_batch=False,\n",
    "            batch_size=self.B,\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "            drop_last=False,\n",
    "            max_squared_res=100*2,\n",
    "        )\n",
    "        \n",
    "        # Loaders\n",
    "        \n",
    "        train_loader = du.create_data_loader(\n",
    "            train_dataset,\n",
    "            sampler=train_sampler,\n",
    "            np_collate=False,\n",
    "            length_batch=False,\n",
    "            batch_size=self.B,\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "            drop_last=False,\n",
    "            max_squared_res=self._exp_conf.max_squared_res,\n",
    "        )\n",
    "        \n",
    "        #####TRAIN DATASET again##########\n",
    "        valid_loader = du.create_data_loader(\n",
    "            valid_dataset,\n",
    "            sampler=valid_sampler,\n",
    "            np_collate=False,\n",
    "            length_batch=False,\n",
    "            batch_size=self.B,\n",
    "            shuffle=False,\n",
    "            num_workers=1,\n",
    "            drop_last=False,\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "        return train_loader, train_sampler, valid_loader, valid_sampler\n",
    "    \n",
    "    def start_training(self, return_logs=False):\n",
    "\n",
    "        # GPU mode\n",
    "        if torch.cuda.is_available():\n",
    "            device = f\"cuda\"\n",
    "            self._model = self.model.to(device)\n",
    "            print(f\"Using device: {device}\")\n",
    "            self._log.info(f\"Using device: {device}\")\n",
    "        else:\n",
    "            device = 'cpu'\n",
    "            self._model = self.model.to(device)\n",
    "            self._log.info(f\"Using device: {device}\")\n",
    "\n",
    "        self._model.train()\n",
    "        #removed valid\n",
    "        (train_loader, train_sampler, valid_loader, valid_sampler) = self.create_dataset()\n",
    "\n",
    "        logs = []\n",
    "        print(self._exp_conf.num_epoch)\n",
    "        for epoch in range(self.trained_epochs, self._exp_conf.num_epoch):\n",
    "            if train_sampler is not None:\n",
    "                train_sampler.set_epoch(epoch)\n",
    "            print(epoch)\n",
    "            epoch_log = self.train_epoch(\n",
    "                train_loader,\n",
    "                valid_loader,\n",
    "                device,\n",
    "                return_logs=return_logs\n",
    "            )\n",
    "            if return_logs:\n",
    "                logs.append(epoch_log)\n",
    "\n",
    "        self._log.info('Done')\n",
    "        return logs\n",
    "    \n",
    "    def train_epoch(self, train_loader, valid_loader, device, return_logs=False):\n",
    "        log_lossses = defaultdict(list)\n",
    "        global_logs = []\n",
    "        log_time = time.time()\n",
    "        step_time = time.time()\n",
    "        losskeeper = []\n",
    "        for train_feats in train_loader:\n",
    "            train_feats = tree.map_structure(lambda x: x.to(device), train_feats)\n",
    "            loss, aux_data = self.update_fn(train_feats)\n",
    "            losskeeper.append(float(loss.detach().cpu()))\n",
    "            for k,v in aux_data.items():\n",
    "                log_lossses[k].append(v.detach().cpu())\n",
    "            self.trained_steps += 1\n",
    "\n",
    "            # Logging to terminal\n",
    "            if self.trained_steps == 1 or self.trained_steps % self._exp_conf.log_freq == 0:\n",
    "                elapsed_time = time.time() - log_time\n",
    "                log_time = time.time()\n",
    "                step_per_sec = self._exp_conf.log_freq / elapsed_time\n",
    "                rolling_losses = tree.map_structure(np.mean, log_lossses)\n",
    "                loss_log = ' '.join([\n",
    "                    f'{k}={v[0]:.4f}'\n",
    "                    for k,v in rolling_losses.items() if 'batch' not in k\n",
    "                ])\n",
    "                self._log.info(\n",
    "                    f'[{self.trained_steps}]: {loss_log}, steps/sec={step_per_sec:.5f}')\n",
    "                log_lossses = defaultdict(list)\n",
    "                \n",
    "                print(f'[{self.trained_steps}]: {loss_log}, steps/sec={step_per_sec:.5f}')\n",
    "                print(np.mean(losskeeper[-1000:]))\n",
    "\n",
    "            # Take checkpoint\n",
    "            if self._exp_conf.ckpt_dir is not None and (\n",
    "                    (self.trained_steps % self._exp_conf.ckpt_freq) == 0\n",
    "                    or (self._exp_conf.early_ckpt and self.trained_steps == 2)\n",
    "                ):\n",
    "                ckpt_path = os.path.join(\n",
    "                    self._exp_conf.ckpt_dir, f'step_{self.trained_steps}.pth')\n",
    "                du.write_checkpoint(\n",
    "                    ckpt_path,\n",
    "                    copy.deepcopy(self.model.state_dict()),\n",
    "                    self._conf,\n",
    "                    copy.deepcopy(self._optimizer.state_dict()),\n",
    "                    self.trained_epochs,\n",
    "                    self.trained_steps,\n",
    "                    logger=self._log,\n",
    "                    use_torch=True\n",
    "                )\n",
    "                \n",
    "\n",
    "                # Run evaluation\n",
    "                self._log.info(f'Running evaluation of {ckpt_path}')\n",
    "                start_time = time.time()\n",
    "                self._exp_conf.eval_dir = 'output/'\n",
    "                eval_dir = os.path.join(\n",
    "                    self._exp_conf.eval_dir, f'step_{self.trained_steps}')\n",
    "                print(eval_dir)\n",
    "                os.makedirs(eval_dir, exist_ok=True)\n",
    "                ckpt_metrics = self.eval_fn(\n",
    "                    eval_dir, valid_loader, device,\n",
    "                    noise_scale=self._exp_conf.noise_scale\n",
    "                )\n",
    "                eval_time = time.time() - start_time\n",
    "                self._log.info(f'Finished evaluation in {eval_time:.2f}s')\n",
    "            else:\n",
    "                ckpt_metrics = None\n",
    "                eval_time = None\n",
    "\n",
    "\n",
    "            if torch.isnan(loss):\n",
    "                if self._use_wandb:\n",
    "                    wandb.alert(\n",
    "                        title=\"Encountered NaN loss\",\n",
    "                        text=f\"Loss NaN after {self.trained_epochs} epochs, {self.trained_steps} steps\"\n",
    "                    )\n",
    "                raise Exception(f'NaN encountered')\n",
    "\n",
    "        if return_logs:\n",
    "            return global_logs\n",
    "        \n",
    "    def eval_fn(self, eval_dir, valid_loader, device, min_t=None, num_t=None, noise_scale=1.0):\n",
    "        ckpt_eval_metrics = []\n",
    "        self._exp_conf.eval_batch_size=self.B\n",
    "        for valid_feats, pdb_names in valid_loader:\n",
    "            res_mask = du.move_to_np(valid_feats['res_mask'].bool())\n",
    "            fixed_mask = du.move_to_np(valid_feats['fixed_mask'].bool())\n",
    "            aatype = du.move_to_np(valid_feats['aatype'])\n",
    "            gt_prot = du.move_to_np(valid_feats['atom37_pos'])\n",
    "            batch_size = res_mask.shape[0]\n",
    "            valid_feats = tree.map_structure(\n",
    "                lambda x: x.to(device), valid_feats)\n",
    "            \n",
    "            # Run inference\n",
    "            infer_out = self.inference_fn(\n",
    "                valid_feats, min_t=min_t, num_t=num_t, noise_scale=noise_scale)\n",
    "            final_prot = infer_out['prot_traj'][0]\n",
    "            for i in range(batch_size):\n",
    "                num_res = int(np.sum(res_mask[i]).item())\n",
    "                unpad_fixed_mask = fixed_mask[i][res_mask[i]]\n",
    "                unpad_diffused_mask = 1 - unpad_fixed_mask\n",
    "                unpad_prot = final_prot[i][res_mask[i]]\n",
    "                unpad_gt_prot = gt_prot[i][res_mask[i]]\n",
    "                unpad_gt_aatype = aatype[i][res_mask[i]]\n",
    "                percent_diffused = np.sum(unpad_diffused_mask) / num_res\n",
    "\n",
    "                # Extract argmax predicted aatype\n",
    "                saved_path = au.write_prot_to_pdb(\n",
    "                    unpad_prot,\n",
    "                    os.path.join(\n",
    "                        eval_dir,\n",
    "                        f'len_{num_res}_sample_{i}_diffused_{percent_diffused:.2f}.pdb'\n",
    "                    ),\n",
    "                    no_indexing=True,\n",
    "                    b_factors=np.tile(1 - unpad_fixed_mask[..., None], 37) * 100\n",
    "                )\n",
    "                try:\n",
    "                    sample_metrics = metrics.protein_metrics(\n",
    "                        pdb_path=saved_path,\n",
    "                        atom37_pos=unpad_prot,\n",
    "                        gt_atom37_pos=unpad_gt_prot,\n",
    "                        gt_aatype=unpad_gt_aatype,\n",
    "                        diffuse_mask=unpad_diffused_mask,\n",
    "                         )\n",
    "                except ValueError as e:\n",
    "                    self._log.warning(\n",
    "                        f'Failed evaluation of length {num_res} sample {i}: {e}')\n",
    "                    continue\n",
    "                sample_metrics['step'] = self.trained_steps\n",
    "                sample_metrics['num_res'] = num_res\n",
    "                sample_metrics['fixed_residues'] = np.sum(unpad_fixed_mask)\n",
    "                sample_metrics['diffused_percentage'] = percent_diffused\n",
    "                sample_metrics['sample_path'] = saved_path\n",
    "                sample_metrics['gt_pdb'] = pdb_names[i]\n",
    "                ckpt_eval_metrics.append(sample_metrics)\n",
    "\n",
    "        # Save metrics as CSV.\n",
    "        eval_metrics_csv_path = os.path.join(eval_dir, 'metrics.csv')\n",
    "        ckpt_eval_metrics = pd.DataFrame(ckpt_eval_metrics)\n",
    "        ckpt_eval_metrics.to_csv(eval_metrics_csv_path, index=False)\n",
    "        return ckpt_eval_metrics\n",
    "    \n",
    "    \n",
    "    def _self_conditioning(self, batch):\n",
    "        model_sc = self.model(batch)\n",
    "        batch['sc_ca_t'] = model_sc['rigids'][..., 4:]\n",
    "        return batch\n",
    "    \n",
    "    def loss_fn(self,batch):\n",
    "        \n",
    "        \n",
    "        \n",
    "        model_out = self._model(batch) #score network calls graph maker/gu_net\n",
    "    \n",
    "        #loss\n",
    "        bb_mask = batch['res_mask']\n",
    "        diffuse_mask = 1 - batch['fixed_mask']\n",
    "        loss_mask = bb_mask * diffuse_mask\n",
    "        batch_size, num_res = bb_mask.shape\n",
    "\n",
    "        gt_rot_score = batch['rot_score']\n",
    "        gt_trans_score = batch['trans_score']\n",
    "        rot_score_scaling = batch['rot_score_scaling']\n",
    "        trans_score_scaling = batch['trans_score_scaling']\n",
    "        batch_loss_mask = torch.any(bb_mask, dim=-1)\n",
    "\n",
    "        pred_rot_score = model_out['rot_score'] * diffuse_mask[..., None]\n",
    "        pred_trans_score = model_out['trans_score'] * diffuse_mask[..., None]\n",
    "\n",
    "        # Translation score loss\n",
    "        trans_score_mse = (gt_trans_score - pred_trans_score)**2 * loss_mask[..., None]\n",
    "        trans_score_loss = torch.sum(\n",
    "            trans_score_mse / trans_score_scaling[:, None, None]**2,\n",
    "            dim=(-1, -2)\n",
    "        ) / (loss_mask.sum(dim=-1) + 1e-10)\n",
    "\n",
    "        # Translation x0 loss\n",
    "        gt_trans_x0 = batch['rigids_0'][..., 4:] * self._exp_conf.coordinate_scaling\n",
    "        pred_trans_x0 = model_out['rigids'][..., 4:] * self._exp_conf.coordinate_scaling\n",
    "        trans_x0_loss = torch.sum(\n",
    "            (gt_trans_x0 - pred_trans_x0)**2 * loss_mask[..., None],\n",
    "            dim=(-1, -2)\n",
    "        ) / (loss_mask.sum(dim=-1) + 1e-10)\n",
    "\n",
    "        trans_loss = (\n",
    "            trans_score_loss * (batch['t'] > self._exp_conf.trans_x0_threshold)\n",
    "            + trans_x0_loss * (batch['t'] <= self._exp_conf.trans_x0_threshold)\n",
    "        )\n",
    "        if self.rot:\n",
    "            # Rotation loss\n",
    "            if self._exp_conf.separate_rot_loss:\n",
    "                gt_rot_angle = torch.norm(gt_rot_score, dim=-1, keepdim=True)\n",
    "                gt_rot_axis = gt_rot_score / (gt_rot_angle + 1e-6)\n",
    "\n",
    "                pred_rot_angle = torch.norm(pred_rot_score, dim=-1, keepdim=True)\n",
    "                pred_rot_axis = pred_rot_score / (pred_rot_angle + 1e-6)\n",
    "\n",
    "                # Separate loss on the axis\n",
    "                axis_loss = (gt_rot_axis - pred_rot_axis)**2 * loss_mask[..., None]\n",
    "                axis_loss = torch.sum(\n",
    "                    axis_loss, dim=(-1, -2)\n",
    "                ) / (loss_mask.sum(dim=-1) + 1e-10)\n",
    "\n",
    "                # Separate loss on the angle\n",
    "                angle_loss = (gt_rot_angle - pred_rot_angle)**2 * loss_mask[..., None]\n",
    "                angle_loss = torch.sum(\n",
    "                    angle_loss / rot_score_scaling[:, None, None]**2,\n",
    "                    dim=(-1, -2)\n",
    "                ) / (loss_mask.sum(dim=-1) + 1e-10)\n",
    "                angle_loss *= self._exp_conf.rot_loss_weight\n",
    "                angle_loss *= batch['t'] > self._exp_conf.rot_loss_t_threshold\n",
    "                rot_loss = angle_loss + axis_loss\n",
    "            else:\n",
    "                rot_mse = (gt_rot_score - pred_rot_score)**2 * loss_mask[..., None]\n",
    "                rot_loss = torch.sum(\n",
    "                    rot_mse / rot_score_scaling[:, None, None]**2,\n",
    "                    dim=(-1, -2)\n",
    "                ) / (loss_mask.sum(dim=-1) + 1e-10)\n",
    "                rot_loss *= self._exp_conf.rot_loss_weight\n",
    "                rot_loss *= batch['t'] > self._exp_conf.rot_loss_t_threshold\n",
    "            rot_loss *= int(self._diff_conf.diffuse_rot)\n",
    "            \n",
    "        else:\n",
    "            rot_loss = torch.zeros_like(trans_loss,device=self.device)\n",
    "        \n",
    "\n",
    "        if self.bbloss:\n",
    "            # Backbone atom loss\n",
    "            pred_atom37 = model_out['atom37'][:, :, :5]\n",
    "            gt_rigids = ru.Rigid.from_tensor_7(batch['rigids_0'].type(torch.float32))\n",
    "            gt_psi = batch['torsion_angles_sin_cos'][..., 2, :]\n",
    "            gt_atom37, atom37_mask, _, _ = all_atom.compute_backbone(\n",
    "                gt_rigids, gt_psi)\n",
    "            gt_atom37 = gt_atom37[:, :, :5]\n",
    "            atom37_mask = atom37_mask[:, :, :5]\n",
    "\n",
    "            gt_atom37 = gt_atom37.to(pred_atom37.device)\n",
    "            atom37_mask = atom37_mask.to(pred_atom37.device)\n",
    "            bb_atom_loss_mask = atom37_mask * loss_mask[..., None]\n",
    "            bb_atom_loss = torch.sum(\n",
    "                (pred_atom37 - gt_atom37)**2 * bb_atom_loss_mask[..., None],\n",
    "                dim=(-1, -2, -3)\n",
    "            ) / (bb_atom_loss_mask.sum(dim=(-1, -2)) + 1e-10)\n",
    "            bb_atom_loss *= self._exp_conf.bb_atom_loss_weight\n",
    "            bb_atom_loss *= batch['t'] < self._exp_conf.bb_atom_loss_t_filter\n",
    "            bb_atom_loss *= self._exp_conf.aux_loss_weight\n",
    "\n",
    "\n",
    "            # Pairwise distance loss\n",
    "            gt_flat_atoms = gt_atom37.reshape([batch_size, num_res*5, 3])\n",
    "            gt_pair_dists = torch.linalg.norm(\n",
    "                gt_flat_atoms[:, :, None, :] - gt_flat_atoms[:, None, :, :], dim=-1)\n",
    "            pred_flat_atoms = pred_atom37.reshape([batch_size, num_res*5, 3])\n",
    "            pred_pair_dists = torch.linalg.norm(\n",
    "                pred_flat_atoms[:, :, None, :] - pred_flat_atoms[:, None, :, :], dim=-1)\n",
    "\n",
    "            flat_loss_mask = torch.tile(loss_mask[:, :, None], (1, 1, 5))\n",
    "            flat_loss_mask = flat_loss_mask.reshape([batch_size, num_res*5])\n",
    "            flat_res_mask = torch.tile(bb_mask[:, :, None], (1, 1, 5))\n",
    "            flat_res_mask = flat_res_mask.reshape([batch_size, num_res*5])\n",
    "\n",
    "            gt_pair_dists = gt_pair_dists * flat_loss_mask[..., None]\n",
    "            pred_pair_dists = pred_pair_dists * flat_loss_mask[..., None]\n",
    "            pair_dist_mask = flat_loss_mask[..., None] * flat_res_mask[:, None, :]\n",
    "\n",
    "            # No loss on anything >6A\n",
    "            proximity_mask = gt_pair_dists < 6\n",
    "            pair_dist_mask  = pair_dist_mask * proximity_mask\n",
    "\n",
    "            dist_mat_loss = torch.sum(\n",
    "                (gt_pair_dists - pred_pair_dists)**2 * pair_dist_mask,\n",
    "                dim=(1, 2))\n",
    "            dist_mat_loss /= (torch.sum(pair_dist_mask, dim=(1, 2)) - num_res)\n",
    "            dist_mat_loss *= self._exp_conf.dist_mat_loss_weight\n",
    "            dist_mat_loss *= batch['t'] < self._exp_conf.dist_mat_loss_t_filter\n",
    "            dist_mat_loss *= self._exp_conf.aux_loss_weight\n",
    "            \n",
    "        else:\n",
    "            bb_atom_loss = torch.zeros_like(trans_loss,device=self.device)\n",
    "            dist_mat_loss = torch.zeros_like(trans_loss,device=self.device)\n",
    "\n",
    "         \n",
    "        final_loss = (\n",
    "            rot_loss\n",
    "            + trans_loss\n",
    "            + bb_atom_loss\n",
    "            + dist_mat_loss\n",
    "        )\n",
    "\n",
    "        def normalize_loss(x):\n",
    "            return x.sum() /  (batch_loss_mask.sum() + 1e-9)\n",
    "\n",
    "        aux_data = {\n",
    "            'batch_train_loss': final_loss,\n",
    "            'batch_rot_loss': rot_loss,\n",
    "            'batch_trans_loss': trans_loss,\n",
    "            'batch_bb_atom_loss': bb_atom_loss,\n",
    "            'batch_dist_mat_loss': dist_mat_loss,\n",
    "            'total_loss': normalize_loss(final_loss),\n",
    "            'rot_loss': normalize_loss(rot_loss),\n",
    "            'trans_loss': normalize_loss(trans_loss),\n",
    "            'bb_atom_loss': normalize_loss(bb_atom_loss),\n",
    "            'dist_mat_loss': normalize_loss(dist_mat_loss),\n",
    "            'examples_per_step': torch.tensor(batch_size),\n",
    "            'res_length': torch.mean(torch.sum(bb_mask, dim=-1)),\n",
    "        }\n",
    "\n",
    "        return normalize_loss(final_loss),aux_data\n",
    "        \n",
    "        \n",
    "    def update_fn(self, data):\n",
    "        \"\"\"Updates the state using some data and returns metrics.\"\"\"\n",
    "        self._optimizer.zero_grad()\n",
    "        loss, aux_data = self.loss_fn(data)\n",
    "        loss.backward()\n",
    "        self._optimizer.step()\n",
    "        return loss, aux_data\n",
    "\n",
    "    def _calc_trans_0(self, trans_score, trans_t, t):\n",
    "        beta_t = self._diffuser._se3_diffuser._r3_diffuser.marginal_b_t(t)\n",
    "        beta_t = beta_t[..., None, None]\n",
    "        cond_var = 1 - torch.exp(-beta_t)\n",
    "        return (trans_score * cond_var + trans_t) / torch.exp(-1/2*beta_t)\n",
    "\n",
    "    def _set_t_feats(self, feats, t, t_placeholder):\n",
    "        feats['t'] = t * t_placeholder\n",
    "        rot_score_scaling, trans_score_scaling = self.diffuser.score_scaling(t)\n",
    "        feats['rot_score_scaling'] = rot_score_scaling * t_placeholder\n",
    "        feats['trans_score_scaling'] = trans_score_scaling * t_placeholder\n",
    "        return feats\n",
    "\n",
    "    def forward_traj(self, x_0, min_t, num_t):\n",
    "        forward_steps = np.linspace(min_t, 1.0, num_t)[:-1]\n",
    "        x_traj = [x_0]\n",
    "        for t in forward_steps:\n",
    "            x_t = self.diffuser.se3_diffuser._r3_diffuser.forward(\n",
    "                x_traj[-1], t, num_t)\n",
    "            x_traj.append(x_t)\n",
    "        x_traj = torch.stack(x_traj, axis=0)\n",
    "        return x_traj\n",
    "\n",
    "    def inference_fn(\n",
    "            self,\n",
    "            data_init,\n",
    "            num_t=None,\n",
    "            min_t=None,\n",
    "            center=True,\n",
    "            aux_traj=False,\n",
    "            self_condition=True,\n",
    "            noise_scale=1.0,\n",
    "        ):\n",
    "        \"\"\"Inference function.\n",
    "\n",
    "        Args:\n",
    "            data_init: Initial data values for sampling.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Run reverse process.\n",
    "        sample_feats = copy.deepcopy(data_init)\n",
    "        device = sample_feats['rigids_t'].device\n",
    "#         for k,v in sample_feats.items():\n",
    "#             sample_feats[k] = torch.repeat_interleave(sample_feats[k],torch.tensor([2]*8).to(device),dim=0)\n",
    "        device = sample_feats['rigids_t'].device\n",
    "        if sample_feats['rigids_t'].ndim == 2:\n",
    "            t_placeholder = torch.ones((1,)).to(device)#test remove\n",
    "#             t_placeholder = torch.ones(\n",
    "#                 (self.B,)).to(device)\n",
    "        else:\n",
    "            t_placeholder = torch.ones(\n",
    "                (sample_feats['rigids_t'].shape[0],)).to(device)\n",
    "        if num_t is None:\n",
    "            num_t = self._data_conf.num_t\n",
    "        if min_t is None:\n",
    "            min_t = self._data_conf.min_t\n",
    "        reverse_steps = np.linspace(min_t, 1.0, num_t)[::-1]\n",
    "        dt = 1/num_t\n",
    "        all_rigids = [du.move_to_np(copy.deepcopy(sample_feats['rigids_t']))]\n",
    "        all_bb_prots = []\n",
    "        all_trans_0_pred = []\n",
    "        all_bb_0_pred = []\n",
    "        with torch.no_grad():\n",
    "#             if self._model_conf.embed.embed_self_conditioning and self_condition:\n",
    "#                 sample_feats = self._set_t_feats(\n",
    "#                     sample_feats, reverse_steps[0], t_placeholder)\n",
    "#                 sample_feats = self._self_conditioning(sample_feats)\n",
    "            for t in reverse_steps:\n",
    "                if t > min_t:\n",
    "                    sample_feats = self._set_t_feats(sample_feats, t, t_placeholder)\n",
    "                    model_out = self._model(sample_feats)\n",
    "                    #model_out = self.model(sample_feats)\n",
    "                    rot_score = model_out['rot_score']\n",
    "                    trans_score = model_out['trans_score']\n",
    "                    rigid_pred = model_out['rigids']\n",
    "#                     if self._model_conf.embed.embed_self_conditioning:\n",
    "#                         sample_feats['sc_ca_t'] = rigid_pred[..., 4:]\n",
    "                    fixed_mask = sample_feats['fixed_mask'] * sample_feats['res_mask']\n",
    "                    diffuse_mask = (1 - sample_feats['fixed_mask']) * sample_feats['res_mask']\n",
    "                    rigids_t = self.diffuser.reverse(\n",
    "                        rigid_t=ru.Rigid.from_tensor_7(sample_feats['rigids_t']),\n",
    "                        rot_score=du.move_to_np(rot_score),\n",
    "                        trans_score=du.move_to_np(trans_score),\n",
    "                        diffuse_mask=du.move_to_np(diffuse_mask),\n",
    "                        t=t,\n",
    "                        dt=dt,\n",
    "                        center=center,\n",
    "                        noise_scale=noise_scale,\n",
    "                    )\n",
    "                else:\n",
    "                    model_out = self._model(sample_feats)\n",
    "                    #model_out = self.model(sample_feats)\n",
    "                    rigids_t = ru.Rigid.from_tensor_7(model_out['rigids'])\n",
    "                sample_feats['rigids_t'] = rigids_t.to_tensor_7().to(device)\n",
    "                if aux_traj:\n",
    "                    all_rigids.append(du.move_to_np(rigids_t.to_tensor_7()))\n",
    "\n",
    "                # Calculate x0 prediction derived from score predictions.\n",
    "                gt_trans_0 = sample_feats['rigids_t'][..., 4:]\n",
    "                pred_trans_0 = rigid_pred[..., 4:]\n",
    "                trans_pred_0 = diffuse_mask[..., None] * pred_trans_0 + fixed_mask[..., None] * gt_trans_0\n",
    "                psi_pred = model_out['psi']\n",
    "                if aux_traj:\n",
    "                    atom37_0 = all_atom.compute_backbone(\n",
    "                        ru.Rigid.from_tensor_7(rigid_pred),\n",
    "                        psi_pred\n",
    "                    )[0]\n",
    "                    all_bb_0_pred.append(du.move_to_np(atom37_0))\n",
    "                    all_trans_0_pred.append(du.move_to_np(trans_pred_0))\n",
    "                atom37_t = all_atom.compute_backbone(\n",
    "                    rigids_t, psi_pred)[0]\n",
    "                all_bb_prots.append(du.move_to_np(atom37_t))\n",
    "\n",
    "        # Flip trajectory so that it starts from t=0.\n",
    "        # This helps visualization.\n",
    "        flip = lambda x: np.flip(np.stack(x), (0,))\n",
    "        all_bb_prots = flip(all_bb_prots)\n",
    "        if aux_traj:\n",
    "            all_rigids = flip(all_rigids)\n",
    "            all_trans_0_pred = flip(all_trans_0_pred)\n",
    "            all_bb_0_pred = flip(all_bb_0_pred)\n",
    "\n",
    "        ret = {\n",
    "            'prot_traj': all_bb_prots,\n",
    "        }\n",
    "        if aux_traj:\n",
    "            ret['rigid_traj'] = all_rigids\n",
    "            ret['trans_traj'] = all_trans_0_pred\n",
    "            ret['psi_pred'] = psi_pred[None]\n",
    "            ret['rigid_0_traj'] = all_bb_0_pred\n",
    "        return ret\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33e16cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import se3_diffuse.utils as du"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a31f547",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_rigid_diffuser import so3_diffuser\n",
    "from data_rigid_diffuser import r3_diffuser\n",
    "from data_rigid_diffuser import oneHot_diffuser\n",
    "from scipy.spatial.transform import Rotation\n",
    "from data_rigid_diffuser import rigid_utils as ru\n",
    "import yaml\n",
    "from data_rigid_diffuser.diffuser import FrameDiffNoise\n",
    "from gudiff_model import Data_Graph\n",
    "from gudiff_model.Data_Graph import build_npose_from_coords, dump_coord_pdb, define_graph_edges, make_pe_encoding\n",
    "from gudiff_model.Data_Graph import Helix4_Dataset, Make_KNN_MP_Graphs\n",
    "from gudiff_model.Data_Graph_Null import  Make_nullKNN_MP_Graphs\n",
    "from data_rigid_diffuser import diffuser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf443f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from se3_transformer.model.basis import get_basis, update_basis_with_fused\n",
    "from se3_transformer.model.transformer import Sequential, SE3Transformer\n",
    "from se3_transformer.model.transformer_topk import SE3Transformer_topK\n",
    "from se3_transformer.model.FAPE_Loss import FAPE_loss, Qs2Rs, normQ\n",
    "from se3_transformer.model.layers.attentiontopK import AttentionBlockSE3\n",
    "from se3_transformer.model.layers.linear import LinearSE3\n",
    "from se3_transformer.model.layers.convolution import ConvSE3, ConvSE3FuseLevel\n",
    "from se3_transformer.model.layers.norm import NormSE3\n",
    "from se3_transformer.model.layers.pooling import GPooling, Latent_Unpool, Unpool_Layer\n",
    "from se3_transformer.runtime.utils import str2bool, to_cuda\n",
    "from se3_transformer.model.fiber import Fiber\n",
    "from se3_transformer.model.transformer import get_populated_edge_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48bf9476",
   "metadata": {},
   "outputs": [],
   "source": [
    "from se3_transformer.model.FAPE_Loss import FAPE_loss_null, FAPE_loss_real\n",
    "from se3_transformer.model.FAPE_Loss import get_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e213e885",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3eae2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B = 16\n",
    "# L=65\n",
    "# limit = 1028\n",
    "# h4_trainData = Helix4_Dataset(coords_tog[:limit])\n",
    "# h4_valData = Helix4_Dataset(coords_apa[:limit])\n",
    "# train_dL = DataLoader(h4_trainData, batch_size=B, shuffle=True, drop_last=True)\n",
    "# val_dL   = DataLoader(h4_valData, batch_size=B, shuffle=True, drop_last=True)\n",
    "# testiter = iter(train_dL)\n",
    "# bb_dict = next(testiter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbf8e13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87f67d73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "581b3006",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I think I adjusted inputs and outputs,\n",
    "#sigmoid for nodes features for real/null pred?\n",
    "#need loss function for real/null nodes\n",
    "#need to get function to just pull real nodes for viewing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16c6656",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b0ba79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02ba860f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pV_to_points(dict_in):\n",
    "    \n",
    "    CA_fp  = dict_in['bb_firstp']['CA'].reshape(B, L, 3).to(device)\n",
    "    NC_fp = CA_fp + dict_in['bb_firstp']['N_CA'].reshape(B, L, 3).to(device)\n",
    "    CC_fp = CA_fp + dict_in['bb_firstp']['C_CA'].reshape(B, L, 3).to(device)\n",
    "    fp =  torch.cat((NC_fp,CA_fp,CC_fp),dim=2).reshape(B,L,3,3)\n",
    "    \n",
    "    CA_lp  = dict_in['bb_firstp']['CA'].reshape(B, L, 3).to(device)\n",
    "    NC_lp = CA_fp + dict_in['bb_firstp']['N_CA'].reshape(B, L, 3).to(device)\n",
    "    CC_lp = CA_fp + dict_in['bb_firstp']['C_CA'].reshape(B, L, 3).to(device)\n",
    "    lp =  torch.cat((NC_lp,CA_lp,CC_lp),dim=2).reshape(B,L,3,3)\n",
    "    \n",
    "    return fp, lp\n",
    "def get_noise_pred_true_null(noised_dict, batched_t, graph_maker, graph_unet):\n",
    "    \n",
    "    CA_t  = noised_dict['bb_shifted']['CA'].reshape(B, L, 3).to('cuda')\n",
    "    NC_t = CA_t + noised_dict['bb_shifted']['N_CA'].reshape(B, L, 3).to('cuda')*N_CA_dist\n",
    "    CC_t = CA_t + noised_dict['bb_shifted']['C_CA'].reshape(B, L, 3).to('cuda')*C_CA_dist\n",
    "    true =  torch.cat((NC_t,CA_t,CC_t),dim=2).reshape(B,L,3,3)\n",
    "    \n",
    "    CA_n  = noised_dict['bb_noised']['CA'].reshape(B, L, 3).to('cuda')\n",
    "    NC_n = CA_n + noised_dict['bb_noised']['N_CA'].reshape(B, L, 3).to('cuda')*N_CA_dist\n",
    "    CC_n = CA_n + noised_dict['bb_noised']['C_CA'].reshape(B, L, 3).to('cuda')*C_CA_dist\n",
    "    noise_xyz =  torch.cat((NC_n,CA_n,CC_n),dim=2).reshape(B,L,3,3)\n",
    "    \n",
    "    feat_dict = graph_maker.prep_for_network(noised_dict)\n",
    "    out = graph_unet(feat_dict, batched_t)\n",
    "    CA_p = out['1'][:,0,:].reshape(B, L, 3)+CA_n #translation of Calpha\n",
    "    Qs = out['1'][:,1,:] # rotation\n",
    "    Qs = Qs.unsqueeze(1).repeat((1,2,1))\n",
    "    Qs = torch.cat((torch.ones((B*L,2,1),device=Qs.device),Qs),dim=-1).reshape(B,L,2,4)\n",
    "    Qs = normQ(Qs)\n",
    "    Rs = Qs2Rs(Qs)\n",
    "    N_C_to_Rot = torch.cat((noised_dict['bb_noised']['N_CA'].reshape(B, L, 3).to('cuda'),\n",
    "                            noised_dict['bb_noised']['C_CA'].reshape(B, L, 3).to('cuda')),dim=2).reshape(B,L,2,1,3)\n",
    "\n",
    "    \n",
    "    rot_vecs = einsum('bnkij,bnkhj->bnki',Rs, N_C_to_Rot)\n",
    "    NC_p = CA_p + rot_vecs[:,:,0,:].to('cuda')*N_CA_dist\n",
    "    CC_p = CA_p + rot_vecs[:,:,1,:].reshape(B, L, 3).to('cuda')*C_CA_dist\n",
    "\n",
    "    pred = torch.cat((NC_p,CA_p,CC_p),dim=2).reshape(B,L,3,3)\n",
    "    \n",
    "    nf_pred = out['0']\n",
    "    \n",
    "    nf_pred = out['0']\n",
    "    real_nodes_pred = torch.round(nf_pred ).clamp(0,1)\n",
    "    real_nodes_pred_mask = (real_nodes_pred.squeeze().sum(-1)>1.99).reshape(B,L)\n",
    "    \n",
    "    real_nodes_true_mask = noised_dict['real_nodes_mask']\n",
    "    #place roll here later\n",
    "#     true = true.to('cpu').numpy()*10\n",
    "#     noise_xyz = noise_xyz.to('cpu').numpy()*10\n",
    "#     pred = pred.detach().to('cpu').numpy()*10\n",
    "    \n",
    "    \n",
    "    return true, noise_xyz, pred , real_nodes_pred_mask, real_nodes_true_mask\n",
    "        \n",
    "def roll2_continous_true(real_mask_in):\n",
    "    \"\"\"Return roll amount to set zero on Nterminal residue for pdb file view\"\"\"\n",
    "\n",
    "    roll_con_out = []\n",
    "    for i,rmr in enumerate(real_mask_in):\n",
    "        ep_bool = (rmr^rmr.roll(-1) | rmr^rmr.roll(1)) & rmr\n",
    "        si = torch.arange(ep_bool.shape[0])[ep_bool]\n",
    "        #circular if start/end real nodes and we need to roll\n",
    "        if rmr[0] and rmr[-1]:\n",
    "            #roll last group across barrier\n",
    "            roll_con = -si[-1]\n",
    "        elif not rmr[0]: #move first group to front\n",
    "            roll_con = -si[0]\n",
    "        else:\n",
    "            roll_con=0\n",
    "\n",
    "        roll_con_out.append(roll_con)\n",
    "\n",
    "    return roll_con_out\n",
    "      \n",
    "def dump_tnp_null(true, noise, pred, t_val, e=0, numOut=1, real_mask=None, pred_mask=None, outdir='output/'):\n",
    "    \n",
    "    if numOut>true.shape[0]:\n",
    "        numOut = true.shape[0]\n",
    "    \n",
    "    tnk_dir = f'{outdir}/true_node_mask/'\n",
    "    pnk_dir = f'{outdir}/pred_node_mask/'\n",
    "    f_dir = f'{outdir}/full/'\n",
    "    \n",
    "    if not os.path.isdir(tnk_dir) and real_mask is not None:\n",
    "        os.makedirs(tnk_dir)\n",
    "    if not os.path.isdir(pnk_dir) and pred_mask is not None:\n",
    "        os.makedirs(pnk_dir)\n",
    "    if not os.path.isdir(f_dir) and real_mask is not None:\n",
    "        os.makedirs(f_dir)\n",
    "    \n",
    "    if real_mask is not None:\n",
    "        rc = roll2_continous_true(real_mask)\n",
    "        for x in range(numOut):\n",
    "            t_o = true[x].roll(int(rc[x]),dims=0).detach().to('cpu').numpy()*10\n",
    "            n_o = noise[x].roll(int(rc[x]),dims=0).detach().to('cpu').numpy()*10\n",
    "            p_o = pred[x].roll(int(rc[x]),dims=0).detach().to('cpu').numpy()*10\n",
    "            dump_coord_pdb(t_o, fileOut=f'{f_dir}/true_{t_val[x]*100:.0f}_e{e}_{x}.pdb')\n",
    "            dump_coord_pdb(n_o, fileOut=f'{f_dir}/noise_{t_val[x]*100:.0f}_e{e}_{x}.pdb')\n",
    "            dump_coord_pdb(p_o, fileOut=f'{f_dir}/pred_{t_val[x]*100:.0f}_e{e}_{x}.pdb')\n",
    "        \n",
    "    if pred_mask is not None:\n",
    "        rc = roll2_continous_true(pred_mask)\n",
    "        for x,c in enumerate(np.arange(numOut)):\n",
    "            t_o = true[x].roll(int(rc[x]),dims=0).detach().to('cpu').numpy()*10\n",
    "            n_o = noise[x].roll(int(rc[x]),dims=0).detach().to('cpu').numpy()*10\n",
    "            p_o = pred[x].roll(int(rc[x]),dims=0).detach().to('cpu').numpy()*10\n",
    "            pm = pred_mask[x].roll(int(rc[x]),dims=0)\n",
    "            dump_coord_pdb(t_o[pm], fileOut=f'{pnk_dir}/true_{t_val[x]*100:.0f}_e{e}_{x}.pdb')\n",
    "            dump_coord_pdb(n_o[pm], fileOut=f'{pnk_dir}/noise_{t_val[x]*100:.0f}_e{e}_{x}.pdb')\n",
    "            dump_coord_pdb(p_o[pm], fileOut=f'{pnk_dir}/pred_{t_val[x]*100:.0f}_e{e}_{x}.pdb')\n",
    "            \n",
    "    if real_mask is not None:\n",
    "        rc = roll2_continous_true(real_mask)\n",
    "        for x,c in enumerate(np.arange(numOut)):\n",
    "            t_o = true[x].roll(int(rc[x]),dims=0).detach().to('cpu').numpy()*10\n",
    "            n_o = noise[x].roll(int(rc[x]),dims=0).detach().to('cpu').numpy()*10\n",
    "            p_o = pred[x].roll(int(rc[x]),dims=0).detach().to('cpu').numpy()*10\n",
    "            rm = real_mask[x].roll(int(rc[x]),dims=0)\n",
    "            dump_coord_pdb(t_o[rm], fileOut=f'{pnk_dir}/true_{t_val[x]*100:.0f}_e{e}_{x}.pdb')\n",
    "            dump_coord_pdb(n_o[rm], fileOut=f'{pnk_dir}/noise_{t_val[x]*100:.0f}_e{e}_{x}.pdb')\n",
    "            dump_coord_pdb(p_o[rm], fileOut=f'{pnk_dir}/pred_{t_val[x]*100:.0f}_e{e}_{x}.pdb')\n",
    "            \n",
    "#     if real_mask is not None:\n",
    "#         rc = roll2_continous_true(real_mask)\n",
    "#         for x in range(numOut):\n",
    "#             dump_coord_pdb(true[x][real_mask[x]], fileOut=f'{tnk_dir}/true_{t_val[x]*100:.0f}_e{e}_{x}.pdb')\n",
    "#             dump_coord_pdb(noise[x][real_mask[x]], fileOut=f'{tnk_dir}/noise_{t_val[x]*100:.0f}_e{e}_{x}.pdb')\n",
    "#             dump_coord_pdb(pred[x][real_mask[x]], fileOut=f'{tnk_dir}/pred_{t_val[x]*100:.0f}_e{e}_{x}.pdb')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7e3e99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pV_to_points(dict_in):\n",
    "    \n",
    "    CA_fp  = dict_in['bb_firstp']['CA'].to(device)\n",
    "    NC_fp = CA_fp + dict_in['bb_firstp']['N_CA'].to(device)\n",
    "    CC_fp = CA_fp +dict_in['bb_firstp']['C_CA'].to(device)\n",
    "    fp =  torch.cat((NC_fp,CA_fp,CC_fp),dim=2).reshape(B,1,3,3)\n",
    "    \n",
    "    CA_lp  = dict_in['bb_lastp']['CA'].to(device)\n",
    "    NC_lp = CA_fp + dict_in['bb_lastp']['N_CA'].to(device)\n",
    "    CC_lp = CA_fp + dict_in['bb_lastp']['C_CA'].to(device)\n",
    "    lp =  torch.cat((NC_lp,CA_lp,CC_lp),dim=2).reshape(B,1,3,3)\n",
    "    \n",
    "    return fp, lp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7eeb65f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def model_step_null(noised_dict, batched_t, graph_maker, graph_unet, train=True):\n",
    "    #prep coordinates for output display from and comparison via FAPE\n",
    "    \n",
    "    CA_t  = noised_dict['bb_shifted']['CA'].reshape(B, L, 3).to(device)\n",
    "    NC_t = CA_t + noised_dict['bb_shifted']['N_CA'].reshape(B, L, 3).to(device)#not mult by bond distance, seems to help?\n",
    "    CC_t = CA_t + noised_dict['bb_shifted']['C_CA'].reshape(B, L, 3).to(device)#not mult \n",
    "    true =  torch.cat((NC_t,CA_t,CC_t),dim=2).reshape(B,L,3,3)\n",
    "\n",
    "    CA_n  = noised_dict['bb_noised']['CA'].reshape(B, L, 3).to(device)\n",
    "    NC_n = CA_n + noised_dict['bb_noised']['N_CA'].reshape(B, L, 3).to(device)\n",
    "    CC_n = CA_n + noised_dict['bb_noised']['C_CA'].reshape(B, L, 3).to(device)\n",
    "    noise_xyz =  torch.cat((NC_n,CA_n,CC_n),dim=2).reshape(B,L,3,3)\n",
    "\n",
    "    #prepare graphs\n",
    "    feat_dict = graph_maker.prep_for_network(noised_dict, cuda=True)\n",
    "    out =graph_unet(feat_dict,batched_t)\n",
    "    \n",
    "    \n",
    "    #FAPE Loss for the prediction\n",
    "    CA_p = out['1'][:,0,:].reshape(B, L, 3)+CA_n #translation of Calpha\n",
    "    Qs = out['1'][:,1,:] # rotation , convert from x,y,z (Quat) to rotate input vectors\n",
    "    Qs = Qs.unsqueeze(1).repeat((1,2,1))\n",
    "    Qs = torch.cat((torch.ones((B*L,2,1),device=Qs.device),Qs),dim=-1).reshape(B,L,2,4)\n",
    "    Qs = normQ(Qs)\n",
    "    Rs = Qs2Rs(Qs)\n",
    "    N_C_to_Rot = torch.cat((noised_dict['bb_noised']['N_CA'].reshape(B, L, 3).to(device),\n",
    "                            noised_dict['bb_noised']['C_CA'].reshape(B, L, 3).to(device)),dim=2).reshape(B,L,2,1,3)\n",
    "    rot_vecs = einsum('bnkij,bnkhj->bnki',Rs, N_C_to_Rot)\n",
    "    NC_p = CA_p + rot_vecs[:,:,0,:]*N_CA_dist #comparable but seems better not have it for true, but have it for pred\n",
    "    CC_p = CA_p + rot_vecs[:,:,1,:]*C_CA_dist #maybe this helep prevent \n",
    "\n",
    "    pred = torch.cat((NC_p,CA_p,CC_p),dim=2).reshape(B,L,3,3)\n",
    "    \n",
    "    #divide loss by real and null nodes\n",
    "    \n",
    "    fp, lp  = convert_pV_to_points(noised_dict)\n",
    "\n",
    "    real_mask = noised_dict['real_nodes_mask'].to('cuda')\n",
    "    score_scales = noised_dict['score_scales'].to('cuda')\n",
    "\n",
    "    lr, lr_d = FAPE_loss_real(pred, true, score_scales, real_mask,  d_clamp=10.0, d_clamp_inter=30.0,\n",
    "                   A=10.0, gamma=1.0, eps=1e-6)\n",
    "    \n",
    "    ln, ln_d = FAPE_loss_null(pred, fp, lp, real_mask, score_scales,  d_clamp=10.0,\n",
    "                       d_clamp_inter=30.0, A=10.0, gamma=1.0, eps=1e-6)\n",
    "    \n",
    "    structure_loss = lr*score_weights['3D_real']+ln*score_weights['3D_null']\n",
    "    \n",
    "    #score for node feats determining whether node is real or fake\n",
    "    nf_pred = out['0']\n",
    "\n",
    "    nf_feat_dim = noised_dict['real_nodes_noise'].shape[-1]\n",
    "    nf_true = torch.ones(noised_dict['real_nodes_mask'].shape+(nf_feat_dim,) + (1,),\n",
    "                         dtype=torch.float,device=device)\n",
    "\n",
    "    nf_real_mask_mult = real_mask.unsqueeze(-1).unsqueeze(-1).to(device)\n",
    "    nf_true = nf_true*nf_real_mask_mult\n",
    "\n",
    "    nf_pred = nf_pred.reshape(B,-1,nf_feat_dim)\n",
    "    pred_nf_loss = torch.sum(torch.abs(nf_true.squeeze()-nf_pred),dim=-1) #absolute value loss\n",
    "    pred_nf_loss = pred_nf_loss.to(device)\n",
    "    \n",
    "    ss_scales = to_cuda(noised_dict['score_scales'])[:,None,None]\n",
    "    pnfloss = (torch.sum((pred_nf_loss*ss_scales/L)))*score_weights['nf_real']\n",
    "    \n",
    "    final_loss = structure_loss + pnfloss\n",
    "    \n",
    "    \n",
    "    return final_loss, pnfloss.detach().cpu(), structure_loss.detach().cpu(), lr.detach().cpu(), ln.detach().cpu()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9e308d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ea96ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfa33c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4d19f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97322803",
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda'\n",
    "\n",
    "B = 8\n",
    "L= 128\n",
    "limit = 1028\n",
    "prot_trainData = Data_Graph.ProteinBB_Dataset(coords_tog[:limit], n_nodes=L,\n",
    "              n_atoms=5, coord_div=10, cast_type=torch.float32)\n",
    "train_dL = DataLoader(prot_trainData, batch_size=B, shuffle=True, drop_last=True)\n",
    "stride=4##########\n",
    "mkg = Make_nullKNN_MP_Graphs(KNN=30, mp_stride=stride, n_nodes=L)\n",
    "\n",
    "score_weights = {}\n",
    "score_weights['nf_real'] = torch.tensor(0.2,device=device)\n",
    "score_weights['3D_real'] = torch.tensor(1.0,device=device)\n",
    "score_weights['3D_null'] = torch.tensor(1.0,device=device)\n",
    "\n",
    "config_path='data_rigid_diffuser/base.yaml'\n",
    "fnd = FrameDiffNoise(config_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77cce375",
   "metadata": {},
   "outputs": [],
   "source": [
    "gunn= GraphUNet_Null(fiber_start = Fiber({0:17, 1:2}),\n",
    "                     fiber_out = Fiber({0:5,1:2}),\n",
    "                      k=4,\n",
    "                      batch_size = B,\n",
    "                      stride=stride,\n",
    "                       max_degree=3,\n",
    "                       channels=64,\n",
    "                      num_heads = 16,\n",
    "                      channels_div=8,\n",
    "                      num_layers = 1,\n",
    "                     num_layers_ca = 2,\n",
    "                     edge_feature_dim=1,\n",
    "                     latent_pool_type = 'max',\n",
    "                     t_size = 12,\n",
    "                     mult=2,\n",
    "                    zero_lin=True,\n",
    "                   use_tdeg1 = False,\n",
    "                 cuda=True).to('cuda')\n",
    "\n",
    "opti = torch.optim.Adam(gunn.parameters(), lr=0.0005, weight_decay=5e-6)\n",
    "#opti =torch.optim.Adadelta(gunn.parameters(), lr=0.0005, weight_decay=5e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7bf8bd0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.optim.adadelta.Adadelta"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.optim.Adadelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b7af006",
   "metadata": {},
   "outputs": [],
   "source": [
    "testiter = iter(train_dL)\n",
    "bb_dict = next(testiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1be1401e",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_cpu = np.ones((B,))*0.01\n",
    "noised_dict = fnd.forward(bb_dict,t_vec=t_cpu)\n",
    "batched_t = to_cuda(noised_dict['t_vec'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b615080",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nwoodall/miniconda3/envs/se33/lib/python3.9/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
      "/home/nwoodall/miniconda3/envs/se33/lib/python3.9/site-packages/dgl/backend/pytorch/tensor.py:449: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(3221.5481, device='cuda:0') tensor(2649.7085) tensor(443.5340) tensor(128.3047)\n",
      "1 tensor(721.7962, device='cuda:0') tensor(187.8804) tensor(426.0406) tensor(107.8751)\n",
      "2 tensor(524.7708, device='cuda:0') tensor(6.3072) tensor(419.9707) tensor(98.4929)\n",
      "3 tensor(509.0513, device='cuda:0') tensor(4.0618) tensor(413.1630) tensor(91.8263)\n",
      "4 tensor(499.3692, device='cuda:0') tensor(1.8367) tensor(408.9172) tensor(88.6152)\n",
      "5 tensor(495.5051, device='cuda:0') tensor(0.8494) tensor(407.3964) tensor(87.2594)\n",
      "6 tensor(496.9202, device='cuda:0') tensor(1.4569) tensor(408.6588) tensor(86.8044)\n",
      "7 tensor(493.8044, device='cuda:0') tensor(0.7936) tensor(407.4102) tensor(85.6007)\n",
      "8 tensor(491.7913, device='cuda:0') tensor(1.2068) tensor(406.0603) tensor(84.5241)\n",
      "9 tensor(488.5466, device='cuda:0') tensor(1.5562) tensor(403.9198) tensor(83.0707)\n",
      "10 tensor(484.4487, device='cuda:0') tensor(0.5390) tensor(403.2652) tensor(80.6445)\n",
      "11 tensor(481.1455, device='cuda:0') tensor(0.4078) tensor(401.9539) tensor(78.7837)\n",
      "12 tensor(478.2026, device='cuda:0') tensor(0.2079) tensor(400.9020) tensor(77.0928)\n",
      "13 tensor(474.8612, device='cuda:0') tensor(0.2196) tensor(400.6619) tensor(73.9797)\n",
      "14 tensor(473.4901, device='cuda:0') tensor(0.8739) tensor(399.3948) tensor(73.2215)\n",
      "15 tensor(468.6091, device='cuda:0') tensor(0.2750) tensor(397.2854) tensor(71.0488)\n",
      "16 tensor(467.0658, device='cuda:0') tensor(0.1887) tensor(397.2616) tensor(69.6156)\n",
      "17 tensor(464.7002, device='cuda:0') tensor(0.1762) tensor(396.1198) tensor(68.4043)\n",
      "18 tensor(462.8651, device='cuda:0') tensor(0.1286) tensor(396.0864) tensor(66.6500)\n",
      "19 tensor(459.7138, device='cuda:0') tensor(0.1261) tensor(394.3859) tensor(65.2017)\n",
      "20 tensor(460.9500, device='cuda:0') tensor(2.4028) tensor(394.8625) tensor(63.6847)\n",
      "21 tensor(455.2230, device='cuda:0') tensor(0.4518) tensor(390.8613) tensor(63.9100)\n",
      "22 tensor(450.2308, device='cuda:0') tensor(0.2255) tensor(388.4796) tensor(61.5258)\n",
      "23 tensor(450.7272, device='cuda:0') tensor(2.1631) tensor(386.9889) tensor(61.5751)\n",
      "24 tensor(441.6721, device='cuda:0') tensor(0.2580) tensor(381.2077) tensor(60.2064)\n",
      "25 tensor(438.3170, device='cuda:0') tensor(0.0759) tensor(378.9686) tensor(59.2725)\n",
      "26 tensor(537.4526, device='cuda:0') tensor(57.8400) tensor(399.1910) tensor(80.4215)\n",
      "27 tensor(486.5722, device='cuda:0') tensor(1.7096) tensor(402.2144) tensor(82.6480)\n",
      "28 tensor(475.6711, device='cuda:0') tensor(1.0660) tensor(399.8142) tensor(74.7909)\n",
      "29 tensor(469.0255, device='cuda:0') tensor(1.4766) tensor(396.2985) tensor(71.2503)\n",
      "30 tensor(464.1310, device='cuda:0') tensor(0.0390) tensor(395.4902) tensor(68.6020)\n",
      "31 tensor(461.4666, device='cuda:0') tensor(0.4446) tensor(395.1832) tensor(65.8387)\n",
      "32 tensor(457.6926, device='cuda:0') tensor(0.1059) tensor(393.2300) tensor(64.3567)\n",
      "33 tensor(458.2358, device='cuda:0') tensor(0.0622) tensor(392.7801) tensor(65.3935)\n",
      "34 tensor(453.8103, device='cuda:0') tensor(0.0083) tensor(391.5746) tensor(62.2274)\n",
      "35 tensor(453.1089, device='cuda:0') tensor(0.0334) tensor(390.2734) tensor(62.8024)\n",
      "36 tensor(449.2149, device='cuda:0') tensor(0.1153) tensor(388.7463) tensor(60.3533)\n",
      "37 tensor(448.2466, device='cuda:0') tensor(0.0767) tensor(387.2961) tensor(60.8739)\n",
      "38 tensor(445.4190, device='cuda:0') tensor(0.0535) tensor(385.8611) tensor(59.5045)\n",
      "39 tensor(440.4971, device='cuda:0') tensor(0.0025) tensor(382.0197) tensor(58.4749)\n",
      "40 tensor(435.1274, device='cuda:0') tensor(0.0033) tensor(377.9801) tensor(57.1440)\n",
      "41 tensor(430.5036, device='cuda:0') tensor(0.0040) tensor(374.7790) tensor(55.7205)\n",
      "42 tensor(427.2604, device='cuda:0') tensor(0.0032) tensor(371.5925) tensor(55.6646)\n",
      "43 tensor(423.3967, device='cuda:0') tensor(0.1755) tensor(369.4853) tensor(53.7358)\n",
      "44 tensor(419.2504, device='cuda:0') tensor(0.1910) tensor(366.3576) tensor(52.7019)\n",
      "45 tensor(416.0633, device='cuda:0') tensor(0.0026) tensor(364.3643) tensor(51.6964)\n",
      "46 tensor(410.9224, device='cuda:0') tensor(0.0026) tensor(360.4525) tensor(50.4673)\n",
      "47 tensor(434.8892, device='cuda:0') tensor(0.0041) tensor(374.6582) tensor(60.2270)\n",
      "48 tensor(439.4052, device='cuda:0') tensor(0.0061) tensor(378.2735) tensor(61.1257)\n",
      "49 tensor(429.1835, device='cuda:0') tensor(0.0038) tensor(372.5628) tensor(56.6169)\n",
      "50 tensor(425.2114, device='cuda:0') tensor(0.0041) tensor(369.6381) tensor(55.5692)\n",
      "51 tensor(419.3882, device='cuda:0') tensor(0.0030) tensor(365.3520) tensor(54.0332)\n",
      "52 tensor(416.2638, device='cuda:0') tensor(0.0031) tensor(363.6121) tensor(52.6486)\n",
      "53 tensor(413.5621, device='cuda:0') tensor(0.0028) tensor(362.1032) tensor(51.4561)\n",
      "54 tensor(410.8506, device='cuda:0') tensor(0.0018) tensor(359.9374) tensor(50.9116)\n",
      "55 tensor(409.0936, device='cuda:0') tensor(0.0018) tensor(357.7891) tensor(51.3026)\n",
      "56 tensor(404.7655, device='cuda:0') tensor(0.0017) tensor(355.8239) tensor(48.9401)\n",
      "57 tensor(403.5749, device='cuda:0') tensor(0.0143) tensor(354.6316) tensor(48.9290)\n",
      "58 tensor(403.5695, device='cuda:0') tensor(0.3414) tensor(355.1867) tensor(48.0413)\n",
      "59 tensor(402.5637, device='cuda:0') tensor(0.0143) tensor(354.0202) tensor(48.5291)\n",
      "60 tensor(401.1631, device='cuda:0') tensor(0.2449) tensor(353.6154) tensor(47.3030)\n",
      "61 tensor(399.3080, device='cuda:0') tensor(0.0009) tensor(352.7016) tensor(46.6053)\n",
      "62 tensor(398.5374, device='cuda:0') tensor(0.0012) tensor(350.9989) tensor(47.5372)\n",
      "63 tensor(396.4164, device='cuda:0') tensor(0.0002) tensor(349.6059) tensor(46.8103)\n",
      "64 tensor(395.2093, device='cuda:0') tensor(0.0003) tensor(347.7632) tensor(47.4457)\n",
      "65 tensor(393.5380, device='cuda:0') tensor(0.0003) tensor(347.6620) tensor(45.8757)\n",
      "66 tensor(393.0102, device='cuda:0') tensor(0.0003) tensor(347.1361) tensor(45.8738)\n",
      "67 tensor(390.4869, device='cuda:0') tensor(0.0008) tensor(345.3205) tensor(45.1658)\n",
      "68 tensor(390.7988, device='cuda:0') tensor(0.0003) tensor(345.2515) tensor(45.5470)\n",
      "69 tensor(389.1422, device='cuda:0') tensor(0.0002) tensor(344.3288) tensor(44.8130)\n",
      "70 tensor(386.8400, device='cuda:0') tensor(0.0002) tensor(342.0103) tensor(44.8295)\n",
      "71 tensor(385.0645, device='cuda:0') tensor(0.0007) tensor(339.5315) tensor(45.5323)\n",
      "72 tensor(379.3577, device='cuda:0') tensor(0.0004) tensor(335.0187) tensor(44.3386)\n",
      "73 tensor(375.7636, device='cuda:0') tensor(0.0007) tensor(330.6038) tensor(45.1592)\n",
      "74 tensor(376.6469, device='cuda:0') tensor(0.0002) tensor(331.6874) tensor(44.9592)\n",
      "75 tensor(373.7442, device='cuda:0') tensor(0.0001) tensor(329.0865) tensor(44.6577)\n",
      "76 tensor(373.9803, device='cuda:0') tensor(0.0001) tensor(329.8415) tensor(44.1387)\n",
      "77 tensor(370.4177, device='cuda:0') tensor(0.3239) tensor(326.2426) tensor(43.8513)\n",
      "78 tensor(365.5276, device='cuda:0') tensor(0.0001) tensor(322.7634) tensor(42.7641)\n",
      "79 tensor(364.3218, device='cuda:0') tensor(0.0001) tensor(321.3036) tensor(43.0182)\n",
      "80 tensor(363.8008, device='cuda:0') tensor(9.0403e-05) tensor(320.7256) tensor(43.0749)\n",
      "81 tensor(361.2311, device='cuda:0') tensor(8.6470e-05) tensor(317.9082) tensor(43.3227)\n",
      "82 tensor(366.3040, device='cuda:0') tensor(1.1817) tensor(320.8825) tensor(44.2399)\n",
      "83 tensor(364.5942, device='cuda:0') tensor(0.0051) tensor(320.7852) tensor(43.8039)\n",
      "84 tensor(360.9010, device='cuda:0') tensor(0.1629) tensor(318.2293) tensor(42.5089)\n",
      "85 tensor(360.3383, device='cuda:0') tensor(0.0007) tensor(317.5934) tensor(42.7442)\n",
      "86 tensor(358.7050, device='cuda:0') tensor(0.0005) tensor(316.7149) tensor(41.9896)\n",
      "87 tensor(357.5758, device='cuda:0') tensor(0.0004) tensor(315.3867) tensor(42.1886)\n",
      "88 tensor(355.6270, device='cuda:0') tensor(0.0003) tensor(313.3973) tensor(42.2292)\n",
      "89 tensor(355.4436, device='cuda:0') tensor(0.0003) tensor(312.8431) tensor(42.6002)\n",
      "90 tensor(353.3784, device='cuda:0') tensor(0.0002) tensor(311.9073) tensor(41.4709)\n",
      "91 tensor(352.8914, device='cuda:0') tensor(0.0004) tensor(310.2683) tensor(42.6228)\n",
      "92 tensor(350.3728, device='cuda:0') tensor(0.0002) tensor(309.6812) tensor(40.6916)\n",
      "93 tensor(348.7525, device='cuda:0') tensor(0.0004) tensor(307.4651) tensor(41.2870)\n",
      "94 tensor(348.5085, device='cuda:0') tensor(0.0003) tensor(307.4258) tensor(41.0823)\n",
      "95 tensor(348.5995, device='cuda:0') tensor(0.0001) tensor(308.1937) tensor(40.4057)\n",
      "96 tensor(348.4612, device='cuda:0') tensor(0.0002) tensor(306.4120) tensor(42.0491)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97 tensor(347.9015, device='cuda:0') tensor(0.0001) tensor(307.3578) tensor(40.5437)\n",
      "98 tensor(347.9319, device='cuda:0') tensor(0.0002) tensor(306.9611) tensor(40.9706)\n",
      "99 tensor(345.1673, device='cuda:0') tensor(0.0002) tensor(305.2675) tensor(39.8996)\n"
     ]
    }
   ],
   "source": [
    "t=0.02\n",
    "for e in range(100):\n",
    "    pnf_Score=0\n",
    "    e_score = 0\n",
    "    r_score = 0\n",
    "    l_score = 0\n",
    "    for i, bb_dict in enumerate(train_dL):\n",
    "        t_cpu = np.ones((B,))*t\n",
    "        \n",
    "        noised_dict = fnd.forward(bb_dict,t_vec=t_cpu)\n",
    "        batched_t = to_cuda(noised_dict['t_vec'])\n",
    "\n",
    "\n",
    "        #final_loss, pred_nf_loss, loss_3D = model_step_null(noised_dict,batched_t, mkg, gunn, train=True)\n",
    "        final_loss, pnfloss, structure_loss, real_loss, null_loss = model_step_null(noised_dict,batched_t, mkg, gunn, train=True)\n",
    "        \n",
    "        opti.zero_grad()\n",
    "        final_loss.backward()\n",
    "        opti.step()\n",
    "        fl = final_loss.detach()\n",
    "        e_score += fl\n",
    "        pnf_Score += pnfloss\n",
    "        r_score += real_loss\n",
    "        l_score += null_loss\n",
    "    #print(e,e_score,pred_nf_loss.sum(), loss_3D.sum())\n",
    "    if e%5==1:\n",
    "        true, noise_xyz, pred , real_nodes_pred_mask, real_nodes_true_mask = get_noise_pred_true_null(noised_dict, batched_t, mkg, gunn)\n",
    "        dump_tnp_null(true, noise_xyz, pred, t_cpu,e=e, numOut=1, real_mask=real_nodes_true_mask, pred_mask=None, outdir='output/')\n",
    "        \n",
    "    print(e,e_score,pnf_Score,r_score,l_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0873f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, bb_dict in enumerate(train_dL):\n",
    "    t_cpu = np.ones((B,))*0.05\n",
    "    batched_t = to_cuda(noised_dict['t_vec'])\n",
    "    noised_dict = fnd.forward(bb_dict,t_vec=t_cpu)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6291b623",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_dict = mkg.prep_for_network(noised_dict)\n",
    "out = gunn(feat_dict, batched_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333b5d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "noised_dict['real_nodes_noise'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b3613b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_loss, pred_nf_loss, loss_3D = model_step_null(noised_dict, batched_t, mkg, gunn, train=True)\n",
    "#final_loss, pred_nf_loss, loss_3D = model_step_null(noised_dict, batched_t, mkg, gunn, train=True)\n",
    "true, noise_xyz, pred , real_nodes_pred_mask, real_nodes_true_mask = get_noise_pred_true_null(noised_dict, batched_t, mkg, gunn)\n",
    "dump_tnp_null(true, noise_xyz, pred, t_cpu, numOut=1, real_mask=real_nodes_true_mask, pred_mask=None, outdir='output/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c515ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc1824c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c89d1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device='cuda'\n",
    "\n",
    "# B = 8\n",
    "# L= 128\n",
    "# limit = 1028\n",
    "# prot_trainData = Data_Graph.ProteinBB_Dataset(coords_tog[:limit], n_nodes=L,\n",
    "#               n_atoms=5, coord_div=10, cast_type=torch.float32)\n",
    "# train_dL = DataLoader(prot_trainData, batch_size=B, shuffle=True, drop_last=True)\n",
    "# stride=4##########\n",
    "# mkg = Make_nullKNN_MP_Graphs(KNN=30, mp_stride=stride, n_nodes=L)\n",
    "\n",
    "# score_weights = {}\n",
    "# score_weights['nf_real'] = torch.tensor(0.2,device=device)\n",
    "# score_weights['3D_real'] = torch.tensor(1.0,device=device)\n",
    "# score_weights['3D_null'] = torch.tensor(1.0,device=device)\n",
    "\n",
    "# config_path='data_rigid_diffuser/base.yaml'\n",
    "# fnd = FrameDiffNoise(config_path)\n",
    "\n",
    "# gunn= GraphUNet_Null(fiber_start = Fiber({0:17, 1:2}),\n",
    "#                      fiber_out = Fiber({0:5,1:2}),\n",
    "#                       k=4,\n",
    "#                       batch_size = B,\n",
    "#                       stride=stride,\n",
    "#                        max_degree=3,\n",
    "#                        channels=64,\n",
    "#                       num_heads = 16,\n",
    "#                       channels_div=8,\n",
    "#                       num_layers = 1,\n",
    "#                      num_layers_ca = 2,\n",
    "#                      edge_feature_dim=1,\n",
    "#                      latent_pool_type = 'max',\n",
    "#                      t_size = 12,\n",
    "#                      mult=2,\n",
    "#                     zero_lin=True,\n",
    "#                    use_tdeg1 = False,\n",
    "#                  cuda=True).to('cuda')\n",
    "\n",
    "# opti = torch.optim.Adam(gunn.parameters(), lr=0.0005, weight_decay=5e-6)\n",
    "# #opti =torch.optim.Adadelta(gunn.parameters(), lr=0.0005, weight_decay=5e-7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc000156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testiter = iter(train_dL)\n",
    "# bb_dict = next(testiter)\n",
    "\n",
    "# t_cpu = np.ones((B,))*0.05\n",
    "# noised_dict = fnd.forward(bb_dict,t_vec=t_cpu)\n",
    "# batched_t = to_cuda(noised_dict['t_vec'])\n",
    "# CA_t  = noised_dict['bb_shifted']['CA'].reshape(B, L, 3).to(device)\n",
    "# NC_t = CA_t + noised_dict['bb_shifted']['N_CA'].reshape(B, L, 3).to(device)#not mult by bond distance, seems to help?\n",
    "# CC_t = CA_t + noised_dict['bb_shifted']['C_CA'].reshape(B, L, 3).to(device)#not mult \n",
    "# true =  torch.cat((NC_t,CA_t,CC_t),dim=2).reshape(B,L,3,3)\n",
    "\n",
    "# CA_n  = noised_dict['bb_noised']['CA'].reshape(B, L, 3).to(device)\n",
    "# NC_n = CA_n + noised_dict['bb_noised']['N_CA'].reshape(B, L, 3).to(device)\n",
    "# CC_n = CA_n + noised_dict['bb_noised']['C_CA'].reshape(B, L, 3).to(device)\n",
    "# noise_xyz =  torch.cat((NC_n,CA_n,CC_n),dim=2).reshape(B,L,3,3)\n",
    "\n",
    "# #prepare graphs\n",
    "# feat_dict = mkg.prep_for_network(noised_dict, cuda=True)\n",
    "# out =gunn(feat_dict,batched_t)\n",
    "\n",
    "\n",
    "# #FAPE Loss for the prediction\n",
    "# CA_p = out['1'][:,0,:].reshape(B, L, 3)+CA_n #translation of Calpha\n",
    "# Qs = out['1'][:,1,:] # rotation , convert from x,y,z (Quat) to rotate input vectors\n",
    "# Qs = Qs.unsqueeze(1).repeat((1,2,1))\n",
    "# Qs = torch.cat((torch.ones((B*L,2,1),device=Qs.device),Qs),dim=-1).reshape(B,L,2,4)\n",
    "# Qs = normQ(Qs)\n",
    "# Rs = Qs2Rs(Qs)\n",
    "# N_C_to_Rot = torch.cat((noised_dict['bb_noised']['N_CA'].reshape(B, L, 3).to(device),\n",
    "#                         noised_dict['bb_noised']['C_CA'].reshape(B, L, 3).to(device)),dim=2).reshape(B,L,2,1,3)\n",
    "# rot_vecs = einsum('bnkij,bnkhj->bnki',Rs, N_C_to_Rot)\n",
    "# NC_p = CA_p + rot_vecs[:,:,0,:]*N_CA_dist #comparable but seems better not have it for true, but have it for pred\n",
    "# CC_p = CA_p + rot_vecs[:,:,1,:]*C_CA_dist #maybe this helep prevent \n",
    "\n",
    "# pred = torch.cat((NC_p,CA_p,CC_p),dim=2).reshape(B,L,3,3)\n",
    "\n",
    "# #divide loss by real and null nodes\n",
    "\n",
    "# fp, lp  = convert_pV_to_points(noised_dict)\n",
    "\n",
    "# real_mask = noised_dict['real_nodes_mask'].to('cuda')\n",
    "# score_scales = noised_dict['score_scales'].to('cuda')\n",
    "\n",
    "# lr, lr_d = FAPE_loss_real(pred, true, score_scales, real_mask,  d_clamp=10.0, d_clamp_inter=30.0,\n",
    "#                A=10.0, gamma=1.0, eps=1e-6)\n",
    "# ln, ln_d = FAPE_loss_null(pred, fp, lp, real_mask, true, score_scales,  d_clamp=10.0,\n",
    "#                    d_clamp_inter=30.0, A=10.0, gamma=0.1, eps=1e-6)\n",
    "# ld, ln_d = FAPE_loss(pred.unsqueeze(0), true,score_scales,  d_clamp=10.0,\n",
    "#                    d_clamp_inter=30.0, A=10.0, gamma=1.0, eps=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3586095",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "se33",
   "language": "python",
   "name": "se33"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
